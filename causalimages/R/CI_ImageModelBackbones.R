#' Generates image and video representations useful in earth observation tasks for causal inference.
#'
#' Generates image and video representations useful in earth observation tasks for causal inference.
#'
#' @param file Path to a tfrecord file generated by `causalimages::WriteTfRecord`.
#' @param imageKeysOfUnits A vector of length `length(imageKeysOfUnits)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into the package to call images into memory.
#' @param conda_env A `conda` environment where computational environment lives, usually created via `causalimages::BuildBackend()`. Default = `"CausalImagesEnv"`
#' @param conda_env_required A Boolean stating whether use of the specified conda environment is required.
#' @param kernelSize Dimensions used in the convolution kernels.
#' @param nWidth_ImageRep Number of embedding features output.
#' @param strides  Integer specifying the strides used in the convolutional layers.
#' @param InitImageProcess (default = `NULL`) Initial image processing function. Usually left `NULL`.
#' @param batchSize Integer specifying batch size in obtaining representations.
#' @param dataType String specifying whether to assume `"image"` or `"video"` data types. Default is `"image"`.
#' @param temporalAggregation String specifying how to aggregate embeddings across time periods for video/image sequence data. Options are `"transformer"` which uses a temporal transformer with attention pooling, `"concatenate"` which simply concatenates the frame-level embeddings, `"difference"` which computes temporal differences to capture change dynamics (outputs mean embeddings concatenated with mean of first-order differences), or `"variance"` which concatenates temporal mean with temporal variance to capture both typical state and volatility/instability.
#' @param TfRecords_BufferScaler The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#' @param pretrainedModel Optional string specifying a pretrained vision model to use for feature extraction.
#'   Built-in options include:
#'   \itemize{
#'     \item `"vit-base"` - Google's Vision Transformer (ViT-Base, 768-dim embeddings)
#'     \item `"clip-rsicd"` - CLIP fine-tuned on remote sensing data (512-dim embeddings)
#'     \item `"clip-rsicd-v0"` - Legacy CLIP-RSICD implementation
#'   }
#'   For generic HuggingFace models, use the `"transformers-"` prefix followed by the model name:
#'   \itemize{
#'     \item `"transformers-facebook/dinov2-base"` - DINOv2 self-supervised ViT
#'     \item `"transformers-microsoft/resnet-50"` - ResNet-50
#'     \item `"transformers-facebook/convnext-base-224"` - ConvNeXt
#'     \item `"transformers-<any-huggingface-model>"` - Any vision model from HuggingFace
#'   }
#'   The generic handler uses `AutoModel.from_pretrained()` and automatically detects hidden dimensions,
#'   input sizes, and handles various output formats. Default is `NULL` (uses custom model architecture).
#' @param NORM_MEAN Numeric vector of length 1-3 specifying dataset mean(s) for normalization. Used with pretrained models.
#' @param NORM_SD Numeric vector of length 1-3 specifying dataset standard deviation(s) for normalization. Used with pretrained models.
#' @param X Optional numeric matrix of tabular covariates for cross-modal learning.
#' @param returnContents Boolean specifying whether to return internal model contents. Default = `TRUE`.
#' @param getRepresentations Boolean specifying whether to compute and return representations. Default = `TRUE`.
#' @param imageModelClass String specifying the image model architecture. Options include `"VisionTransformer"` (default) or `"CNN"`.
#' @param Sys.setenv_text Optional string for setting environment variables before Python initialization.
#' @param lat,long Optional vectors specifying latitude and longitude coordinates for spatial context.
#' @param image_dtype JAX dtype for image data. Usually set internally.
#' @param image_dtype_tf TensorFlow dtype for image data. Usually set internally.
#' @param XCrossModal Boolean specifying whether to use cross-modal learning with tabular data. Default = `TRUE`.
#' @param XForceModal Boolean specifying whether to force modal learning. Default = `FALSE`.
#' @param nDepth_ImageRep Integer specifying depth of image representation model. Default = `1L`.
#' @param nDepth_TemporalRep Integer specifying depth of temporal representation model for video data. Default = `1L`.
#' @param nonLinearScaler Optional string specifying non-linear scaling function for outputs.
#' @param optimizeImageRep Boolean specifying whether to optimize image representation parameters. Default = `TRUE`.
#' @param patchEmbedDim Integer specifying patch embedding dimension for Vision Transformer. Default = `16L`.
#' @param dropoutRate Dropout rate used in training. Default = `0`.
#' @param droppathRate Droppath rate used in training. Default = `0`.
#' @param bn_momentum Batch normalization momentum. Default = `0.99`.
#' @param inputAvePoolingSize Integer specifying average pooling size for downshifting image resolution. Default = `1L`.
#' @param CleanupEnv Boolean specifying whether to clean up environment after processing. Default = `FALSE`.
#' @param initializingFxns Boolean specifying whether to only initialize functions without computing representations. Default = `FALSE`.
#' @param seed Optional integer for reproducibility.
#'
#' @return A list containing two items:
#' \itemize{
#' \item `Representations` (matrix) A matrix containing image/video representations, with rows corresponding to observations.
#' \item `ImageRepArm_OneObs,ImageRepArm_batch_R, ImageRepArm_batch` (functions) Image modeling functions.
#' \item `ImageModel_And_State_And_MPPolicy_List` List containing image model parameters fed into functions.
#' }
#'
#' @section References:
#' \itemize{
#' \item Rolf, Esther, et al. "A generalizable and accessible approach to machine learning with global satellite imagery." *Nature Communications* 12.1 (2021): 4392.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md

GetImageRepresentations <- function(
    X = NULL,
    imageKeysOfUnits = NULL,
    file = NULL,
    conda_env = "CausalImagesEnv",
    conda_env_required = T,
    returnContents = T,
    getRepresentations = T,
    imageModelClass = "VisionTransformer",
    NORM_MEAN = NULL, 
    NORM_SD = NULL, 
    Sys.setenv_text = NULL,

    InitImageProcess = NULL,
    pretrainedModel = NULL, 
    lat = NULL, long = NULL, 
    image_dtype = NULL, 
    image_dtype_tf = NULL,  
    XCrossModal = T, 
    XForceModal = F, 
    nWidth_ImageRep = 64L,
    nDepth_ImageRep = 1L,
    nDepth_TemporalRep = 1L,
    batchSize = 16L,
    nonLinearScaler = NULL, 
    optimizeImageRep = T,
    strides = 1L,
    kernelSize = 3L,
    patchEmbedDim = 16L,
    TfRecords_BufferScaler = 10L,
    dropoutRate=0,
    droppathRate=0,
    dataType = "image",
    temporalAggregation = "concatenate",
    bn_momentum = 0.99,
    inputAvePoolingSize = 1L, # set > 1L if seeking to downshift the image resolution
    CleanupEnv = FALSE, 
    initializingFxns = FALSE, 
    seed = NULL){
  
  # IMPORTANT: If using a pretrained model that requires torch/transformers,

  # initialize torch BEFORE JAX/TF/NumPy to avoid import conflicts
  if (pretrained_model_requires_torch(pretrainedModel)) {
    if (!"torch" %in% ls(envir = cienv)) {
      initialize_torch(conda_env = conda_env,
                       conda_env_required = conda_env_required,
                       Sys.setenv_text = Sys.setenv_text)
    }
  }

  if(!"jax" %in% ls(envir = cienv)) {
    initialize_jax(conda_env = conda_env,
                   conda_env_required = conda_env_required,
                   Sys.setenv_text = Sys.setenv_text)
  }
  
  FlashMultiheadAttention <- function(query, key_ = NULL, value = NULL, mask = NULL,
                                      W_q, W_k, W_v, W_o,
                                      num_heads = 12L, is_causal = FALSE) {
    
    # Default key and value to query if not provided
    if(is.null(key_)) key_ <- query
    if(is.null(value)) value <- query
    
    # 1) Linear projections: [T, D] @ [D, D] -> [T, D]
    q <- cienv$jnp$dot(query, W_q)
    k <- cienv$jnp$dot(key_, W_k)
    v <- cienv$jnp$dot(value, W_v)
    
    # infer head dims
    D <- as.integer(q$shape[[2]])
    if (D %% as.integer(num_heads) != 0L) {
      stop("Embedding dimension (D) must be divisible by num_heads")
    }
    head_dim <- as.integer(D / as.integer(num_heads))
    
    # 2) Reshape to heads: [T, D] -> [T, N, H]
    q <- cienv$jnp$reshape(q, list(q$shape[[1]], num_heads, head_dim))
    k <- cienv$jnp$reshape(k, list(k$shape[[1]], num_heads, head_dim))
    v <- cienv$jnp$reshape(v, list(v$shape[[1]], num_heads, head_dim))
    
    # 3) Process mask if provided
    mask_bool <- NULL
    if(!is.null(mask)) {
      # Convert to boolean and broadcast to [N, T, S]
      mask_bool <- cienv$jnp$greater(mask, 0)
      mask_bool <- cienv$jnp$expand_dims(mask_bool, 0L)
      mask_bool <- cienv$jnp$broadcast_to(mask_bool, list(num_heads, q$shape[[1]], k$shape[[1]]))
    }
    
    # Auto-detect backend
    attention_path <- ifelse(any(sapply(cienv$jax$devices(), function(d) d$platform == "gpu")),
                             yes = "cudnn", no = "xla")
    if(attention_path == "xla") {
      attn_out <- cienv$jax$nn$dot_product_attention(
        q$astype(cienv$jnp$float32),
        k$astype(cienv$jnp$float32),
        v$astype(cienv$jnp$float32),
        mask = mask_bool,
        is_causal = is_causal,
        implementation = "xla"
      )$astype(k$dtype)
    }
    if(attention_path == "cudnn") {
      attn_out <- cienv$flash_mha(
        q$astype(cienv$jnp$float16),
        k$astype(cienv$jnp$float16),
        v$astype(cienv$jnp$float16),
        is_causal = is_causal,
        mask = mask_bool
      )$astype(k$dtype)
   }
    
    # 5) Merge heads back: [T, N, H] -> [T, D]
    attn_out <- cienv$jnp$reshape(attn_out, list(attn_out$shape[[1]], num_heads * head_dim))
    
    # 6) Output projection: [T, D] @ [D, D] -> [T, D]
    output <- cienv$jnp$dot(attn_out, W_o)
    
    return(output)
  }
  
  # deal with null dropouts
  if(is.null(dropoutRate)){ dropoutRate <- 0 }
  if(is.null(droppathRate)){ droppathRate <- 0 }

  # validate temporalAggregation parameter
  if(!temporalAggregation %in% c("transformer", "concatenate", "difference", "variance")){
    stop("temporalAggregation must be 'transformer', 'concatenate', 'difference', or 'variance'")
  }

  # image dtype
  if(is.null(image_dtype)){
    image_dtype <- cienv$jnp$float16
    image_dtype_tf <- cienv$tf$float16
  }
  nScalePatches <- ai(3L^2)
  seed <- as.integer(runif(1,1,10^8))
  
  message2("Setting input types in GetImageRepresentations()...") 
  if(!is.null(pretrainedModel)){ pretrainedModel <- as.character(pretrainedModel) } 
  if(!is.null(optimizeImageRep)){ optimizeImageRep <- as.logical(optimizeImageRep) }
  if(!is.null(imageModelClass)){ imageModelClass <- as.character(imageModelClass) }
  if(!is.null(nWidth_ImageRep)){ nWidth_ImageRep <- as.integer(f2n(nWidth_ImageRep)) }

  if( batchSize > length( unique(imageKeysOfUnits) )){
    batchSize <- length( unique(imageKeysOfUnits)  )
  }

  # define base tf record + train/test fxns
  orig_wd <- getwd()
  if(  !is.null(  file  )  ){
    # established tfrecord connection
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    setwd( new_wd <- paste(tf_record_name[-length(tf_record_name)],collapse = "/") )
    tf_dataset = cienv$tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideo <- (dataType == "video")
    getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      return( dataset <- dataset$batch( as.integer(max(2L, round(batchSize/2L)  ))) )
    }

    getParsed_tf_dataset_train <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo, image_dtype = image_dtype_tf)} ) # return
      dataset <- dataset$shuffle(cienv$tf$constant(as.integer(TfRecords_BufferScaler*batchSize), dtype=cienv$tf$int64),
                                 reshuffle_each_iteration = T)
      dataset <- dataset$batch(as.integer(batchSize))
    }

    # setup iterators
    tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )

    # reset iterators
    ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )
  }

  # acquire image to set dimensions
  setwd(orig_wd); test_ <- cienv$tf$expand_dims(GetElementFromTfRecordAtIndices( 
                                                             uniqueKeyIndices = 1L,
                                                             filename = file,
                                                             readVideo = useVideo,
                                                             image_dtype = image_dtype_tf,
                                                             nObs = length(unique(imageKeysOfUnits)))[[1]],0L); setwd(new_wd)
  rawShape <- cienv$np$array(test_$shape)
  imageDims <- ai( length( rawShape ) - 2L )
  rawSpatialDims <- ai( rawShape[length(rawShape)-1] )
  rawChannelDims <- ai( rawShape[length(rawShape)] )
  # for video data, rawShape is (1, time, height, width, channels)
  nTimePeriods <- ifelse(dataType == "video", yes = ai(rawShape[2]), no = 1L)
  
  if(!is.null(pretrainedModel)){
    if(grepl(pretrainedModel,pattern="clip")){ nWidth_ImageRep <- nWidth_VideoRep <- 512L }
    if(grepl(pretrainedModel,pattern="vit-base")){ nWidth_ImageRep <- nWidth_VideoRep <- 768L }
    if(grepl(pretrainedModel,pattern="clay")){ nWidth_ImageRep <- nWidth_VideoRep <- 768L }
    if(grepl(pretrainedModel,pattern="swin")){ nWidth_ImageRep <- nWidth_VideoRep <- 768L }
  }

  # setup jax model
  {
    message2("Setting up image representation model...")
    MPList <- list(cienv$jmp$Policy(compute_dtype="float16",  param_dtype="float32", output_dtype="float32"),
                   cienv$jmp$DynamicLossScale(cienv$jnp$array(2^15), period = 1000L))

    # coerce to integer for safety
    kernelSize <- ai(kernelSize); strides <- ai(strides)
    rawChannelDims <- ai(rawChannelDims); nWidth_ImageRep <- ai(nWidth_ImageRep)

    # set batch name
    batch_axis_name <- "batch";
    if(!"bn_momentum" %in% ls()){ bn_momentum <- 0.90 }

    # transformer preliminaries
    ffmap <- cienv$jax$vmap(function(L_, x){ L_(x) }, in_axes = list(NULL,0L))
    RMS_norm <- function(x_){ cienv$jnp$divide( x_, cienv$jnp$sqrt(0.001+cienv$jnp$mean(cienv$jnp$square(x_), -1L, keepdims=T))) }
    LayerNorm <- function(x_){ cienv$jax$nn$standardize( x_, -1L, epsilon = 1e-5) }
    # InvSoftPlus <- function(x){ cienv$jnp$log(cienv$jnp$exp(x) - 1) }
    InvSoftPlus <- function(y) {  
      y <- cienv$jnp$clip(y, 1e-6, 1e6)
      y + cienv$jnp$log1p(-cienv$jnp$exp(-y))
    }
    
    # Calculate number of patches
    nPatches_side = ai(rawSpatialDims / patchEmbedDim)
    nPatches = ai(nPatches_side^2)
    InitialPatchDims <- ai( (nPatches_side*patchEmbedDim*nPatches_side*patchEmbedDim)/nPatches * rawChannelDims )
    
    # adjust widths as needed 
    nWidth_VideoRep <- nWidth_ImageRep
    if(!is.null(pretrainedModel)){ if(grepl(pretrainedModel, pattern = "videomae")){ nWidth_ImageRep <- nWidth_VideoRep <- 2L*768L } }  
    
    # rotary embedding setup
    RotaryPositionalEmbeddings_spatial <- cienv$eq$nn$RotaryPositionalEmbedding( nWidth_ImageRep ) 
    RotaryPositionalEmbeddings_temporal <- cienv$eq$nn$RotaryPositionalEmbedding( nWidth_VideoRep  ) 
    WideMultiplicationFactor <- 3.5
    nTransformerOutputWidth <- nWidth_ImageRep
    
    # define projection for cross modal attention 
    XProj <- cienv$jnp$array(1.)
    if (!is.null(X)) { # project each featureâ€vector into embedding space
       XProj <- cienv$eq$nn$Linear(
                   in_features  = ai(ncol(X)),
                   out_features = ai(nWidth_ImageRep),
                   use_bias     = FALSE,
                   key          = cienv$jax$random$key(ai(33244151433 + seed))
                )
   } 
    
    # setup initial processor
    if( is.null(InitImageProcess) ){
      InitImageProcess <- (function(im, seed, inference =  F){ return( im  ) })
    }
    # Ensure InitImageProcess accepts batch_indices parameter for consistent calling
    # (The batch_indices parameter is used for geospatial embedding in some pretrained models
    # but must be accepted by all InitImageProcess functions for the call at line ~1505)
    {
      InitImageProcess_wrapper_base <- InitImageProcess
      InitImageProcess <- function(m, seed, inference, batch_indices = NULL){
        InitImageProcess_wrapper_base(m, seed, inference)
      }
    }
    if( !is.null(pretrainedModel) ){
      InitImageProcess_orig <- InitImageProcess
      InitImageProcess <- function(m, seed, inference, batch_indices = NULL){ 
        # normalize for this/these models
        if( grepl(pretrainedModel,pattern="clay")  ){ 
          m <- InitImageProcess_orig(m, cienv$jax$random$key(1L), T) 
        }
        
        # model specific transformations 
        if(!grepl(pretrainedModel,pattern="video")){
          if( dataType == "video" ){ 
            m_shape_orig <- m$shape
            m <- cienv$jnp$reshape(m, list(-1L,m$shape[[3]],m$shape[[4]], m$shape[[5]]))
          }
          if(m$shape[[4]] == 1L){ m <- m * cienv$jnp$expand_dims(cienv$jnp$expand_dims(cienv$jnp$array(t(c(1,1,1))),0L),0L)$astype(m$dtype) }
          if(m$shape[[4]] > 3L){ m <- cienv$jnp$take(m,0L:2L,axis=3L) }
        }
        if( pretrainedModel == "clip-rsicd-v0" ){
          # https://huggingface.co/flax-community/clip-rsicd-v2
          if(!"FeatureExtractor" %in% ls(.GlobalEnv)){
            message2("Loading a pre-trained model (clip-rsicd)...")
            initialize_torch(conda_env = conda_env,
                             conda_env_required = conda_env_required,
                             Sys.setenv_text = Sys.setenv_text)
            
            PretrainedImageModelName <- "flax-community/clip-rsicd-v2"
            cienv$FeatureExtractor <- cienv$transformers$CLIPProcessor$from_pretrained(PretrainedImageModelName)
            cienv$torch$set_default_device(
              RunOnDevice <- ifelse(cienv$torch$cuda$is_available(), 
                                     yes = list(cienv$torch$device("cuda")), 
                                     no = list(cienv$torch$device("cpu")))[[1]] 
            )
            cienv$torch$set_default_dtype( RunDtype <- cienv$torch$float32 ); 
            cienv$TransformersModel <- cienv$transformers$CLIPModel$from_pretrained(PretrainedImageModelName)$to(RunOnDevice)#$half()
            cienv$TransformersProcessor <- cienv$transformers$CLIPImageProcessor$from_pretrained(PretrainedImageModelName)
            cienv$nParameters_Pretrained <- cienv$TransformersModel$num_parameters()

            # from examining FeatureExtractor (?)
            cienv$MEAN_RESCALER <- cienv$jnp$array(c(0, 0,0))
            cienv$MEAN_RESCALER <- cienv$jnp$reshape(cienv$MEAN_RESCALER,list(1L,3L,1L,1L))
            
            cienv$SD_RESCALER <- cienv$jnp$array(c(1,1,1))
            cienv$SD_RESCALER <- cienv$jnp$reshape(cienv$SD_RESCALER,list(1L,3L,1L,1L))
            
            cienv$NORM_MEAN_array_inner <- cienv$jnp$reshape(cienv$jnp$array(NORM_MEAN),list(1L,1L,1L,3L))
            cienv$NORM_SD_array_inner <- cienv$jnp$reshape(cienv$jnp$array(NORM_SD),list(1L,1L,1L,3L))
            
            cienv$ScaleResizeTranspose <- cienv$jax$jit(function(m){
              # m <- (m - NORM_MEAN_array_inner) / NORM_SD_array_inner
              m <- cienv$jnp$clip((m / 255.) ,0.0001,0.9999)
              m <- cienv$jax$image$resize( image=m,
                                     shape=c(m$shape[[1]], 224L, 224L, 3L), method="bilinear")
              m <- cienv$jnp$transpose(m, c(0L,3L,1L,2L))
            })
          }
          
          # stretch and re-normalize. see FeatureExtractor for details 
          m <- cienv$ScaleResizeTranspose(m)
          # m <- (m*SD_RESCALER)+MEAN_RESCALER
          # cienv$jnp$mean(cienv$jnp$array(m), axis = c(0L,2L:3L)); cienv$jnp$std(cienv$jnp$array(m), axis =  c(0L,2L:3L)) 
          #m <- cienv$torch$tensor( reticulate::np_array( cienv$tf$constant(m, cienv$tf$float32), dtype = cienv$np$float32), 
          m <- cienv$torch$tensor( m,  dtype = cienv$torch$float32)
          m <- cienv$TransformersProcessor(m, do_rescale = F, return_tensors="pt")['pixel_values']
          # m <- reticulate::np_array( cienv$tf$constant(m, cienv$tf$float32), dtype = cienv$np$float32)
          # m <- FeatureExtractor(images = m, return_tensors="pt", do_resize = T)["pixel_values"]$type(RunDtype)$to(RunOnDevice)
          m <- cienv$TransformersModel$get_image_features(pixel_values = m)$cpu()$detach()$numpy() 
          # plot(m[,1:10])
          cienv$py_gc$collect()
        }
        if( grepl("^transformers-", pretrainedModel) ){
          # Generic handler for any HuggingFace transformers vision model
          # Usage: pretrainedModel = "transformers-facebook/dinov2-base"
          #        pretrainedModel = "transformers-microsoft/resnet-50"
          #        pretrainedModel = "transformers-google/vit-base-patch16-224"

          if(!"JAX_Model" %in% ls(envir = cienv)){
            # Extract the actual HuggingFace model name from "transformers-XXX" pattern
            PretrainedImageModelName <- sub("^transformers-", "", pretrainedModel)
            message2(sprintf("Loading generic transformers model '%s' via torchax compilation...", PretrainedImageModelName))

            # 1. Initialize TorchAx and Transformers if needed
            if(!"torchax" %in% ls(envir = cienv)){cienv$torchax <- reticulate::import("torchax")}
            if(!"transformers" %in% ls(envir = cienv)){cienv$transformers <- reticulate::import("transformers")}

            # 2. Load the model using AutoModel for maximum flexibility
            # AutoModel will automatically select the correct model class
            pt_model <- cienv$transformers$AutoModel$from_pretrained(PretrainedImageModelName)
            pt_model$eval()

            # 3. Configure model for inference
            pt_model$config$return_dict <- FALSE
            pt_model$config$output_attentions <- FALSE
            pt_model$config$output_hidden_states <- FALSE

            # 4. Compile the PyTorch model to JAX using torchax
            JaxModelPackage <- cienv$torchax$extract_jax(pt_model)
            cienv$JAX_Weights <- JaxModelPackage[[1]]
            cienv$JAX_Model   <- JaxModelPackage[[2]]
            cienv$JAX_Model   <- cienv$torchax$interop$jax_jit( cienv$JAX_Model )

            # 5. Detect model output dimension from config
            # Different model architectures use different attribute names
            model_config <- pt_model$config
            hidden_dim <- NULL
            if (!is.null(model_config$hidden_size)) {
              hidden_dim <- as.integer(model_config$hidden_size)
            } else if (!is.null(model_config$num_features)) {
              hidden_dim <- as.integer(model_config$num_features)
            } else if (!is.null(model_config$embed_dim)) {
              hidden_dim <- as.integer(model_config$embed_dim)
            } else {
              # Fallback: use 768 (common ViT dimension)
              hidden_dim <- 768L
              message2(sprintf("Could not auto-detect hidden dimension, defaulting to %d", hidden_dim))
            }

            # 6. Detect expected input image size from config
            expected_size <- 224L  # Default
            if (!is.null(model_config$image_size)) {
              expected_size <- as.integer(model_config$image_size)
            }
            cienv$EXPECTED_IMAGE_SIZE <- expected_size

            # 7. Set Metadata
            cienv$nParameters_Pretrained <- pt_model$num_parameters()
            cienv$nWidth_ImageRep <- hidden_dim
            cienv$GENERIC_TRANSFORMERS_MODEL <- TRUE  # Flag for output handling

            # Store model name for reference
            cienv$TRANSFORMERS_MODEL_NAME <- PretrainedImageModelName

            # 8. Define Preprocessing Constants (JAX Arrays)
            # Use ImageNet normalization by default (most common for vision models)
            cienv$MEAN_RESCALER <- cienv$jnp$reshape(cienv$jnp$array(c(0.485, 0.456, 0.406)), list(1L, 3L, 1L, 1L))
            cienv$SD_RESCALER   <- cienv$jnp$reshape(cienv$jnp$array(c(0.229, 0.224, 0.225)), list(1L, 3L, 1L, 1L))

            # Dataset-specific normalization (applied to NHWC input)
            if(is.null(cienv$NORM_MEAN_array_inner)){
              if(length(NORM_MEAN) == 1){
                cienv$NORM_MEAN <- rep(NORM_MEAN, 3); cienv$NORM_SD <- rep(NORM_SD, 3)
              }
              if(length(NORM_MEAN) == 2){
                NORM_MEAN <- c(NORM_MEAN, NORM_MEAN[1]); NORM_SD <- c(NORM_SD, NORM_SD[1])
              }
              cienv$NORM_MEAN_array_inner <- cienv$jnp$reshape(cienv$jnp$array(NORM_MEAN), list(1L, 1L, 1L, 3L))
              cienv$NORM_SD_array_inner   <- cienv$jnp$reshape(cienv$jnp$array(NORM_SD), list(1L, 1L, 1L, 3L))
            }

            # Cleanup PyTorch model to free memory
            try(rm("pt_model", "JaxModelPackage"),T)
            cienv$py_gc$collect()
          }

          # --- JAX Preprocessing Pipeline ---
          # 1. Normalize using Dataset Statistics (Standardization)
          # Input 'm' is (Batch, Height, Width, Channel)
          m <- (m - cienv$NORM_MEAN_array_inner) / cienv$NORM_SD_array_inner

          # 2. Resize to expected size (typically 224x224)
          target_size <- cienv$EXPECTED_IMAGE_SIZE
          m <- cienv$jax$image$resize(
            image = m,
            shape = c(m$shape[[1]], target_size, target_size, 3L),
            method = "bilinear"
          )

          # 3. Transpose NHWC -> NCHW (PyTorch Model Requirement)
          m <- cienv$jnp$transpose(m, c(0L, 3L, 1L, 2L))

          # 4. Apply ImageNet normalization: (x - mean) / std
          m <- (m - cienv$MEAN_RESCALER) / cienv$SD_RESCALER

          # 5. Execute the compiled JAX model
          out <- cienv$JAX_Model(
            cienv$JAX_Weights,
            tuple(m),          # positional: (pixel_values,)
            dict()             # kwargs empty
          )

          # 6. Extract embeddings - handle various output formats
          # Different models return different structures:
          # - ViT-like: (last_hidden_state, pooler_output)
          # - ResNet-like: (last_hidden_state,) or just features
          # - DINOv2-like: (last_hidden_state, pooler_output)
          if(inherits(out, "python.builtin.tuple") || is.list(out)){
            if(length(out) >= 2 && !is.null(out[[2]])){
              # Use pooler_output if available (CLS token or global avg pool)
              m <- out[[2]]
            } else {
              # Otherwise use last_hidden_state and apply global average pooling
              last_hidden <- out[[1]]
              # Check dimensionality - if 3D (batch, seq, hidden), pool over seq
              if(length(last_hidden$shape) == 3){
                # Global average pooling over sequence dimension (excluding CLS for some models)
                m <- cienv$jnp$mean(last_hidden, axis = 1L)
              } else {
                m <- last_hidden
              }
            }
          } else {
            # Single output tensor - assume it's already pooled
            m <- out
          }

          # Handle if output is still wrapped in tuple/list
          if(inherits(m, "python.builtin.tuple") || is.list(m)){
            m <- m[[1]]
          }

          # Explicit garbage collection
          cienv$py_gc$collect()
        }
        if( pretrainedModel == "clip-rsicd" ){
          # https://huggingface.co/flax-community/clip-rsicd-v2
          if(!"JAX_Model" %in% ls(envir = cienv)){
            message2("Loading clip-rsicd model via torchax compilation...")

            # 1. Initialize TorchAx and Transformers if needed
            if(!"torchax" %in% ls(envir = cienv)){cienv$torchax <- reticulate::import("torchax")}
            if(!"transformers" %in% ls(envir = cienv)){cienv$transformers <- reticulate::import("transformers")}

            # 2. Load the PyTorch CLIP model
            PretrainedImageModelName <- 'flax-community/clip-rsicd-v2'
            pt_model <- cienv$transformers$CLIPModel$from_pretrained(PretrainedImageModelName)
            pt_model$eval()

            # 3. Create a wrapper module that calls get_image_features directly
            # This avoids the BaseModelOutputWithPooling issue - get_image_features returns a simple tensor
            reticulate::py_run_string("
import torch
import torch.nn as nn

class CLIPImageFeatureExtractor(nn.Module):
    def __init__(self, clip_model):
        super().__init__()
        self.clip_model = clip_model

    def forward(self, pixel_values):
        # get_image_features returns (batch, 512) tensor directly
        return self.clip_model.get_image_features(pixel_values)
")
            feature_extractor <- reticulate::py$CLIPImageFeatureExtractor(pt_model)
            feature_extractor$eval()

            # 4. Compile the wrapper to JAX using torchax
            # Note: Don't use JIT - the non-JIT version returns proper JAX arrays directly
            # JIT would return torchax.tensor.Tensor which requires .jax() conversion
            JaxModelPackage <- cienv$torchax$extract_jax(feature_extractor)
            cienv$JAX_Weights <- JaxModelPackage[[1]]
            cienv$JAX_Model   <- JaxModelPackage[[2]]

            # 5. Set Metadata
            cienv$nParameters_Pretrained <- pt_model$num_parameters()
            cienv$nWidth_ImageRep <- 512L  # CLIP embedding dimension

            # 6. Define Preprocessing Constants (JAX Arrays)
            # CLIP uses specific normalization values (OpenAI CLIP standard)
            cienv$MEAN_RESCALER <- cienv$jnp$reshape(cienv$jnp$array(c(0.48145466, 0.4578275, 0.40821073)), list(1L, 3L, 1L, 1L))
            cienv$SD_RESCALER   <- cienv$jnp$reshape(cienv$jnp$array(c(0.26862954, 0.26130258, 0.27577711)), list(1L, 3L, 1L, 1L))

            # Dataset-specific normalization (applied to NHWC input)
            if(is.null(cienv$NORM_MEAN_array_inner)){
              if(length(NORM_MEAN) == 1){
                cienv$NORM_MEAN <- rep(NORM_MEAN, 3); cienv$NORM_SD <- rep(NORM_SD, 3)
              }
              if(length(NORM_MEAN) == 2){
                NORM_MEAN <- c(NORM_MEAN, NORM_MEAN[1]); NORM_SD <- c(NORM_SD, NORM_SD[1])
              }
              cienv$NORM_MEAN_array_inner <- cienv$jnp$reshape(cienv$jnp$array(NORM_MEAN), list(1L, 1L, 1L, 3L))
              cienv$NORM_SD_array_inner   <- cienv$jnp$reshape(cienv$jnp$array(NORM_SD), list(1L, 1L, 1L, 3L))
            }

            # Cleanup PyTorch models to free memory
            try(rm("pt_model", "feature_extractor", "JaxModelPackage"),T)
            cienv$py_gc$collect()
          }

          # --- JAX Preprocessing Pipeline ---
          # 1. Normalize using Dataset Statistics (Standardization)
          # Input 'm' is (Batch, Height, Width, Channel)
          m <- (m - cienv$NORM_MEAN_array_inner) / cienv$NORM_SD_array_inner

          # 2. Resize to 224x224 (CLIP Requirement)
          m <- cienv$jax$image$resize(
            image = m,
            shape = c(m$shape[[1]], 224L, 224L, 3L),
            method = "bilinear"
          )

          # 3. Transpose NHWC -> NCHW (PyTorch Model Requirement)
          m <- cienv$jnp$transpose(m, c(0L, 3L, 1L, 2L))

          # 4. Apply CLIP normalization: (x - mean) / std
          m <- (m - cienv$MEAN_RESCALER) / cienv$SD_RESCALER

          # 5. Execute the compiled JAX model
          # The wrapper calls get_image_features which returns (batch, 512) directly
          m <- cienv$JAX_Model(
            cienv$JAX_Weights,
            tuple(m),          # positional: (pixel_values,)
            dict()             # kwargs empty
          )

          # Handle output format (may be tuple, extract first element if so)
          if(inherits(m, "python.builtin.tuple") || is.list(m)){
            m <- m[[1]]
          }

          # Explicit garbage collection
          cienv$py_gc$collect()
      }
        if( grepl(pretrainedModel, pattern = "vit-base") ){
          if(!"JAX_Model" %in% ls(envir = cienv)){
            message2("Loading vit-base model via torchax compilation...")
            
            # 1. Initialize TorchAx and Transformers if needed
            if(!"torchax" %in% ls(envir = cienv)){cienv$torchax <- reticulate::import("torchax")}
            if(!"transformers" %in% ls(envir = cienv)){cienv$transformers <- reticulate::import("transformers")}
            
            # 2. Load the standard PyTorch ViT model
            # Matches the checkpoint used in the 'vit-base-XXX' reference
            PretrainedImageModelName <- 'google/vit-base-patch16-224-in21k'
            pt_model <- cienv$transformers$ViTModel$from_pretrained(PretrainedImageModelName)
            pt_model$eval()
            pt_model$config$return_dict <- FALSE
            pt_model$config$output_attentions <- FALSE
            pt_model$config$output_hidden_states <- FALSE
            
            # 3. Compile the PyTorch model to a JAX function using torchax
            # This returns a JIT-compatible function that accepts JAX arrays
            JaxModelPackage <- cienv$torchax$extract_jax(pt_model)
            cienv$JAX_Weights <- JaxModelPackage[[1]]
            cienv$JAX_Model   <- JaxModelPackage[[2]]
            cienv$JAX_Model   <- cienv$torchax$interop$jax_jit( cienv$JAX_Model ) 
            
            # 4. Set Metadata
            cienv$nParameters_Pretrained <- pt_model$num_parameters()
            cienv$nWidth_ImageRep <- 768L
            
            # 5. Define Preprocessing Constants (JAX Arrays)
            # Model-specific rescaling (Affine transform: *0.5 + 0.5)
            # Reshaped for NCHW format broadcasting: (1, 3, 1, 1)
            cienv$MEAN_RESCALER <- cienv$jnp$reshape(cienv$jnp$array(c(0.5, 0.5, 0.5)), list(1L, 3L, 1L, 1L))
            cienv$SD_RESCALER   <- cienv$jnp$reshape(cienv$jnp$array(c(0.5, 0.5, 0.5)), list(1L, 3L, 1L, 1L))
            
            # Dataset-specific normalization (applied to NHWC input)
            # Replicates the robust setup logic from the reference branch
            if(is.null(cienv$NORM_MEAN_array_inner)){
              if(length(NORM_MEAN) == 1){ 
                cienv$NORM_MEAN <- rep(NORM_MEAN, 3); cienv$NORM_SD <- rep(NORM_SD, 3)
              }
              if(length(NORM_MEAN) == 2){ 
                NORM_MEAN <- c(NORM_MEAN, NORM_MEAN[1]); NORM_SD <- c(NORM_SD, NORM_SD[1])
              }
              cienv$NORM_MEAN_array_inner <- cienv$jnp$reshape(cienv$jnp$array(NORM_MEAN), list(1L, 1L, 1L, 3L))
              cienv$NORM_SD_array_inner   <- cienv$jnp$reshape(cienv$jnp$array(NORM_SD), list(1L, 1L, 1L, 3L))
            }
            
            # Cleanup PyTorch model to free memory
            try(rm("pt_model","JaxModelPackage"),T)
            cienv$py_gc$collect()
          }
          
          # --- JAX Preprocessing Pipeline ---
          # 1. Normalize using Dataset Statistics (Standardization)
          # Input 'm' is (Batch, Height, Width, Channel)
          m <- (m - cienv$NORM_MEAN_array_inner) / cienv$NORM_SD_array_inner
          
          # 2. Resize to 224x224 (ViT Requirement)
          m <- cienv$jax$image$resize(
            image = m,
            shape = c(m$shape[[1]], 224L, 224L, 3L),
            method = "bilinear"
          )
          
          # 3. Transpose NHWC -> NCHW (PyTorch Model Requirement)
          m <- cienv$jnp$transpose(m, c(0L, 3L, 1L, 2L))
          
          # 4. Rescale to Model Statistics
          # Matches reference logic: m_final = m_std * 0.5 + 0.5
          m <- (m * cienv$SD_RESCALER) + cienv$MEAN_RESCALER
          
          # 5. Execute the compiled JAX model
          #out <- cienv$JAX_ViT_Model(
          out <- cienv$JAX_Model(
            cienv$JAX_Weights,
            tuple(m),          # positional: (pixel_values,)
            dict()             # kwargs empty
          )
          
          if(FALSE){ # working pipeline
            pt_model <- cienv$transformers$ViTModel$from_pretrained(PretrainedImageModelName)
            pt_model$eval()
            pt_model$config$return_dict <- FALSE
            pt_model$config$output_attentions <- FALSE
            pt_model$config$output_hidden_states <- FALSE
            weights_func <- cienv$torchax$extract_jax(pt_model)
            cienv$JAX_ViT_Weights <- weights_func[[1]]
            cienv$JAX_ViT_Model   <- weights_func[[2]]
            cienv$JAX_ViT_Model_compiled   <- cienv$torchax$interop$jax_jit( cienv$JAX_ViT_Model ) 
            
            out <- cienv$JAX_ViT_Model_compiled(
              cienv$JAX_ViT_Weights,
              tuple(m),  # positional: (pixel_values,)
              #dict(return_dict = FALSE,output_hidden_states = FALSE,output_attentions = FALSE)
              dict()
            )
          }
          
          # 6. Extract the CLS token embedding (pooler_output)
          # in second element (first entry is activations)
          m <- out[[2]]
          
          # Explicit garbage collection
          cienv$py_gc$collect()
      }
        if( grepl(pretrainedModel, pattern = "clay") ){ 
          # https://clay-foundation.github.io/model/tutorials/clay-v1-wall-to-wall.html
          #Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
          if(!"ClayModel" %in% ls(.GlobalEnv)){
            message2("Loading a pre-trained model (Clay)...")
            initialize_torch(conda_env = conda_env, 
                           conda_env_required = conda_env_required,
                           Sys.setenv_text = Sys.setenv_text) 
            
            
            if( !Sys.info()["machine"] == "x86_64" ){ 
              oldwd <- getwd(); setwd("~/Documents/model/");
              cienv$ClayModel <- reticulate::import_from_path("model", path = "./src")
              setwd(oldwd)
              cienv$ClayModel <- cienv$ClayModel$ClayMAEModule$load_from_checkpoint(
                "/Users/cjerzak/Documents/Clay/Clay-1.0.5.7_epoch-13_val-loss-0.3098.ckpt", 
                metadata_path="/Users/cjerzak/Documents/model/configs/metadata.yaml", 
                shuffle=F,  mask_ratio=0, batch_first = T)
            }
            if( Sys.info()["machine"] == "x86_64" & T == F ){ 
              cienv$ClayModel <- cienv$ClayModel$ClayMAEModule$load_from_checkpoint(
                "/home/cjerzak/Documents/Clay/clay-v1-base.ckpt", 
                metadata_path="/home/cjerzak/Documents/model/configs/metadata.yaml", 
                shuffle=F,  mask_ratio=0) 
            }
            #})
          }
          if(!"RunOnDevice" %in% ls(.GlobalEnv)){
            cienv$torch$set_default_device(
              RunOnDevice <- ifelse(cienv$torch$cuda$is_available(), 
                                     yes = list(cienv$torch$device("cuda")), 
                                     no = list(cienv$torch$device("cpu")))[[1]] 
            )
            cienv$torch$set_default_dtype( RunDtype <- cienv$torch$float32 ); 
            cienv$ClayModel <- cienv$ClayModel$to(RunOnDevice)
            
            cienv$nParameters_Pretrained <- reticulate::as_iterator(  cienv$ClayModel$model$encoder$parameters() )
            nParameters_Pretrained_ <- 0; 
            DoneLoop <- F; while(!DoneLoop){ 
              theNextN <- try(reticulate::iter_next( cienv$nParameters_Pretrained )$numel(), T)
              if('integer' %in% class(theNextN)){ nParameters_Pretrained_ <- nParameters_Pretrained_ + theNextN  }
              if(!'integer' %in% class(theNextN)){ DoneLoop <- T }
            }
            cienv$nParameters_Pretrained <- nParameters_Pretrained_
          }
          
          # get place embeddings 
          latlong_embed <- cbind(lat[batch_indices] * pi / 180, long[batch_indices] * pi / 180)
          latlong_embed <-  cbind(sin(latlong_embed[,1]),cos(latlong_embed[,1]),
                                  sin(latlong_embed[,2]),cos(latlong_embed[,2]))
          if(dataType == "video"){
            latlong_embed <- do.call(rbind, unlist(apply(latlong_embed,1,function(zer){
              list(cbind(rep(zer[1], times = cienv$np$array(m$shape)[1]/m_shape_orig[[1]]),
                         rep(zer[2], times = cienv$np$array(m$shape)[1]/m_shape_orig[[1]]),
                         rep(zer[3], times = cienv$np$array(m$shape)[1]/m_shape_orig[[1]]),
                         rep(zer[4], times = cienv$np$array(m$shape)[1]/m_shape_orig[[1]])))
            }), recursive = F))
            #latlong_embed <- cienv$np$array( cienv$jnp$reshape(cienv$jnp$concatenate( list(cienv$jnp$expand_dims(cienv$jnp$array(latlong_embed),1L), cienv$jnp$expand_dims(cienv$jnp$array(latlong_embed),1L)), 1L), list(-1L,4L) )) 
          }
          time_embed <- t(replicate(cienv$np$array(m$shape)[1],{c(-0.1205, -0.9927,  0.2588, -0.9659)})) 
          
          # obtain embeddings 
          # cienv$ClayModel$model$metadata$`landsat-c2l1`
          # RGB means: 10678.0, 10563.0, 11083.0
          # RGB stds: 9578.0, 9408.0, 10144.0
          # cienv$ClayModel$model$metadata$`landsat-c2l2-sr`

          # resize to 256 if image is greater in size 
          #if(m$shape[[2]] > 256L){
            #m <- cienv$jax$image$resize(
              #image=m, shape=c(m$shape[[1]],  256L, 256L, m$shape[[3]]),
              #method="bilinear")
          #}
          
          m <- cienv$ClayModel$model$encoder(
            dict("platform" = "landsat-c2l1",  # platform
                 "time" = cienv$torch$tensor( time_embed, dtype = RunDtype)$to(RunOnDevice), # temporal embedding 
                 "latlon" = cienv$torch$tensor( latlong_embed, dtype = RunDtype )$to(RunOnDevice), # lat long embedding 
                 #"pixels" = cienv$torch$tensor( reticulate::np_array(m$transpose(c(0L,3L,1L,2L))), dtype = RunDtype )$to(RunOnDevice), # normalized image 
                 #"pixels" = cienv$torch$tensor( reticulate::np_array(cienv$tf$constant(m$transpose(c(0L,3L,1L,2L)))), dtype = cienv$torch$float32),
                 "pixels" = cienv$torch$tensor( m$transpose(c(0L,3L,1L,2L)), dtype = cienv$torch$float32),
                 "gsd" = cienv$torch$tensor(30, dtype = RunDtype)$to(RunOnDevice),  # resolution 
                 'waves' = cienv$torch$tensor(c(0.65, 0.56, 0.48), dtype = RunDtype)$to(RunOnDevice)  # wavelength in micrometers?, this assumes RGB
                 #'waves' = cienv$torch$tensor(c(0.493, 0.560, 0.665), dtype = RunDtype)$to(RunOnDevice)  # wavelength in micrometers?, this assumes BGR
            )
          )[[1]]  
          # The first embedding is the [CLS], which is a global embedding
          m = cienv$jnp$array(  m$cpu()$detach()$numpy()[,1,] ) 
          # plot(cienv$np$array(m)[,sample(1:10,2)])
        }
        if( !grepl(pretrainedModel,pattern="video") & dataType == "video" ){ 
          # reshape if not using videomae
          m <- cienv$jnp$reshape(cienv$jnp$array(m), list(m_shape_orig[[1]], m_shape_orig[[2]], -1L) ) 
         } 
        if(grepl(pretrainedModel,pattern="videomae")){ 
          if(!"FeatureExtractor" %in% ls(.GlobalEnv) ){  # https://huggingface.co/docs/transformers/en/model_doc/videomae
            message2("Loading a pre-trained model (videomae)...")
            PretrainedVideoModelName <- "MCG-NJU/videomae-base"
            #videoModelName <- "MCG-NJU/videomae-base-finetuned-kinetics"
            
            initialize_torch(conda_env = conda_env,
                             conda_env_required = conda_env_required,
                             Sys.setenv_text = Sys.setenv_text)
            
            # set device and dtypes
            cienv$torch$set_default_device(
              RunOnDevice <- ifelse(cienv$torch$cuda$is_available(), 
                                     yes = list(cienv$torch$device("cuda")), 
                                     no = list(cienv$torch$device("cpu")))[[1]] 
            )
            cienv$torch$set_default_dtype( RunDtype <- cienv$torch$float32 ); 
            
            # load models
            cienv$py_gc$collect()
            # use VideoMAEImageProcessor? 
            cienv$FeatureExtractor <- cienv$transformers$ViTImageProcessor$from_pretrained(PretrainedVideoModelName)
            cienv$TransformersModel <- cienv$transformers$ViTModel$from_pretrained(PretrainedVideoModelName, 
                                                                              torch_dtype = cienv$torch$float16)#half()$to(RunOnDevice)
            cienv$nParameters_Pretrained <- cienv$TransformersModel$num_parameters()
            
            cienv$MEAN_RESCALER <- cienv$jnp$array(c(1,1,1))
            cienv$MEAN_RESCALER <- cienv$jnp$reshape(cienv$MEAN_RESCALER,
                                                     list(1L,1L,1L,1L,3L))
            
            cienv$SD_RESCALER <- cienv$jnp$array(c(1,1,1))
            cienv$SD_RESCALER <- cienv$jnp$reshape(cienv$SD_RESCALER,
                                                   list(1L,1L,1L,1L,3L))
            
            cienv$NORM_MEAN_array_inner <- cienv$jnp$reshape(cienv$jnp$array(NORM_MEAN),list(1L,1L,1L,1L,3L))
            cienv$NORM_SD_array_inner <- cienv$jnp$reshape(cienv$jnp$array(NORM_SD),list(1L,1L,1L,1L, 3L))
          }
          #m <- reticulate::np_array( cienv$tf$constant(m), dtype = cienv$np$uint8)
          
          m <- (m - NORM_MEAN_array_inner) / NORM_SD_array_inner
          m <- cienv$jax$image$resize(
            image=m,
            shape=c(m$shape[[1]], m$shape[[2]],  224L, 224L, 3L),
            method="bilinear")
          #m <- FeatureExtractor(images = m, return_tensors="pt",do_resize = T, do_rescale = F, do_normalize = F)["pixel_values"]$type(RunDtype)$to(RunOnDevice)
          m <- (m*SD_RESCALER)+MEAN_RESCALER
          # cienv$jnp$mean(cienv$jnp$array(m), axis = c(0L,2L:3L)); cienv$jnp$std(cienv$jnp$array(m), axis =  c(0L,2L:3L)) 
          
          #m <- cienv$jnp$array(m, dtype = cienv$np$uint8)
          m_rep <- c(); for(j_ in 1L:as.integer(m$shape[[1]]) ){ # iterate over the batch dimension? 
            m_ <- cienv$jnp$take(m, j_-1L, axis = 0L)
            if(m_$shape[[4]] == 1L){ m_ <- m_ * cienv$jnp$expand_dims(cienv$jnp$expand_dims(cienv$jnp$array(t(c(1,1,1))),0L),0L)$astype(m$dtype) }
            if(m_$shape[[4]] > 3L){ m_ <- cienv$jnp$take(m,0L:2L,axis=3L) }
            m_ <- cienv$jnp$transpose(  m_, c(0L,3L,1L,2L))
            #m_ <- cienv$torch$tensor( reticulate::np_array( cienv$tf$constant(m_, cienv$tf$float32), dtype = cienv$np$float32), dtype = cienv$torch$float32)
            m_ <- cienv$torch$tensor( m, dtype = cienv$torch$float32)
            
            # run model
            # output of extractor is T by C by W by H
            m_ <- TransformersModel(m_)$pooler_output$cpu()$detach()$numpy()
            
            # save final data 
            m_rep <- rbind( m_rep, c(colMeans(m_), apply(m_, 2,sd) ))
          }
          m <- cienv$jnp$array( m_rep )
        }
        return( m ) 
      }
    }
    
    # get norm stats if needed 
    if(is.null(NORM_MEAN)){ 
      NORM_MEAN <- GetMoments(ds_iterator_train, dataType = dataType, image_dtype = image_dtype, momentCalIters = 34)
      NORM_SD <- NORM_MEAN$NORM_SD_array; NORM_MEAN <- NORM_MEAN$NORM_MEAN
    }
    
    # define a transformer backbone - background fxns 
    {
    keeppathRate <- 1 - ( droppathRate )
    # Custom dropout layer with dynamic branching for JAX in R
    # note: cienv$eq$nn$Dropout(p = dropoutRate) -> breaks gradients 
    #dropout_layer <- dropout_layer_init(dropoutRate)
    compute_branch_attention <- function(ModelList_d, m, 
                                         RotaryPositionalEmbeddings, seed, is_droppath, inference){
      m <- RMS_norm(mtm1 <- m) * ModelList_d$TransformerRenormer$NormScaler1
      m_pos <- RotaryPositionalEmbeddings(m)
      m <- FlashMultiheadAttention(
        query = m_pos,
        key_ = m_pos, 
        value = m, 
        mask = NULL,
        W_q = ModelList_d$Multihead$W_q,
        W_k = ModelList_d$Multihead$W_k,
        W_v = ModelList_d$Multihead$W_v,
        W_o = ModelList_d$Multihead$W_o,
        num_heads = 8L,
        is_causal = FALSE
      )
      m <- m * cienv$jax$nn$softplus( ModelList_d$ResidualWts$RightWt1$astype(cienv$jnp$float32) )$astype(m$dtype)
      scale_factor <- (1 / keeppathRate)*is_droppath + (1-is_droppath)*1
      return( mtm1 + m * scale_factor ) }
    compute_branch_mlp <- function(ModelList_d, m, seed, is_droppath, inference){ 
      # Normalization and swiglu to high dim 
      m <- RMS_norm(mtm1 <- m) * ModelList_d$TransformerRenormer$NormScaler2
      m <- cienv$jax$nn$swish(ffmap(ModelList_d$FF$FFWide1, m)) *
        ffmap(ModelList_d$FF$FFWide2, m) # swiglu proj to high dim
      
      # dropout in high dim 
      m <- dropout_layer_init(dropoutRate)(m, key = seed, inference = inference) # dropout, per element
      
      # project back to low dim
      m <- ffmap(ModelList_d$FF$FFNarrow, m) # linear proj to low dim
      m <- m * cienv$jax$nn$softplus( ModelList_d$ResidualWts$RightWt2$astype(cienv$jnp$float32) )$astype(m$dtype)
      scale_factor <- (1 / keeppathRate)*is_droppath + (1-is_droppath)*1
      return( mtm1 + m*scale_factor ) }
    }
    
    # define a transformer backbone 
    TransformerBackbone <- function(ModelList, m, x,
                                    StateList, seed, MPList, inference, type){
      if(type == "Spatial"){ # patch embed
          m <- ModelList$SpatialTransformerSupp$PatchEmbedder(cienv$jnp$transpose(m, c(2L, 0L, 1L)))
          m <- cienv$jnp$transpose(cienv$jnp$reshape(m, list(m$shape[[1]],-1L)))
          
          # incorporate X
          if (!is.null(X) & XCrossModal & !XForceModal) {
            x_proj <- ffmap(ModelList$SpatialTransformerSupp$XProj, cienv$jnp$expand_dims(x,0L))
            x_proj <- dropout_layer_init(dropoutRate)(x_proj, key = seed, inference = inference)
            m <- cienv$jnp$concatenate(
                    list(m, x_proj), 
                    axis = 0L)
          }
          if (!is.null(X) & XForceModal) {
             x_proj <- ffmap(ModelList$SpatialTransformerSupp$XProj, cienv$jnp$expand_dims(x,0L))
             x_proj <- dropout_layer_init(dropoutRate)(x_proj, key = seed, inference = inference)
             m <- x_proj
          }
          message2(sprintf("Transformer dims: [%s]", paste(unlist(m$shape),collapse=",")))
      }

      # append start and stop tokens
      m <- cienv$jnp$concatenate(list( eval(parse(text = sprintf("ModelList$%sTransformerSupp$StartEmbed",type))), m), 0L)
      m <- cienv$jnp$concatenate(list(m, eval(parse(text = sprintf("ModelList$%sTransformerSupp$StopEmbed",type)))), 0L) 
      
      {
          DepthOfThisTransformer <- ifelse(type=="Spatial", yes = nDepth_ImageRep, no = nDepth_TemporalRep)
          layer_fn <- function(carry, ModelList_d){
            m <- carry$m; seed <- carry$seed
            
            if( inference || droppathRate == 0 ){
              print("Not using droppath here...")
               m <- compute_branch_attention(ModelList_d, m, 
                                        RotaryPositionalEmbeddings_spatial, seed, 
                                        cienv$jnp$array(FALSE),  # is droppath indicator 
                                        inference)
               seed   <- cienv$jax$random$split(seed)[[1L]]
               m <- compute_branch_mlp(ModelList_d, m, seed, 
                                       cienv$jnp$array(FALSE), # is droppath indicator 
                                       inference)
            }
            if( !(inference || droppathRate == 0) ){
              print("Using droppath here...")
              seed   <- cienv$jax$random$split(seed)[[1L]]
              if(TRUE){
                # --ATTENTION--
                do_path_indicator   <- cienv$jax$random$bernoulli(seed, p = keeppathRate, shape = list()); seed   <- cienv$jax$random$split(seed)[[1L]]
                mm_ <- compute_branch_attention(ModelList_d , 
                                                m,
                                                RotaryPositionalEmbeddings_spatial, seed,
                                                cienv$jnp$array(TRUE), # is droppath
                                                inference)
                m <- m + do_path_indicator$astype(m$dtype) * (mm_ - m)
                # Explanation: 
                # m + 0 * (m_ - m) -> m -> follow old
                # m + 1 * (m_ - m) -> m_ -> take new 
                
                # --MLP--
                do_path_indicator   <- cienv$jax$random$bernoulli(seed, p = keeppathRate, shape = list()); seed   <- cienv$jax$random$split(seed)[[1L]]
                mm_ <- compute_branch_mlp(ModelList_d , 
                                          m,
                                          seed,
                                          cienv$jnp$array(TRUE),# is droppath
                                          inference)
                m <- m + do_path_indicator$astype(m$dtype) * (mm_ - m)
              }
              if(FALSE){
              # Note: conditional path via lax.cond is under construction 
              m   <- cienv$jax$lax$cond(pred = do_path_indicator$astype(cienv$jnp$bool_), 
                                        true_fun = function(operands){ # true fun 
                                          m_ <- compute_branch_attention(operands[[1]] , operands[[2]], 
                                                                   operands[[3]], operands[[4]],
                                                                   cienv$jnp$array(TRUE), # is droppath 
                                                                   inference)
                                          #seed_   <- cienv$jax$random$split(operands[[4]])[[1L]]
                                          m_ <- compute_branch_mlp(operands[[1]] , m_, 
                                                             operands[[4]], 
                                                             cienv$jnp$array(TRUE),# is droppath 
                                                             inference)
                                                             #operands[[5]])
                                          return(m_)
                                          },  
                                        false_fun = function(operands){return(operands[[2]])}, # false fun
                                        operand = list(ModelList_d, m, 
                                                       RotaryPositionalEmbeddings_spatial, seed))  # operands
              }
            }
            seed  <- cienv$jax$random$split(seed)[[1L]]
            return( list(list("m"=m, 
                              "seed"=seed), NULL) ) }
          m <- cienv$jax$lax$scan(f = layer_fn, 
                                  init = list(m = m, seed = seed), 
                                  xs = ModelList$SpatialTransformer)[[1]]$m
      }

      # take CLS embedding from position 0 [Start]
      #m <- cienv$jnp$take(m, indices = 0L, axis = 0L)
      
      # CLS + global pooling
      { 
      #m <- cienv$jnp$squeeze(ffmap(ModelList$SpatialTransformerSupp$MixCLSPool,
              #cienv$jnp$expand_dims(cienv$jnp$concatenate( 
                #list(
                     #cienv$jnp$take(m, indices = 0L, axis = 0L), # CLS 
                     #cienv$jnp$mean(m, axis = 0L) # global pool 
                    #), axis = 0L), 0L)),0L)
      }
      
      # attention pooling
      {
          pooled <- FlashMultiheadAttention(
            query = cienv$jnp$expand_dims(cienv$jnp$take(m, 0L, axis = 0L), 0L),  # CLS
            key_ = RotaryPositionalEmbeddings_spatial(m), 
            value = m, 
            mask = NULL,
            W_q = ModelList$SpatialTransformerSupp$PoolMultihead$W_q,
            W_k = ModelList$SpatialTransformerSupp$PoolMultihead$W_k,
            W_v = ModelList$SpatialTransformerSupp$PoolMultihead$W_v,
            W_o = ModelList$SpatialTransformerSupp$PoolMultihead$W_o,
            num_heads = 8L,
            is_causal = FALSE
          )
          m <- cienv$jnp$squeeze(pooled, 0L)  
          
          # keep your projection + norm (optional, but preserves interface)
          m <- ffmap(ModelList$SpatialTransformerSupp$PoolProject, cienv$jnp$expand_dims(m,0L))
          m <- cienv$jnp$squeeze(m,0L)
      }

      # output block -- only executed in final pass over sequences
      if( (dataType == "image" & type == "Spatial") |  (type == "Temporal") ){
        # final norm
        m <- cienv$jnp$squeeze( RMS_norm( cienv$jnp$expand_dims(m,0L) ) *
                           eval(parse(text = sprintf("ModelList$%sTransformerSupp$FinalNormScaler",type))) )

        # linear proj, note: dense layers starts with linear projection, so generally this is not needed
        # m <- eval(parse(text = sprintf("ModelList$%sTransformerSupp$FinalProj",type)))( m )  
      }
      message2("Returning output in TransformerBackbone()")
      return( list(m, StateList) )
    }
    
    # Spatial backbone 
    message2("Setting up a spatial backbone...")
    DoFineTuning <- grepl(pretrainedModel,pattern="-ft")
    if(length(DoFineTuning) == 0){ DoFineTuning <- FALSE}
    if(DoFineTuning){
      setwd(orig_wd); batch_inference_ <- GetElementFromTfRecordAtIndices( uniqueKeyIndices = 1:batchSize,
                                                                          filename = file,
                                                                          nObs = length(unique(imageKeysOfUnits)),
                                                                          return_iterator = T,
                                                                          readVideo = useVideo,
                                                                          image_dtype = image_dtype_tf,
                                                                          iterator = NULL); setwd(new_wd)
      InitImageProcess(cienv$jnp$array( batch_inference_[[1]][[1]]), cienv$jax$random$key(ai(2000L + seed)), inference = T);rm(batch_inference_)
      
      ModelList <- c("FTParams"= list(cienv$FeatureExtractor$params),
                     "FTParams_NormRescaler"= cienv$jnp$array(t(rep(1,times = nWidth_ImageRep))),
                     "FTParams_Proj"= cienv$eq$nn$Linear(in_features = ai(nWidth_ImageRep),
                                                   out_features = ai(nWidth_ImageRep),
                                                   use_bias = F, # hidden bias
                                                   key = cienv$jax$random$key(ai(33440L)))
                     )
      StateList <- list("None"=cienv$jnp$array(.0)) # initialize with 0's
      FTBackbone <- function(ModelList, m, StateList, seed, MPList, inference, type){
        #m <- FeatureExtractor(
        m <- cienv$FeatureExtractor$get_image_features(
                pixel_values = cienv$jnp$expand_dims(m,0L),
                params = ModelList$FTParams,
                dropout_rng = seed,
                train = !inference # with dropout 
        )
        #      )$pooler_output
        m <- LayerNorm(m)*ModelList$FTParams_NormRescaler
        # m <- ffmap(ModelList$FTParams_Proj,m)
        m <- cienv$jnp$squeeze(m, 0L)
        return( list(m, cienv$jnp$array(1)) ) # representation, state 
      }
    }
    
    if(!DoFineTuning){
    # Transformer backbone 
    if(imageModelClass == "VisionTransformer"){
      StateList <- ModelList <- replicate(nDepth_ImageRep, cienv$jnp$array(0.)) # initialize with 0's
      rand_array <- function(x,key){
        x <- cienv$jnp$array( x )
        return( x + cienv$jax$random$normal(shape = x$shape,dtype=x$dtype,key=key)*0.0001 )
      }
      base_seed <- cienv$jax$random$split(cienv$jax$random$key(seed),nDepth_ImageRep)
      create_layer <- function(key, l_){ 
          if(!is.null(nonLinearScaler)){
              nonLinearScaler_ <- rand_array(rep(nonLinearScaler, times=nWidth_ImageRep), key)
          }
          if(is.null(nonLinearScaler)){
            if(dataType == "video"){ 
              nonLinearScaler_ <- rand_array( rep(( 2*(nDepth_ImageRep + nDepth_TemporalRep) )^(-1/2), times = nWidth_ImageRep), key )
            }
            if(dataType == "image"){ 
              #nonLinearScaler_ <- rand_array( rep(( 2*nDepth_ImageRep )^(-1/2), times = nWidth_ImageRep), key)
              nonLinearScaler_ <- cienv$jnp$broadcast_to(cienv$jnp$array(nDepth_ImageRep,cienv$jnp$float32)^(-1/(2*l_-2)), ai(nWidth_ImageRep)) # Fixup Scaling
            }
          }
          key <- cienv$jax$random$split(key)[[1]]
          TransformerRenormer_d <- list("NormScaler1"=rand_array( t(rep(1,times=nWidth_ImageRep) ),key ),
                                        "NormScaler2"=rand_array( t(rep(1,times=nWidth_ImageRep) ),key ))
          key <- cienv$jax$random$split(key)[[1]]
          {
            # â€”â€” Define Q/K/V/O weights for (Flash) attention â€”â€”
            # head_dim must divide ModelDims by TransformerHeads (already ensured elsewhere)
            key <- cienv$jax$random$split(key,4L)
            Multihead_d <- list(
              # [D, D] projections: Q=xt_posÂ·W_q, K=xt_posÂ·W_k, V=xtÂ·W_v; O merges heads back
              "W_q" = wt_init(list(nWidth_ImageRep, nWidth_ImageRep), key[0L]),
              "W_k" = wt_init(list(nWidth_ImageRep, nWidth_ImageRep), key[1L]),
              "W_v" = wt_init(list(nWidth_ImageRep, nWidth_ImageRep), key[2L]),
              "W_o" = wt_init(list(nWidth_ImageRep, nWidth_ImageRep), key[3L])
            )
            key <- cienv$jax$random$split(key[[1]])[[1]]
          }
          key <- cienv$jax$random$split(key,3L)
          FF_d <- list("FFWide1"=cienv$eq$nn$Linear(in_features = nWidth_ImageRep,
                                           out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                           use_bias = F, # hidden bias
                                           key = key[0L]),
                              "FFWide2"=cienv$eq$nn$Linear(in_features = nWidth_ImageRep,
                                           out_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                           use_bias = F, # swiglu bias
                                           key = key[1L]),
                              "FFNarrow"=cienv$eq$nn$Linear(in_features = ai(nWidth_ImageRep*WideMultiplicationFactor),
                                           out_features = nWidth_ImageRep,
                                           use_bias = F, # final bias
                                           key = key[2L])
                                           )
          key <- cienv$jax$random$split(key[[0]])[[1]]
          ModelList_d <- list("Multihead" = Multihead_d,
                                  "FF" = FF_d,
                                  "ResidualWts" = list("RightWt1"=InvSoftPlus(nonLinearScaler_),
                                                       "RightWt2"=InvSoftPlus(nonLinearScaler_)),
                                  "TransformerRenormer" = TransformerRenormer_d)
          return(ModelList_d)
      }
      # spatial transformer, vmapping over seeds 
      ModelList$SpatialTransformer <- cienv$jax$vmap(create_layer, 
                                                     in_axes = list(0L,0L),
                                                     out_axes = 0L)(base_seed, 
                                                                    cienv$jnp$array(as.matrix(1.:nDepth_ImageRep)))
      StateList <- cienv$jnp$array(0.)
      
      ModelList$SpatialTransformerSupp = list(
          "XProj" = XProj,
          "MixCLSPool" = cienv$eq$nn$Linear(
                                  in_features  = ai(nWidth_ImageRep*2L),
                                  out_features = ai(nWidth_ImageRep),
                                  use_bias     = FALSE,
                                  key          = cienv$jax$random$key(ai(3324 + seed))),
          "StartEmbed" = cienv$jax$random$uniform(key = cienv$jax$random$key(ai(333324L + seed)),
                                                  minval = -sqrt(6/nWidth_ImageRep), maxval = sqrt(6/nWidth_ImageRep), shape = list(1L,nWidth_ImageRep)), # Start
          "StopEmbed" = cienv$jax$random$uniform(key = cienv$jax$random$key(ai(33326124L + seed )),
                                                minval = -sqrt(6/nWidth_ImageRep), maxval = sqrt(6/nWidth_ImageRep), shape = list(1L,nWidth_ImageRep)), # Stop
          "PatchEmbedder" = cienv$eq$nn$Conv(kernel_size = ai(c(patchEmbedDim, patchEmbedDim)),
                     num_spatial_dims = 2L, stride = ai(c(patchEmbedDim,patchEmbedDim)),
                     padding_mode = "ZEROS", # "REFLECT", "ZEROS", "REPLICATE", "CIRCULAR" 
                     in_channels = rawChannelDims, use_bias = T,
                     out_channels = nWidth_ImageRep, key = cienv$jax$random$key(ai(4L+1040L+seed))), # patch embed
          
          # new 
          "PoolQuery" =
            cienv$eq$nn$Linear(in_features = ai(nWidth_ImageRep),
                               out_features = ai(1L),    # produce logits per token
                               use_bias = TRUE,
                               key = cienv$jax$random$key(ai(3322144 + seed))),
          "PoolProject" =
            cienv$eq$nn$Linear(in_features = ai(nWidth_ImageRep),
                               out_features = ai(nWidth_ImageRep),
                               use_bias = TRUE,
                               key = cienv$jax$random$key(ai(332415 + seed))),
          "PoolMultihead" =  list(
            # [D, D] projections: Q=xt_posÂ·W_q, K=xt_posÂ·W_k, V=xtÂ·W_v; O merges heads back
            "W_q" = wt_init(list(nWidth_ImageRep, nWidth_ImageRep), cienv$jax$random$key(ai(3325 + seed))),
            "W_k" = wt_init(list(nWidth_ImageRep, nWidth_ImageRep), cienv$jax$random$key(ai(3415 + seed))),
            "W_v" = wt_init(list(nWidth_ImageRep, nWidth_ImageRep), cienv$jax$random$key(ai(32415 + seed))),
            "W_o" = wt_init(list(nWidth_ImageRep, nWidth_ImageRep), cienv$jax$random$key(ai(32416 + seed)))
          ),
          
          "FinalNormScaler" = cienv$jnp$array(rep(1,times = nWidth_ImageRep)),  # RMS weighter
          "FinalProj" = cienv$eq$nn$Linear(in_features = nWidth_ImageRep, out_features =  nTransformerOutputWidth,
                        use_bias = F, key = cienv$jax$random$key(ai(999L + seed  ) )) # final dense proj
        )
    }
    }
    
    # Temporal backbone
    if(dataType == "video"){
      message2("Setting up temporal backbone...")
      key <- cienv$jax$random$key(ai(seed + 10000L))
      for(dt_ in 1L:nDepth_TemporalRep){
        TransformerRenormer_d <- list("NormScaler1" = cienv$jnp$array( t(rep(1,times=nWidth_VideoRep) ) ),
                                      "NormScaler2" = cienv$jnp$array( t(rep(1,times=nWidth_VideoRep) ) ))
        {
          key <- cienv$jax$random$split(key,4L)
          Multihead_d <- list(
            # [D, D] projections: Q=xt_posÂ·W_q, K=xt_posÂ·W_k, V=xtÂ·W_v; O merges heads back
            "W_q" = wt_init(list(nWidth_VideoRep, nWidth_VideoRep), key[0L]),
            "W_k" = wt_init(list(nWidth_VideoRep, nWidth_VideoRep), key[1L]),
            "W_v" = wt_init(list(nWidth_VideoRep, nWidth_VideoRep), key[2L]),
            "W_o" = wt_init(list(nWidth_VideoRep, nWidth_VideoRep), key[3L])
          )
          key <- cienv$jax$random$split(key[[1]])[[1]]
        }
        FF_d  <- list(
          "FFWide1" = cienv$eq$nn$Linear(in_features = nWidth_VideoRep,
                          out_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
                          use_bias = F, # hidden bias
                          key = cienv$jax$random$key(ai(334300L + 1L+dt_ + seed  ))),
          "FFWide2" = cienv$eq$nn$Linear(in_features = nWidth_VideoRep,
                          out_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
                          use_bias = F, # swiglu bias
                          key = cienv$jax$random$key(ai(333110L + 1L+dt_ + seed ))),
          "FFNarrow" = cienv$eq$nn$Linear(in_features = ai(nWidth_VideoRep*WideMultiplicationFactor),
                          out_features = nWidth_VideoRep,
                          use_bias = F, # final bias
                          key = cienv$jax$random$key(ai(3333924L + 1L + dt_ + seed ))))
        # Define nonLinearScaler_ for temporal backbone
        if(!is.null(nonLinearScaler)){
            nonLinearScaler_ <- cienv$jnp$broadcast_to(cienv$jnp$array(nonLinearScaler, cienv$jnp$float32), ai(nWidth_VideoRep))
        }
        if(is.null(nonLinearScaler)){
            nonLinearScaler_ <- cienv$jnp$broadcast_to(cienv$jnp$array(( 2*(nDepth_ImageRep + nDepth_TemporalRep) )^(-1/2), cienv$jnp$float32), ai(nWidth_VideoRep))
        }
        eval(parse(text = sprintf('ModelList$Temporal_d%s <- 
                        list("TransformerRenormer" = TransformerRenormer_d,
                             "Multihead" = Multihead_d,
                             "ResidualWts" = list("RightWt1" = InvSoftPlus( nonLinearScaler_ ),
                                                  "RightWt2" = InvSoftPlus( nonLinearScaler_ )),
                             "FF" = FF_d)', dt_)))
      }
      
      ModelList$TemporalTransformerSupp = list(
        "StartEmbed" = cienv$jax$random$uniform(key = cienv$jax$random$key(ai(33932124L + seed +  dt_)),
                                     minval = -sqrt(6/nWidth_VideoRep), maxval = sqrt(6/nWidth_VideoRep), shape = list(1L,nWidth_VideoRep)), # start 
        "StopEmbed" =  cienv$jax$random$uniform(key = cienv$jax$random$key(ai(3324L + seed +  dt_)), 
                                     minval = -sqrt(6/nWidth_VideoRep), maxval = sqrt(6/nWidth_VideoRep), shape = list(1L,nWidth_VideoRep)), # stop
        "PatchEmbedder" =  cienv$jnp$array(0.), # unused  in temporal
        "FinalNormScaler" =  cienv$jnp$array( t(rep(1,times=nWidth_VideoRep) ) ),
        "FinalProj" =  cienv$eq$nn$Linear(in_features = nWidth_VideoRep, out_features =  nWidth_VideoRep,
                               use_bias = F, key = cienv$jax$random$key(ai(1999L+dt_+seed  )))
        )
    }
    
    # Combine all entities
    # m <- InitImageProcess( cienv$jnp$array( batch_inference[[1]]),T)[0,0,,,];  d__ <- 1L; inference <- F
    # m <- InitImageProcess( cienv$jnp$array( batch_inference[[1]]), T);  d__ <- 1L; inference <- F
    ImageRepArm_SpatialArm <- function(ModelList, m, x,
                                       StateList, seed, MPList, inference){
      if(DoFineTuning){
          m <- FTBackbone(ModelList, m, StateList, seed, MPList, inference, type = "Spatial")
          StateList <- cienv$jnp$array(1.); m <- m[[1]]
      }
      if(!DoFineTuning){
        if(imageModelClass == "VisionTransformer" ){
          m <- TransformerBackbone(ModelList, m, x,
                                   StateList, seed, MPList, inference, type = "Spatial")
          StateList <- m[[2]]; m <- m[[1]]
        }
      }
      message2("Returning ImageRepArm_SpatialArm outputs...")
      return( list(m, StateList)  )
    }
    
    #ImageRepArm_batch <- (ImageRepArm_batch_R <- function(ModelList, m, StateList, seed, MPList, inference){ message2("DEBUG MODE IS ON IN IMAGE BACKBONE"); Sys.sleep(5L); 
    ImageRepArm_batch <- cienv$eq$filter_jit(ImageRepArm_batch_R <- function(ModelList, m, x,
                                                                             StateList, seed, MPList, inference){
      ModelList <- MPList[[1]]$cast_to_compute( ModelList )
      StateList <- MPList[[1]]$cast_to_compute( StateList )

      # squeeze temporal dim if needed
      thisPath <- T; if(!is.null(pretrainedModel)){ thisPath <- !grepl(pretrainedModel, pattern="video") }
      if(thisPath){
        if(dataType == "video" & is.null(pretrainedModel)){ m <- cienv$jnp$reshape(m, c(-1L, (orig_shape_m <- cienv$jnp$shape(m))[3:5])) }
  
        message2(sprintf("Image stack dims: [%s]", paste(unlist(m$shape),collapse=",")))
  
        # get spatial representation if custom architecture 
        if( is.null(pretrainedModel) ){ 
          m <- cienv$jax$vmap(function(ModelList, m, x,
                                 StateList, seed, MPList, inference){
                ImageRepArm_SpatialArm(ModelList, m, x,
                                       StateList, seed, MPList, inference) },
                   in_axes = list(NULL, 0L, 0L, 
                                  NULL, 0L, NULL, NULL),
                   axis_name = batch_axis_name,
                   out_axes = list(0L,NULL))(ModelList, m, x,
                                             StateList, seed, MPList, inference)
          StateList <- m[[2]]; m <- m[[1]]
        }
  
        # unsqueeze temporal dim if needed
        if(dataType == "video"){
          if( is.null(pretrainedModel) ){ 
            m <- cienv$jnp$reshape(m, c(orig_shape_m[1:2], -1L)) 
          }
          # (cienv$np$array(m)[1:5,,sample(1:10,1)]); m$shape

          # temporal aggregation: transformer or concatenate
          if( temporalAggregation == "transformer" ){
            m <- cienv$jax$vmap(function(ModelList, m, x,
                                         StateList, seed, MPList, inference){
                        TransformerBackbone(ModelList, m, x,
                                            StateList, seed, MPList, inference, type = "Temporal")},
                          in_axes = list(NULL, 0L, 0L,
                                         NULL, 0L, NULL, NULL),
                          axis_name = batch_axis_name,
                          out_axes = list(0L,NULL))(ModelList, m, x,
                                                    StateList, seed, MPList, inference)
            StateList <- m[[2]]; m <- m[[1]]
          }
          if(temporalAggregation == "concatenate"){
            # simply concatenate embeddings from all time periods
            m <- cienv$jnp$reshape(m, c(m$shape[[1]], -1L))
            message2(sprintf("Concatenated temporal embeddings: [%s]", paste(unlist(m$shape),collapse=",")))
          }
          if(temporalAggregation == "difference"){
            # compute temporal differences to capture change dynamics
            # m shape: [batch, time, dim]
            # compute mean level (average across time)
            m_mean <- cienv$jnp$mean(m, axis = 1L)  # [batch, dim]
            # compute first-order differences between consecutive frames
            m_diffs <- m[, 2L:as.integer(m$shape[[2]]), ] - m[, 1L:(as.integer(m$shape[[2]])-1L), ]  # [batch, time-1, dim]
            # average the differences to get mean change
            m_diff_mean <- cienv$jnp$mean(m_diffs, axis = 1L)  # [batch, dim]
            # concatenate level and change: [batch, 2*dim]
            m <- cienv$jnp$concatenate(list(m_mean, m_diff_mean), axis = -1L)
            message2(sprintf("Difference temporal embeddings (level + change): [%s]", paste(unlist(m$shape),collapse=",")))
          }
          if(temporalAggregation == "variance"){
            # compute mean and variance to capture typical state and volatility
            # m shape: [batch, time, dim]
            # compute mean level (average across time)
            m_mean <- cienv$jnp$mean(m, axis = 1L)  # [batch, dim]
            # compute temporal variance (volatility/instability)
            m_var <- cienv$jnp$var(m, axis = 1L)  # [batch, dim]
            # concatenate mean and variance: [batch, 2*dim]
            m <- cienv$jnp$concatenate(list(m_mean, m_var), axis = -1L)
            message2(sprintf("Variance temporal embeddings (mean + variance): [%s]", paste(unlist(m$shape),collapse=",")))
          }
          message2("Returning ImageRepArm_TemporalArm outputs...")
        }
      }
      message2("Returning ImageRepArm_batch outputs...")
      return( list(m, StateList) )
    })
  }
  
  Representations <- NULL; if(getRepresentations){
  # calculate output dimension based on temporalAggregation method
  baseRepWidth <- ifelse(optimizeImageRep, yes = nWidth_ImageRep,
                         no = nWidth_ImageRep)
  # for concatenate mode with video data, multiply by number of time periods
  if(dataType == "video" && temporalAggregation == "concatenate"){
    repWidth <- baseRepWidth * nTimePeriods
    message2(sprintf("Using concatenate temporal aggregation: %d time periods x %d features = %d total features",
                     nTimePeriods, baseRepWidth, repWidth))
  } else if(dataType == "video" && temporalAggregation == "difference"){
    # difference mode outputs mean level + mean change = 2 * baseRepWidth
    repWidth <- baseRepWidth * 2L
    message2(sprintf("Using difference temporal aggregation: %d features (level) + %d features (change) = %d total features",
                     baseRepWidth, baseRepWidth, repWidth))
  } else if(dataType == "video" && temporalAggregation == "variance"){
    # variance mode outputs mean + variance = 2 * baseRepWidth
    repWidth <- baseRepWidth * 2L
    message2(sprintf("Using variance temporal aggregation: %d features (mean) + %d features (variance) = %d total features",
                     baseRepWidth, baseRepWidth, repWidth))
  } else {
    repWidth <- baseRepWidth
  }
  Representations <- matrix(NA, nrow = length(unique(imageKeysOfUnits)), ncol = repWidth)
  usedImageKeys <- c(); last_i <- 0; ok_counter <- 0; ok<-F; while(!ok){
      ok_counter <- ok_counter + 1

      batch_indices <- (last_i+1):(last_i+batchSize)
      batch_indices <- batch_indices[batch_indices <= length(unique(imageKeysOfUnits))]
      last_i <- batch_indices[ length(batch_indices) ]

      # checks for last / batch size corrections
      if(last_i == length(unique(imageKeysOfUnits))){ ok <- T }
      batchSizeOneCorrection <- ifelse(length(batch_indices) == 1, yes = TRUE, no = FALSE)

      # get the data
      setwd(orig_wd); batch_inference <- try(GetElementFromTfRecordAtIndices( uniqueKeyIndices = batch_indices,
                                                            filename = file,
                                                            nObs = length(unique(imageKeysOfUnits)),
                                                            return_iterator = T,
                                                            readVideo = useVideo,
                                                            image_dtype = image_dtype_tf,
                                                            iterator = ifelse(ok_counter > 1,
                                                                              yes = list(saved_iterator),
                                                                              no = list(NULL))[[1]] ),T); setwd(new_wd)
      if('try-error' %in% class(batch_inference)){print(batch_inference); browser()}
      if(batchSizeOneCorrection){
          batch_indices <- c(batch_indices,batch_indices)
          batch_inference[[1]][[1]] <- cienv$tf$concat(list(cienv$tf$expand_dims(batch_inference[[1]][[1]],0L),
                                                      cienv$tf$expand_dims(batch_inference[[1]][[1]],0L)), 0L)
      }
      saved_iterator <- batch_inference[[2]]
      batch_inference <- batch_inference[[1]]
      batch_keys <- unlist(  lapply( p2l(batch_inference[[3]]$numpy()), as.character) )
      
      if (is.null(X)){ 
        X_batch <- cienv$jnp$ones(list(length(batch_indices),2L))
      }
      if (!is.null(X)){ 
        X_batch <- cienv$jnp$array(X[batch_indices, , drop = FALSE],
                            dtype = cienv$jnp$float16)
      }
      
      gc(); cienv$py_gc$collect() # collect memory
      # im <- cienv$jnp$array(batch_inference[[1]]); seed <- cienv$jax$random$key(ai(2L+ok_counter + seed)); inference = T
      # plot( cienv$np$array( cienv$jnp$array(batch_inference[[1]]))[,,1,3])# check variability across units
      # batch_inference[[1]][3,,,1] # check image inputs 
      #representation_ <-  cienv$np$array( ImageRepArm_batch(
      
      #dropout_layer(dropoutRate)(cienv$jnp$array(matrix(1:100.,nrow=10,ncol=10),dtype=cienv$jnp$float16),key = cienv$jax$random$key(10L), inference = FALSE) 
      #representation_ <-  cienv$np$array( ImageRepArm_batch_R(
      representation_ <-  try(cienv$np$array( ImageRepArm_batch(
                                                      ModelList,
                                                      InitImageProcess(cienv$jnp$array(batch_inference[[1]]),
                                                                       cienv$jax$random$key(ai(2L+ok_counter + seed)), inference = T, batch_indices = batch_indices),
                                                      X_batch,
                                                      
                                                      StateList,
                                                      cienv$jax$random$split(cienv$jax$random$key(ai(last_i + seed)), 
                                                                             cienv$jnp$array(batch_inference[[1]]$shape)[0]),
                                                      MPList, 
                                                      TRUE # inference for testing 
                                                      )[[1]]  ),T)
      if('try-error' %in% class(representation_)){print(representation_);browser()}
      
      # plot(representation_[,sample(1:20)]); hist(as.matrix(representation_)); apply(as.matrix(representation_),2,sd)
      if(T == F){ 
        # sanity checks
        tmp <- cienv$np$array(cienv$jnp$take(cienv$jnp$array(batch_inference[[1]]),(iCheck <- 8L)-1L,axis=0L))
        image2(tmp[,,1])
        cbind(lat[which(imageKeysOfUnits %in% batch_keys[ iCheck ])], long[which(imageKeysOfUnits %in% batch_keys[ iCheck ])])
      }
      if(any(is.na(representation_)) &  !initializingFxns){ 
        stop("Stopping due to missingness in intermediary representation_ [Code reference: ImageModelBackbone.R]") 
      }
      if("try-error" %in% class(representation_)){
        message2(representation_)
        stop("Stopping due to try-error in *intermediary* version of representation_ [Code reference: ImageModelBackbone.R]") 
      }
      
      if(batchSizeOneCorrection){ representation_ <- representation_[1,] }
      usedImageKeys <- c(usedImageKeys, batch_keys)
      Representations[batch_indices,] <- representation_
      if(last_i %% 100 == 0 | last_i < 10){ 
        message2(sprintf("%.2f%% done getting image/video representations", 100*last_i / length(unique(imageKeysOfUnits))))
      }
  }
  Representations <- Representations[match(as.character(imageKeysOfUnits), as.character(usedImageKeys) ),]
  message2("Done getting image/video representations!")
  }

  # reset wd (may have been changed via tfrecords use)
  setwd(  orig_wd  )

  message2("Obtaining approximate parameter count...")
  cienv$nParamsRep <- sum(unlist(lapply(cienv$jax$tree$leaves(cienv$eq$partition(ModelList, cienv$eq$is_array)[[1]]), function(zer){zer$size})))
  if(is.null(pretrainedModel) & optimizeImageRep == F){ cienv$nParamsRep <- cienv$nParamsRep }
  if(!is.null(pretrainedModel) ){
    if(dataType == "image"){ cienv$nParamsRep <- cienv$nParameters_Pretrained }
    if(dataType == "video" & !grepl(pretrainedModel,pattern="video")){ cienv$nParamsRep <- cienv$nParamsRep + cienv$nParameters_Pretrained }
    if(dataType == "video" & grepl(pretrainedModel,pattern="video")){ cienv$nParamsRep <- cienv$nParameters_Pretrained }
  }

  # sanity check 
  if(any(is.na(Representations)) & !initializingFxns){
    stop("Stopping due to missingness in *output* version of Representations [Code reference: ImageModelBackbone.R]")
  }
  
  if(CleanupEnv){
    suppressWarnings( try(rm(PretrainedImageModelName, FeatureExtractor, ClayModel, TransformersModel, cienv$nParameters_Pretrained, pos = .GlobalEnv),T) ) 
  }
  if(returnContents){
   print("Returning contents in GetImageRepresentations()!")
   return( list( "ImageRepresentations"= Representations,
                 "ImageRepArm_batch_R" = ImageRepArm_batch_R,
                 "ImageRepArm_batch" = ImageRepArm_batch,
                 "ImageModel_And_State_And_MPPolicy_List" = list(ModelList, StateList, MPList), 
                 "InitImageProcess" = InitImageProcess,
                 "nParamsRep" = cienv$nParamsRep
                 ) )
  }
  suppressWarnings(rm(ModelList, StateList, MPList)); gc()
}

