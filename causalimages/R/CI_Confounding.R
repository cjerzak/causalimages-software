#' Perform causal estimation under image confounding
#'
#' @param obsW A numeric vector where `0`'s correspond to control units and `1`'s to treated units.
#' @param obsY A numeric vector containing observed outcomes.
#' @param imageKeysOfUnits A vector of length `length(obsY)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into the package to call images into memory.
#' @param file Path to a tfrecord file generated by `WriteTfRecord`.
#' @param conda_env A `conda` environment where computational environment lives, usually created via `causalimages::BuildBackend()`. Default = `"CausalImagesEnv"`.
#' @param conda_env_required A Boolean stating whether use of the specified conda environment is required.
#' @param X An optional numeric matrix containing tabular information used if `orthogonalize = T`. `X` is normalized internally and salience maps with respect to `X` are transformed back to the original scale.
#' @param long,lat Optional vectors specifying longitude and latitude coordinates for units. Used only for describing highest and lowest probability neighborhood units if specified.
#' @param transportabilityMat Optional matrix with a column named `imageKeysOfUnits` specifying keys to be used by the package for generating treatment effect predictions for out-of-sample points.
#' @param figuresTag A string specifying an identifier that is appended to all figure names.
#' @param figuresPath A string specifying file path for saved figures made in the analysis.
#' @param plotBands An integer or vector specifying which band position (from the image representation) should be plotted in the visual results. If a vector, `plotBands` should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RGB plotting).
#' @param nSGD Number of stochastic gradient descent (SGD) iterations. Default = `400L`
#' @param nBoot Number of bootstrap iterations for uncertainty estimation.
#' @param batchSize Batch size used in SGD optimization. Default = `50L`.
#' @param useTrainingPertubations Boolean specifying whether to randomly perturb the image axes during training to reduce overfitting.
#' @param optimizeImageRep Boolean specifying whether to optimize over the image model representation (or only over downstream parameters).
#' @param dropoutRate Dropout rate used in training to prevent overfitting (`dropoutRate = 0` corresponds to no dropout).
#' @param testFrac Default = `0.1`. Fraction of observations held out as a test set to evaluate out-of-sample loss values.
#' @param strides (default = `2L`) Integer specifying the strides used in the convolutional layers.
#' @param plotResults (default = `T`) Should analysis results be plotted?
#' @param dataType (default = `"image"`) String specifying whether to assume `"image"` or `"video"` data types.
#' @param nWidth_ImageRep Integer specifying width of image model representation.
#' @param nDepth_ImageRep Integer specifying depth of image model representation.
#' @param nWidth_Dense Integer specifying width of image model representation.
#' @param nDepth_Dense Integer specifying depth of dense model representation.
#' @param kernelSize Dimensions used in spatial convolutions.
#' @param TfRecords_BufferScaler The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#'
#' @return Returns a list consisting of
#' \itemize{
#'   \item `ATE_est` ATE estimate.
#'   \item `ATE_se` Standard error estimate for the ATE.
#'   \item `plotResults` If set to `TRUE`, causal salience plots are saved to disk, characterizing the image confounding structure. See references for details.
#' }
#'
#' @section References:
#' \itemize{
#' \item  Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Integrating Earth Observation Data into Causal Inference: Challenges and Opportunities. *ArXiv Preprint*, 2023.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md

AnalyzeImageConfounding <- function(
                                   obsW,
                                   obsY,
                                   X = NULL,
                                   file = NULL,
                                   imageKeysOfUnits = NULL,
                                   fileTransport = NULL,
                                   imageKeysOfUnitsTransport = NULL,
                                   nBoot = 10L,
                                   inputAvePoolingSize = 1L,
                                   useTrainingPertubations = T,
                                   useScalePertubations = F,

                                   orthogonalize = F,
                                   transportabilityMat = NULL,
                                   latTransport = NULL,
                                   longTransport = NULL,
                                   lat = NULL,
                                   long = NULL,
                                   conda_env = "CausalImagesEnv",
                                   conda_env_required = T,
                                   Sys.setenv_text = NULL,

                                   figuresTag = NULL,
                                   figuresPath = "./",
                                   plotBands = 1L,
                                   plotResults = T,

                                   XCrossModal = T, 
                                   optimizeImageRep = T,
                                   nWidth_ImageRep = 64L,  nDepth_ImageRep = 1L, kernelSize = 5L,
                                   nWidth_Dense = 64L,  nDepth_Dense = 1L,
                                   imageModelClass = "VisionTransformer",
                                   pretrainedModel = NULL, 

                                   strides = 2L,
                                   nDepth_TemporalRep = 3L,
                                   patchEmbedDim = 16L,
                                   dropoutRate = 0.1,
                                   batchSize = 16L,
                                   nSGD  = 400L,
                                   testFrac = 0.05,
                                   TfRecords_BufferScaler = 4L,
                                   learningRateMax = 0.001,
                                   TFRecordControl = NULL, 
                                   dataType = "image",
                                   image_dtype = "float16",
                                   atError = "stop", # stop or debug
                                   seed = NULL){
  {
    if(!"jax" %in% ls(envir = cienv)) {
      initialize_jax(conda_env = conda_env, 
                     conda_env_required = conda_env_required,
                     Sys.setenv_text = Sys.setenv_text) 
    }
    message(sprintf("Default device: %s",cienv$jnp$array(0.)$devices()))

    # set float type
    library( tensorflow );
    if((image_dtype_char <- image_dtype) == "float16"){  image_dtype_tf <- cienv$tf$float16; ComputeDtype <- image_dtype <- cienv$jnp$float16 }
    if(image_dtype_char == "bfloat16"){  image_dtype_tf <- cienv$tf$bfloat16; ComputeDtype <- image_dtype <- cienv$jnp$bfloat16 }
    if(is.null(seed)){ seed <- ai(runif(1,1,10000)) }
    obsW <- f2n(obsW); obsY <- f2n(obsY)
    
    # set memory growth for tensorflow 
    for(device_ in cienv$tf$config$list_physical_devices()){
       try(cienv$tf$config$experimental$set_memory_growth(device_, T),T)
    }
  }

  message("Setting input types in AnalyzeImageConfounding()...") 
  if(!is.null(pretrainedModel)){ pretrainedModel <- as.character(pretrainedModel) } 
  if(!is.null(optimizeImageRep)){ optimizeImageRep <- as.logical(as.character(optimizeImageRep)) }
  if(!is.null(imageModelClass)){ imageModelClass <- as.character(imageModelClass) }
  if(!is.null(nWidth_ImageRep)){ nWidth_ImageRep <- as.integer(f2n(nWidth_ImageRep)) }
  
  # figure name info 
  FigNameAppend <- sprintf("KW%s_InputAvePool%s_OptimizeImageRep%s_Tag%s",
                           kernelSize, inputAvePoolingSize,
                           optimizeImageRep, figuresTag)
  tagInFigures <- !is.null(figuresTag)
  figuresTag < ifelse(is.null(figuresTag), yes = "", no = figuresTag)

  # make all directory logic explicit
  ImageRepresentations_df_transport <- ImageRepresentations_df <- myGlmnet_coefs <- loss_vec <- NULL
  orig_wd <- getwd()
  if( (cond1 <- substr(figuresPath, start = 0, stop = 1) == ".")  ){
    figuresPath <- gsub(figuresPath, pattern = '\\.', replacement = orig_wd)
  }
  if(!dir.exists(figuresPath)){ dir.create(figuresPath) }
  figuresPath <- paste(strsplit(figuresPath,split="/")[[1]],collapse = "/")
  if(batchSize > length(obsW)){ batchSize <- round(length(obsW) * 0.90) }

  XisNull <- is.null( X  )
  if(!XisNull){ if(!"matrix" %in% class(X)){
    message("Coercing X to matrix class..."); X <- as.matrix(  X )
  } }

  if( !XisNull ){ if(is.na(sum(X))){ stop("Error: is.na(sum(X)) is TRUE; check for NAs or that all variables are numeric.") }}
  if( !XisNull ){ if(any(apply(X,2,sd) == 0)){ stop("Error: any(apply(X,2,sd) == 0) is TRUE; a column in X seems to have no variance; drop column!") }}
  if( XisNull ){ X <- matrix( rnorm(length(obsW)*2, sd = 0.01 ), ncol = 2) }
  X <- t( (t(X) - (X_mean <- colMeans(X)) ) / (0.001+(X_sd <- apply(X,2,sd))) )
  {
    if(is.null(file)){stop("No file specified for tfrecord!")}
    changed_wd <- F; if(  !is.null(  file  )  ){
      message("Establishing connection with tfrecord")
      tf_record_name <- file
      if( !grepl(tf_record_name, pattern = "/") ){
        tf_record_name <- paste("./",tf_record_name, sep = "")
      }
      tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
      new_wd <- paste(tf_record_name[-length(tf_record_name)], collapse = "/")
      message(sprintf("Temporarily re-setting the wd to %s", new_wd ) )
      changed_wd <- T; setwd( new_wd )

      # define video indicator 
      useVideoIndicator <- dataType == "video"
      
      # define tf record 
      tf_dataset <- cienv$tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )
      
      # helper functions
      getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, 
                                                                 readVideo = useVideoIndicator, 
                                                                 image_dtype = image_dtype_tf)} )
        return( dataset <- dataset$batch( ai(max(2L,round(batchSize/2L)  ))) )
      }

      message("Setting up iterators...") # - skip the first test_size observations 
      if(!is.null(TFRecordControl)){
        getParsed_tf_dataset_train_Select <- function( tf_dataset ){
          return( tf_dataset$map( function(x){ parse_tfr_element(x, 
                                                                 readVideo = useVideoIndicator, 
                                                                 image_dtype = image_dtype_tf)},
                                     num_parallel_calls = cienv$tf$data$AUTOTUNE) ) 
        }
        getParsed_tf_dataset_train_BatchAndShuffle <- function( tf_dataset ){
          tf_dataset <- tf_dataset$shuffle(buffer_size = cienv$tf$constant(ai(TfRecords_BufferScaler*batchSize),
                                                                           dtype=cienv$tf$int64),
                                     reshuffle_each_iteration = T) 
          tf_dataset <- tf_dataset$batch(  ai(batchSize)   )
          tf_dataset <- tf_dataset$prefetch( cienv$tf$data$AUTOTUNE ) 
          return( tf_dataset )
        }
        tf_dataset_train_control <- getParsed_tf_dataset_train_Select(
                            tf_dataset$skip(  test_size <-  ai( TFRecordControl$nTest)  )
                            )$take( ai(TFRecordControl$nControl) )$`repeat`(-1L)  
        tf_dataset_train_treated <- getParsed_tf_dataset_train_Select(
                              tf_dataset$skip( test_size <-  ai( TFRecordControl$nTest) )
                              )$skip( ai(TFRecordControl$nControl)+1L)$`repeat`(-1L) 
        tf_dataset_train_treated <- getParsed_tf_dataset_train_BatchAndShuffle(tf_dataset_train_treated)
        tf_dataset_train_control <- getParsed_tf_dataset_train_BatchAndShuffle(tf_dataset_train_control)
        ds_iterator_train_treated <- reticulate::as_iterator( tf_dataset_train_treated )
        ds_iterator_train_control <- reticulate::as_iterator( tf_dataset_train_control )
        ds_iterator_train <- reticulate::as_iterator( tf_dataset_train_control )
      }
      if(is.null(TFRecordControl)){
        getParsed_tf_dataset_train <- function( tf_dataset ){
          dataset <- tf_dataset$map( function(x){ parse_tfr_element(x, readVideo = useVideoIndicator, image_dtype = image_dtype_tf)},
                                     num_parallel_calls = cienv$tf$data$AUTOTUNE)
          dataset <- dataset$shuffle(buffer_size = cienv$tf$constant(ai(TfRecords_BufferScaler*batchSize), dtype=cienv$tf$int64),
                                     reshuffle_each_iteration = FALSE) # set FALSE so same train/test split each re-initialization
          dataset <- dataset$batch(  ai(batchSize)   )
          dataset <- dataset$prefetch( cienv$tf$data$AUTOTUNE ) 
          return( dataset  )
      }
        
        # shuffle (generating different train/test splits)
        tf_dataset <- cienv$tf$data$Dataset$shuffle(  tf_dataset, 
                                                      buffer_size = cienv$tf$constant(ai(10L*TfRecords_BufferScaler*batchSize),
                                                                                      dtype=cienv$tf$int64), reshuffle_each_iteration = F )
        tf_dataset_train <- getParsed_tf_dataset_train( 
                    tf_dataset$skip(test_size <-  as.integer(round(testFrac * length(unique(imageKeysOfUnits)) )) ) )$`repeat`(  -1L )
        ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
      }
      # define inference iterator 
      tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )
      ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )
    }

    if(useTrainingPertubations){
      trainingPertubations <- cienv$jax$vmap( 
        trainingPertubations_OneObs <- function(im_, key){
         # key <- cienv$jax$random$PRNGKey(c(sample(1:100,1)))
         AB <- ifelse(dataType == "video", yes = 1L, no = 0L)
         which_path <- cienv$jnp$squeeze(cienv$jax$random$categorical(key = key, logits = cienv$jnp$array(t(rep(0, times = 4)))),0L)# generates random # from 0L to 3L
         
         # which_path of 0L -> do no flips 
         im_ <- cienv$jax$lax$cond(cienv$jnp$equal(which_path,cienv$jnp$array(1L)),
                                   true_fun = function(){ cienv$jnp$flip(im_, 
                                                                         axis = AB+0L) }, 
                                   false_fun = function(){im_})
         im_ <- cienv$jax$lax$cond(cienv$jnp$equal(which_path,cienv$jnp$array(2L)), 
                                   true_fun = function(){ cienv$jnp$flip(im_, 
                                                                         axis = AB+1L) }, 
                                   false_fun = function(){im_})
         im_ <- cienv$jax$lax$cond(cienv$jnp$equal(which_path,cienv$jnp$array(3L)),
                                   true_fun = function(){ cienv$jnp$flip(cienv$jnp$flip(im_, 
                                                                                        axis = AB+0L),
                                                                         axis = AB+1L) }, 
                                   false_fun = function(){im_})
         return( im_ ) }, in_axes = list(0L,0L))
    }

    InitImageProcessFn <- cienv$jax$jit(function(im, key, inference){
        # expand dims if needed
        if(length(imageKeysOfUnits) == 1){ im <- cienv$jnp$expand_dims(im,0L) }

        # normalize
        im <- (im - NORM_MEAN_array) / NORM_SD_array

        # downshift resolution if desired
        if(inputAvePoolingSize > 1 & dataType == "image"){
          im <- cienv$jax$vmap(function(imm){
            cienv$jnp$transpose(  cienv$eq$nn$AvgPool2d(kernel_size = ai(c(inputAvePoolingSize,inputAvePoolingSize)),
                            stride = ai(c(inputAvePoolingSize,inputAvePoolingSize)))(
                          cienv$jnp$transpose(imm,c(2L,0L,1L)  )), c(1L,2L, 0L)) }, 0L)(im)
        }
        
        # training pertubations
        if(useTrainingPertubations){
          im <- cienv$jax$lax$cond(inference, true_fun = function(){ im }, 
                                              false_fun = function(){  trainingPertubations(im, 
                                                              cienv$jax$random$split(key,im$shape[[1]])) } )
        }
        if(useScalePertubations){
          im <- cienv$jax$lax$cond(inference, true_fun = function(){ im }, 
                                              false_fun = function(){  scalePertubations(im, 
                                                                           cienv$jax$random$split(key,im$shape[[1]])) } )
        }
        return( im  )
    })

    message("Calibrating first moments for input data normalization...")
    #if(grepl(file,pattern="imSeq")){browser()}
    # Input to reshape is a tensor with 58800 values, but the requested shape has 19600
    NORM_MEAN_array <- GetMoments(ds_iterator_train, 
                                  dataType = dataType, 
                                  image_dtype = image_dtype, 
                                  momentCalIters = 34)
    NORM_SD <- NORM_MEAN_array$NORM_SD; NORM_SD_array <- NORM_MEAN_array$NORM_SD_array
    NORM_MEAN <- NORM_MEAN_array$NORM_MEAN; NORM_MEAN_array <- NORM_MEAN_array$NORM_MEAN_array
    EP_LSMOOTH <- cienv$jnp$array( 0.05 )
    cienv$py_gc$collect()

    # set up holders
    sigmoid <- function(x){1/(1+exp(-x))}
    prW_est <- rep(NA,times = length(obsW))
    tauHat_propensity_vec <- tauHat_propensityHajek_vec <- rep(NA,times = nBoot+1)
    if(!optimizeImageRep){
      message("Note: Not optimizing image/video representation...")
      message("Defining train/test indices based on out of sample keys...")
      imageKeysByTreatment <- tapply(obsW, imageKeysOfUnits, mean)
      outKeys <- try(c(sample(names(imageKeysByTreatment[imageKeysByTreatment > 0.5]), 
                          floor(max(c(2,length(unique(imageKeysOfUnits))*testFrac)) / 2)
                          ), 
                       sample(names(imageKeysByTreatment[imageKeysByTreatment <= 0.5]), 
                          floor(max(c(2,length(unique(imageKeysOfUnits))*testFrac)) / 2))
                       ), T)
      if("try-error" %in% class(outKeys)){ 
        outKeys <- sample(unique(imageKeysOfUnits), 
                          floor(max(c(2,length(unique(imageKeysOfUnits))*testFrac)))
                          ) 
      }
      inKeys <- unique(imageKeysOfUnits[!imageKeysOfUnits %in% outKeys])
      testIndices <- (1:length(obsY))[imageKeysOfUnits %in% outKeys]
      trainIndices <- (1:length(obsY))[imageKeysOfUnits %in% inKeys]

      message("Starting generation of image/video representation + outcome prediction [bootstrap done for uncertainty estimation]...")
      for(jr in 1L:(nBoot+1L)){
        if(nBoot > 0L){ message(sprintf("Bootstrap iteration %s of %s", jr-1L, nBoot) ) } 
        if(jr != (nBoot+1L)){ bindices_ <- sample(1:length( imageKeysOfUnits ), length( imageKeysOfUnits ), replace = T) }
        if(jr == (nBoot+1L)){ bindices_ <- 1:length( imageKeysOfUnits ) }

        # note: MyEmbeds_ are indexed by the original data ordering, resampling happens later
        {
          setwd(orig_wd); ImageRepresentations <- GetImageRepresentations(
            X = X,
            file = file,
            dataType = dataType,
            InitImageProcess = InitImageProcessFn,
            NORM_MEAN = NORM_MEAN, 
            NORM_SD = NORM_SD, 
            nWidth_ImageRep = nWidth_ImageRep,
            nDepth_ImageRep = nDepth_ImageRep,
            strides = strides,
            dropoutRate = 0,
            nDepth_TemporalRep = nDepth_TemporalRep,
            patchEmbedDim = patchEmbedDim,
            batchSize = batchSize,
            imageModelClass = imageModelClass,
            pretrainedModel = pretrainedModel, 
            optimizeImageRep = optimizeImageRep, 
            kernelSize = kernelSize,
            TfRecords_BufferScaler = 3L,
            XCrossModal = XCrossModal,
            inputAvePoolingSize = inputAvePoolingSize,
            lat = lat[!duplicated(imageKeysOfUnits)], 
            long = long[!duplicated(imageKeysOfUnits)], 
            imageKeysOfUnits = imageKeysOfUnits[!duplicated(imageKeysOfUnits)],
            getRepresentations = T,
            initializingFxns = F,
            returnContents = T,
            bn_momentum = 0.99,
            conda_env = conda_env,
            conda_env_required = conda_env_required,
            Sys.setenv_text = Sys.setenv_text,
            seed = ai(400L + jr)  ); setwd(new_wd)
          ImageRepresentations_df <- as.data.frame(  ImageRepresentations$ImageRepresentations )
          row.names(ImageRepresentations_df) <- as.character(unique(imageKeysOfUnits))
          ImageRepresentations_df <- ImageRepresentations_df[as.character(imageKeysOfUnits),]
       }
        # subset indices for training
        indices_forTraining <- bindices_[bindices_ %in% trainIndices]
        glmnetInput <- ifelse(XisNull, yes = list(ImageRepresentations_df),
                                       no = list(cbind(as.matrix(X), ImageRepresentations_df)))[[1]]
        if(any(is.na(glmnetInput))){ stop("Stopping due to NA in glmnetInput [Code ref: Confounding.R]") }
        myGlmnet_ <- glmnet::cv.glmnet(
          x = as.matrix(glmnetInput[indices_forTraining,]),
          y = as.matrix(obsW[indices_forTraining]),
          nfolds = 5,
          alpha = 0, # alpha = 0 is the ridge penalty
          type.measure = ifelse(length(unique(obsW))==2,yes="auc",no="default"),
          family = ifelse(length(unique(obsW))==2,yes="binomial",no="gaussian")
          )
        obsW_ <- obsW[bindices_]; obsY_ <- obsY[bindices_]
        prW_est_ <- predict(myGlmnet_, 
                            s = "lambda.min",
                            newx = as.matrix(glmnetInput[bindices_,]), 
                            type = "response")
        # plot(obsW_, c(prW_est_)); cor(obsW_, c(prW_est_)); tapply(prW_est_, obsW_, mean)
        # plot(obsW[indices_forTraining], c(predict(myGlmnet_, s ="lambda.min",newx = as.matrix(glmnetInput[indices_forTraining,]), type = "response")))

        # compute QOIs
        myGlmnet_coefs_ <- as.matrix( glmnet::coef.glmnet(myGlmnet_, s = "lambda.min") )
        tauHat_propensity_vec[jr] <- tauHat_propensity_ <- mean(  obsW_*obsY_/c(prW_est_) - 
                                                            (1-obsW_)*obsY_/c(1-prW_est_) )
        tauHat_propensityHajek_vec[jr] <- tauHat_propensityHajek_ <- sum(  obsY_*prop.table(obsW_/c(prW_est_))) -
                                            sum(obsY_*prop.table((1-obsW_)/c(1-prW_est_) ))
        if(jr == 1){ myGlmnet_coefs_mat <- matrix(NA, nrow = nBoot+1, ncol = length(myGlmnet_coefs_)) }
        myGlmnet_coefs_mat[jr,] <- c(myGlmnet_coefs_)
        if(jr == (nBoot+1L)){
          nTrainable <- length(  myGlmnet_coefs_  )
          tauHat_propensityHajek <- tauHat_propensityHajek_
          tauHat_propensity <- tauHat_propensity_
          myGlmnet_coefs <- myGlmnet_coefs_
          prW_est <- prW_est_
          GetTreatProb_batch <- function( ModelList, ModelList_fixed,
                                          m, x, vseed,
                                          StateList, seed, MPList, inference){
            ImageReps <- ImageRepArm_batch_R(ModelList_fixed, m, x,
                                             StateList, seed, MPList, inference)
            if(!XisNull){
              if(XCrossModal & optimizeImageRep){ 
                x_m <- cienv$jnp$concatenate(list( cienv$jnp$ones(list(m$shape[[1]],1L)), ImageReps[[1]] ), 1L)
              }
              if(!XCrossModal | !optimizeImageRep){ 
                x_m <- cienv$jnp$concatenate(list( cienv$jnp$ones(list(m$shape[[1]],1L)), x, ImageReps[[1]] ), 1L)
              }
            }
            if(XisNull){
              x_m <- cienv$jnp$concatenate(list( cienv$jnp$ones(list(m$shape[[1]],1L)), ImageReps[[1]] ), 1L)
            }
            my_probs <- cienv$jax$nn$sigmoid(  cienv$jnp$matmul(x_m, ModelList$myGlmnet_coefs_tf ) )
            my_probs <- (1. - EP_LSMOOTH) * my_probs + EP_LSMOOTH/2.
            return( list(my_probs, StateList) )
          }
          ModelList <- list("myGlmnet_coefs_tf" = cienv$jnp$array(myGlmnet_coefs, dtype = cienv$jnp$float32))
          ModelList_fixed <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]][[1]]
          StateList <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]][[2]]
          MPList <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]][[3]]
          ImageRepArm_batch_R <- ImageRepresentations$ImageRepArm_batch_R
          InitImageProcessFn <-  ImageRepresentations[["InitImageProcess"]]
          nParamsRep <- ImageRepresentations$nParamsRep
          
          if(!is.null(fileTransport)){
            setwd(orig_wd); ImageRepresentations_df_transport <- GetImageRepresentations(
              X = X,
              file = fileTransport,
              dataType = dataType,
              InitImageProcess = InitImageProcessFn,
              NORM_MEAN = NORM_MEAN, 
              NORM_SD = NORM_SD, 
              nWidth_ImageRep = nWidth_ImageRep,
              nDepth_ImageRep = nDepth_ImageRep,
              strides = strides,
              dropoutRate = 0,
              nDepth_TemporalRep = nDepth_TemporalRep,
              patchEmbedDim = patchEmbedDim,
              batchSize = batchSize,
              imageModelClass = imageModelClass,
              pretrainedModel = pretrainedModel, 
              optimizeImageRep = optimizeImageRep, 
              kernelSize = kernelSize,
              TfRecords_BufferScaler = 3L,
              XCrossModal = XCrossModal,
              inputAvePoolingSize = inputAvePoolingSize,
              lat = latTransport[!duplicated(imageKeysOfUnitsTransport)], 
              long = longTransport[!duplicated(imageKeysOfUnitsTransport)], 
              imageKeysOfUnits = imageKeysOfUnitsTransport[!duplicated(imageKeysOfUnitsTransport)], 
              getRepresentations = T,
              initializingFxns = F, 
              returnContents = T,
              bn_momentum = 0.99,
              conda_env = conda_env,
              conda_env_required = conda_env_required,
              Sys.setenv_text = Sys.setenv_text,
              seed = ai(400L + jr)  ); setwd(new_wd)
            ImageRepresentations_df_transport <- as.data.frame(  ImageRepresentations_df_transport$ImageRepresentations )
          }
        }
      }
    }

    if(optimizeImageRep){
      setwd(orig_wd); ImageRepresentations <- GetImageRepresentations(
        X = X,
        file = file,
        dataType = dataType,
        InitImageProcess = InitImageProcessFn,
        NORM_MEAN = NORM_MEAN, 
        NORM_SD = NORM_SD, 
        nWidth_ImageRep = nWidth_ImageRep,
        nDepth_ImageRep = nDepth_ImageRep,
        strides = strides,
        dropoutRate = dropoutRate,
        nDepth_TemporalRep = nDepth_TemporalRep,
        patchEmbedDim = patchEmbedDim,
        batchSize = batchSize,
        imageModelClass = imageModelClass,
        pretrainedModel = pretrainedModel, 
        optimizeImageRep = optimizeImageRep, 
        kernelSize = kernelSize,
        inputAvePoolingSize = inputAvePoolingSize,
        TfRecords_BufferScaler = 3L,
        XCrossModal = XCrossModal,
        imageKeysOfUnits = (UsedKeys <- sample(unique(imageKeysOfUnits),min(c(length(unique(imageKeysOfUnits)),2*batchSize)))), getRepresentations = T,
        returnContents = T,
        initializingFxns = T, 
        bn_momentum = 0.99,
        conda_env = conda_env,
        conda_env_required = conda_env_required,
        Sys.setenv_text = Sys.setenv_text,
        seed = ai(4003L + seed)  ); setwd(new_wd)
        ImageModel_And_State_And_MPPolicy_List <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]]
        ImageRepArm_batch_R <- ImageRepresentations[["ImageRepArm_batch_R"]]
        InitImageProcessFn <-  ImageRepresentations[["InitImageProcess"]]
        rm( ImageRepresentations )

        batch_axis_name <- "batch"
        DenseList <- DenseStateList <- replicate(nDepth_Dense, list())
        for(d_ in 1L:nDepth_Dense){
          DenseProj_d <- cienv$eq$nn$Linear(in_features = ind_ <- ifelse(d_ == 1, 
                                                                         yes = (nWidth_ImageRep + ifelse(XisNull, no = ncol(X)*(!XCrossModal), yes = 0L)),
                                                                         no =  nWidth_Dense),
                                      out_features = outd_ <- ifelse(d_ == nDepth_Dense,
                                                                     yes = 1L,  no = nWidth_Dense),
                                      use_bias = T, key = cienv$jax$random$PRNGKey(d_ + 44L + as.integer(seed)))
          #LayerBN_d  <- cienv$eq$nn$BatchNorm( input_size = outd_, axis_name = batch_axis_name, momentum = 0.99, eps = 0.001, channelwise_affine = F)
          LayerBN_d <- cienv$jnp$array(1)
          DenseStateList[[d_]] <- list('BNState' = cienv$eq$nn$State( LayerBN_d ))
          DenseList[[d_]] <- list("DenseProj" = DenseProj_d,
                                  "BN" = LayerBN_d)
        }
        names(DenseList) <- names(DenseStateList) <- paste0("Dense", 1:nDepth_Dense)

        # ModelList <- DenseList; StateList <- DenseStateList
        GetDense_OneObs <- function(ModelList, ModelList_fixed, m, x,
                                    vseed, StateList, seed, MPList, inference){
          message("Starting GetDense_OneObs()")
          
          if(!XCrossModal){
            if(!XisNull){  m <- cienv$jnp$concatenate(list(m,x))  }
          }

          for(d__ in 1:nDepth_Dense){
            eval(parse(text = sprintf("DenseList_d <- ModelList$DenseList$Dense%s",d__)))
            eval(parse(text = sprintf("StateDenseList_d <- StateList$DenseStateList$Dense%s",d__)))
            
            # View(cienv$np$array(m$val))
            m <- DenseList_d$DenseProj(  m  )

            # BN + non-linearity
            if(d__ < nDepth_Dense){
              m <- DenseList_d$BN(m, state = StateDenseList_d, inference = inference)
              #StateIndex <- LE_index(StateList, sprintf("BNState_d%s", d__))
              #StateIndex <- paste(sapply(StateIndex, function(zer){ paste("[[", zer, "]]") }), collapse = "")
              #eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex)))
              eval(parse(text = sprintf("StateList$DenseList$Dense%s <- m[[2]]", d__)))
              m <- m[[1]]

              # Non-linearity
              m <- cienv$jax$nn$swish(  m   )
            }
          }
          
          message("Returning output and state in GetDense_OneObs()...")
          return( list(m, StateList)  )
        }
        GetDense_batch <- cienv$jax$vmap(  function(
                  ModelList, ModelList_fixed,
                  m, x, vseed,
                  StateList, seed, MPList, inference){
                    GetDense_OneObs(ModelList, ModelList_fixed, m, x, vseed, StateList, seed, MPList, inference)
                },
                in_axes = list(NULL, NULL, 0L, 0L, 0L, NULL, NULL, NULL, NULL),
                   axis_name = batch_axis_name,
                   out_axes = list(0L, NULL) )
        GetDense_batch_jit <- cienv$eq$filter_jit(   GetDense_batch  )
        GetTreatProb_batch <- function( ModelList, ModelList_fixed,
                                        m, x, vseed,
                                        StateList, seed, MPList, inference){
          message("In GetTreatProb_batch() - image model")
          m <- ImageRepArm_batch_R(ModelList, m, x,
                                   StateList, seed, MPList, inference)
          StateList <- m[[2]]; m <- m[[1]]

          message("In GetTreatProb_batch() - dense model")
          m <- GetDense_batch(ModelList, ModelList_fixed, m, x, vseed, StateList, seed, MPList, inference)
          StateList <- m[[2]]; m <- m[[1]]
          
          # sigmoid & label smoothing to prevent NAs via log(0)
          m <- (1. - EP_LSMOOTH) * cienv$jax$nn$sigmoid( m ) + EP_LSMOOTH/2.

          # return contents
          return( list(m, StateList) )
        }

        GetLoss <-  function( ModelList, ModelList_fixed,
                              m, x, treat, y, vseed,
                              StateList, seed, MPList, inference ){
          ModelList <- MPList[[1]]$cast_to_compute( ModelList ) # compute to output dtype
          ModelList_fixed <- MPList[[1]]$cast_to_compute( ModelList_fixed ) # compute to output dtype
          StateList <- MPList[[1]]$cast_to_compute( StateList ) # compute to output dtype

          m <- GetTreatProb_batch( ModelList, ModelList_fixed,
                                   m, x, vseed,
                                   StateList, seed, MPList, inference )
          StateList <- m[[2]]; m <-  m[[1]]

          # compute negative log-likelihood loss
          m <- MPList[[1]]$cast_to_output( m )
          NegLL <-  cienv$jnp$mean( cienv$jnp$negative(  treat*cienv$jnp$log(m) +  (1-treat)*cienv$jnp$log(1-m) )  ) 

          message("Returning loss + state...")
          if(image_dtype_char == "float16"){ 
            NegLL <- MPList[[1]]$cast_to_output( NegLL ) # compute to output dtype
            NegLL <- MPList[[2]]$scale( NegLL ) # scale loss
            StateList <- MPList[[1]]$cast_to_param( StateList ) # compute to param dtype
          }

          # return
          return( list(NegLL, StateList)  )
        }

        gc(); cienv$py_gc$collect()
        message("Set state and model lists..." ) 
        GradAndLossAndAux <-  cienv$eq$filter_jit( cienv$eq$filter_value_and_grad( GetLoss, has_aux = T) )
        ModelList <- c(ImageModel_And_State_And_MPPolicy_List[[1]], "DenseList" = list(DenseList))
        StateList <- c(ImageModel_And_State_And_MPPolicy_List[[2]], "DenseStateList" = list(DenseStateList))
        ModelList_fixed <- cienv$jnp$array(0.)
        MPList <- list(cienv$jmp$Policy(compute_dtype=ComputeDtype, 
                                  param_dtype="float32", 
                                  output_dtype=(outputDtype <- ComputeDtype)),
                       cienv$jmp$DynamicLossScale(loss_scale = cienv$jnp$array(2^15,dtype = ComputeDtype ),
                                            min_loss_scale = cienv$jnp$array(2^1.,dtype = ComputeDtype ),
                                            period = 50L))
        ModelList <- MPList[[1]]$cast_to_param( ModelList )
        ModelList_fixed <- MPList[[1]]$cast_to_param( ModelList_fixed )
        rm( ImageModel_And_State_And_MPPolicy_List, DenseStateList, DenseList )
        
        message("Define trainer...")
        LocalFxnSource(TrainDefine, evaluation_environment = environment())
        
        message("Starting training...")
        LocalFxnSource(TrainDo, evaluation_environment = environment())
      
        message("Getting predicted quantities...")
        GetImageArm_OneX <- cienv$eq$filter_jit( function(ModelList, ModelList_fixed,
                 m, x, vseed,
                 StateList, seed, MPList){
          # image representation model
          m <- ImageRepArm_batch_R(ModelList, m, x, 
                                   StateList, seed, MPList, T)
          StateList <- m[[2]] ; m <- m[[1]]
          
          # sigmoid 
          m <- cienv$jax$nn$sigmoid( m )
          
          # label smoothing to prevent NAs via log(0)
          m <- (1. - EP_LSMOOTH) * m + EP_LSMOOTH/2.
          
          return( m )
        })

        inference_counter <- 0; nUniqueKeys <- length( unique(imageKeysOfUnits) )
        KeyQuantCuts <- 1L:nUniqueKeys
        passedIterator <- NULL; Results_by_keys <- replicate(length(unique(KeyQuantCuts)),list());
        ImageRepArm_batch_jit <- cienv$eq$filter_jit( ImageRepArm_batch_R )
        pb <- txtProgressBar(min = 0, max = nUniqueKeys, style = 3)  # Initialize progress bar
        usedKeys <- c(); for(cut_ in unique(KeyQuantCuts)){ 
          # cut_ <- unique(KeyQuantCuts)[1]
          inference_counter <- inference_counter + 1
          zer <- which(cut_  ==  KeyQuantCuts)
          #gc(); cienv$py_gc$collect()
          atP <- max(zer)/nUniqueKeys
          if( any(zer %% 10 == 0) | 1 %in% zer ){ setTxtProgressBar(pb, max(zer)) }
          {
            setwd(orig_wd); ds_next_in <- GetElementFromTfRecordAtIndices(
                                                uniqueKeyIndices = which(unique(imageKeysOfUnits) %in% unique(imageKeysOfUnits)[zer]),
                                                filename = file,
                                                iterator = passedIterator,
                                                readVideo = useVideoIndicator,
                                                image_dtype = image_dtype_tf,
                                                nObs = length(unique(imageKeysOfUnits)),
                                                return_iterator = T ); setwd(new_wd)
            passedIterator <- ds_next_in[[2]]
            key_ <- unlist(  lapply( p2l(ds_next_in[[1]][[3]]$numpy() ), as.character) )
            ds_next_in <-  cienv$jnp$array( ds_next_in[[1]][[1]] )

            # deal with batch 1 case here
            if(length(ds_next_in$shape) == 3 & dataType == "image"){ ds_next_in <- cienv$jnp$expand_dims(ds_next_in, 0L) }
            if(length(ds_next_in$shape) == 4 & dataType == "video"){ ds_next_in <- cienv$jnp$expand_dims(ds_next_in, 0L) }
          }
          
          # get summaries and save
          usedKeys <- c(usedKeys, key_)
          obs_with_key <- which(imageKeysOfUnits %in% key_)
          x <- cienv$jnp$expand_dims(cienv$jnp$array(  ifelse(length(obs_with_key) == 1, 
                                                              yes = list(t(X[obs_with_key,])),
                                                              no = list(X[obs_with_key,]))[[1]],
                           dtype = cienv$jnp$float16), 0L)$transpose( c(1L, 0L, 2L) )
          m_ImageRep <- ImageRepArm_batch_jit(ifelse(optimizeImageRep, yes = list(ModelList), no = list(ModelList_fixed) )[[1]],
                                          InitImageProcessFn(cienv$jnp$array(ds_next_in), cienv$jax$random$PRNGKey(600L+cut_), inference = T), # m 
                                      cienv$jnp$expand_dims(cienv$jnp$squeeze(x,1L)$take(0L,0L),0L), # x
                                          StateList, cienv$jax$random$PRNGKey(900L+cut_), MPList, T)[[1]]
          GottenSummaries <- sapply(1L:ifelse(XisNull, yes = 1L, no = x$shape[[1]]), function(r_){
            m <- GetDense_batch_jit(ModelList, ModelList_fixed,
                                m_ImageRep,
                                x[r_-1L,],
                                cienv$jax$random$split(cienv$jax$random$PRNGKey(as.integer(runif(1,0, 10000))), ds_next_in$shape[[1]]),
                                StateList,
                                cienv$jax$random$PRNGKey(as.integer(runif(1,0,100000))),
                                MPList, T)[[1]]
            m <- cienv$jax$nn$sigmoid( m )
            m <- (1. - EP_LSMOOTH) * m + EP_LSMOOTH/2.
            if(XisNull){m <- list(replicate(m, n = x$shape[[1]]))}
            return( m )
          })
          GottenSummaries <- as.matrix(cienv$np$array(cienv$jnp$concatenate(unlist(GottenSummaries),0L)))
          ret_list <- list("ProbW" = GottenSummaries,
                           "obsIndex" = as.matrix(obs_with_key),
                           "key" = as.matrix( rep(key_, length(obs_with_key)) ))
          Results_by_keys[[inference_counter]] <- ret_list
        }
        close(pb)  # Close the progress bar after the loop
        Results_by_keys <- as.data.frame(
                        apply(do.call(rbind, Results_by_keys),2,function(zer){(do.call(rbind,zer))}))
        prW_est <-  Results_by_keys$ProbW <-  f2n(  Results_by_keys$ProbW ) 
        if(any(is.na(prW_est))){
          warning("NAs in output probabilities...Imputing them with average estimate")
          prW_est[is.na(prW_est)] <- mean(prW_est, na.rm = T)
        }
        
        # sanity checks
        # usedKeys  == unique(imageKeysOfUnits)
        # unlist(Results_by_keys[["key"]]) == unique(imageKeysOfUnits)
        # mean(unlist(Results_by_keys[["key"]]) %in% imageKeysOfUnits)
        # mean(imageKeysOfUnits %in% unlist(  Results_by_keys[["key"]] ))
        trainIndices <- which( imageKeysOfUnits %in% keysUsedInTraining )
        testIndices <- which( !imageKeysOfUnits %in% keysUsedInTraining )
        tauHat_propensityHajek <- sum(  obsY*prop.table(obsW/c(prW_est))) - sum(obsY*prop.table((1-obsW)/c(1-prW_est) ))
        tauHat_propensityHajek_vec <- unlist(replicate(nBoot, { i_ <- sample(1:length(obsW),length(obsW), T)
                        sum(  obsY[i_]*prop.table(obsW[i_]/c(prW_est[i_]))) -
                          sum(obsY[i_]*prop.table((1-obsW)[i_]/(1-prW_est)[i_] )) } ))
    }

    # process in and out of sample losses
    prWEst_baseline <- prW_est 
    prWEst_baseline[] <- mean( obsW[trainIndices] )
    
    # cross entropy loss calcs
    binaryCrossLoss <- function(W,prW){ return( - mean( log(prW)*W + log(1-prW)*(1-W) ) ) }
    lossCE_OUT_baseline <- binaryCrossLoss(obsW[testIndices], prWEst_baseline[testIndices])
    lossCE_IN_baseline <- binaryCrossLoss(obsW[trainIndices], prWEst_baseline[trainIndices])
    lossCE_OUT <-  binaryCrossLoss(  obsW[testIndices], prW_est[testIndices]  )
    lossCE_IN <-  binaryCrossLoss(  obsW[trainIndices], prW_est[trainIndices]  )

    # class error calcs
    lossClassError_OUT_baseline <- 1/length(testIndices) * (sum( prWEst_baseline[testIndices][ obsW[testIndices] == 1] < 0.5) +
                           sum( prWEst_baseline[testIndices][ obsW[testIndices] == 0] > 0.5))
    lossClassError_IN_baseline <- 1/length(trainIndices) * (sum( prWEst_baseline[trainIndices][ obsW[trainIndices] == 1] < 0.5) +
                                                              sum( prWEst_baseline[trainIndices][ obsW[trainIndices] == 0] > 0.5))
    lossClassError_OUT <- 1/length(testIndices) * (sum( prW_est[testIndices][ obsW[testIndices] == 1] < 0.5) +
                           sum( prW_est[testIndices][ obsW[testIndices] == 0] > 0.5))
    lossClassError_IN <- 1/length(trainIndices) * (sum( prW_est[trainIndices][ obsW[trainIndices] == 1] < 0.5) +
                                                     sum( prW_est[trainIndices][ obsW[trainIndices] == 0] > 0.5))
    
    # store output
    ModelEvaluationMetrics <- list(
      "CELoss_out" = lossCE_OUT,
      "CELoss_out_baseline" = lossCE_OUT_baseline,
      "CELoss_in" = lossCE_IN,
      "CELoss_in_baseline" = lossCE_IN_baseline,
      "ClassError_out" = lossClassError_OUT,
      "ClassError_out_baseline" = lossClassError_OUT_baseline,
      "ClassError_in" = lossClassError_IN,
      "ClassError_in_baseline" = lossClassError_IN_baseline
    )

    # reset to original wd which was altered during records initialization
    # do this before plotting to avoid disrupting plot save locations
    if( changed_wd ){ setwd(  orig_wd  ) }

    # do some analysis with examples
    processedDims <- NULL; if( plotResults ){
      message("Plotting image confounding results...")
      indices_t <- (1:length(obsW))[which(obsW==1)]
      indices_c <- (1:length(obsW))[which(obsW==0)]

      showPerGroup <- min(c(3,unlist(table(obsW))), na.rm = T)
      top_control <- ordered_control <- indices_c[order_c <- order(prW_est[indices_c],decreasing = F)]
      top_treated <- ordered_treated <- indices_t[order_t <- order(prW_est[indices_t],decreasing = T)]

      # drop duplicates
      if(!is.null(long)){
        longLat_t <- paste(round(long[indices_t[order_t]],5L),
                                round(lat[indices_t[order_t]],5L),sep="_")
        longLat_c <- paste(round(long[indices_c[order_c]],5L),
                                round(lat[indices_c[order_c]],5L),sep="_")
        top_treated <- ordered_treated[!duplicated(longLat_t)]
        top_control <- ordered_control[!duplicated(longLat_c)]
      }
      plot_indices <- c( top_control <- top_control[1:showPerGroup],
                         top_treated <- top_treated[1:showPerGroup] )

      for(pos_ in 2L:3L){
        dLogProb_d <- cienv$jax$grad(function(ModelList, ModelList_fixed,
                                        m, x, vseed,
                                        StateList, seed, MPList, inference){
                    ModelList <- MPList[[1]]$cast_to_param( ModelList )
                    ModelList_fixed <- MPList[[1]]$cast_to_param( ModelList_fixed )
                    StateList <- MPList[[1]]$cast_to_param( StateList )
                    m <- MPList[[1]]$cast_to_param( m ); x <- MPList[[1]]$cast_to_param( x )
                    m <- cienv$jax$device_put(m, cienv$jax$devices('cpu')[[1]])
                    out_ <-  GetTreatProb_batch(ModelList, ModelList_fixed,
                                           m, x, vseed,
                                           StateList, seed, MPList, T)[[1]]  # scaling for non-zero gradients
                    out_ <- cienv$jnp$log(out_ / (1-out_))
                    return(  cienv$jnp$squeeze(out_)  ) }, pos_)
        if(pos_ == 2L){ dLogProb_dImage <- dLogProb_d }
        if(pos_ == 3L){ dLogProb_dX <- cienv$eq$filter_jit( dLogProb_d ) }
      }
      ImGrad_fxn <- cienv$eq$filter_jit( function(ModelList, ModelList_fixed,
                                            m, x, vseed,
                                            StateList, seed, MPList){
        # cast to float32
        ModelList <- MPList[[1]]$cast_to_param( ModelList )
        ModelList_fixed <- MPList[[1]]$cast_to_param( ModelList_fixed )
        StateList <- MPList[[1]]$cast_to_param( StateList )
        m <- MPList[[1]]$cast_to_param( m ); x <- MPList[[1]]$cast_to_param( x )
        m <- cienv$jax$device_put(m, cienv$jax$devices('cpu')[[1]])
        ImageGrad_o <- cienv$jnp$squeeze(dLogProb_dImage(ModelList, ModelList_fixed,
                                       m, x, vseed, StateList, seed, MPList), 0L)
        reduceDim <- ifelse( dataType == "video", yes = 3L, no = 2L)
        ImageGrad_L2 <- cienv$jnp$linalg$norm(ImageGrad_o+0.000001, axis = reduceDim, keepdims = T)
        ImageGrad_mean <- cienv$jnp$mean(ImageGrad_o, axis = reduceDim, keepdims = T)
        return( list(ImageGrad_L2,  # salience magnitude
                     ImageGrad_mean) ) # salience direction
      }, device = cienv$jax$devices('cpu')[[1]])
      MPList <- list(cienv$jmp$Policy(compute_dtype="float32", param_dtype="float32", output_dtype="float32"),
                      cienv$jmp$DynamicLossScale(cienv$jnp$array(2^15), 
                                           period = 20L))
      makePlots <- function(){
        salience_try <- try({
        message("Plotting salience maps...")
        nrows_im <- 2
        eval(parse(text = ifelse(dataType == "image", yes = 'pdf(sprintf("%s/CSM_%s.pdf", figuresPath, FigNameAppend),
            width = length(plot_indices)*5+2,height = nrows_im*5)', no = "NULL") ))
        {
          layout(matrix(1:(nrows_im*(1+length(plot_indices))),
                        ncol = 1+length(plot_indices)),
                 width = c(0.5,rep(5,length(plot_indices))),
                 height = rep(5,times=nrows_im)); in_counter <- 0

        # create axis labels in plot positions 1 and 2
        for(text_ in c("Raw Image","Salience Map")){
            if(dataType == "image"){
              par(mar=c(0,0,0,0));
              plot(0, main = "", ylab = "",cex = 0, xlab = "", ylim = c(0,1), xlim = c(0,1), xaxt = "n",yaxt = "n",bty = "n")
              text(0.5,0.5,labels = text_, srt=90,cex=3)
            }
        }

        # generate rest of plot
          plot_index_counter <- 0; for(in_ in plot_indices){
            print(c(text_, in_))
            gc(); cienv$py_gc$collect()
            plot_index_counter <- plot_index_counter + 1

            # get data
            setwd(orig_wd); ds_next_in <- GetElementFromTfRecordAtIndices(
                                                    uniqueKeyIndices = which(unique(imageKeysOfUnits) %in% imageKeysOfUnits[in_]),
                                                    filename = file,
                                                    readVideo = useVideoIndicator,
                                                    nObs = length(imageKeysOfUnits) ); setwd(new_wd)
            ds_next_in[[1]] <- cienv$jnp$array( ds_next_in[[1]] )
            if(length(ds_next_in[[1]]$shape) == 3 & dataType == "image"){ ds_next_in[[1]] <- cienv$tf$expand_dims(ds_next_in[[1]], 0L) }
            if(length(ds_next_in[[1]]$shape) == 4 & dataType == "video"){ ds_next_in[[1]] <- cienv$tf$expand_dims(ds_next_in[[1]], 0L) }

            col_ <- ifelse(in_ %in% top_treated, yes = "black", no = "gray")
            in_counter <- in_counter + 1
            long_lat_in_ <- ""; if(  !is.null(lat)  ){ long_lat_in_ <- sprintf("Lat-Lon: %.3f, %.3f", f2n(lat[in_]), f2n(long[in_])) }

            im_orig <- im_ <- InitImageProcessFn( im = cienv$jnp$array(ds_next_in[[1]]), key = cienv$jax$random$PRNGKey(3L), inference = T )
            XToConcat_values <- cienv$jnp$array(t(X[in_,]),cienv$jnp$float16)
            im_ <- cienv$np$array(cienv$jnp$squeeze(im_,c(0L)))

            # calculate salience map using log probabilities
            # m <- cienv$jmp$cast_to_full(im_orig); x <- cienv$jmp$cast_to_full(XToConcat_values); seed <- cienv$jax$random$PRNGKey(10L); vseed <- cienv$jnp$expand_dims(seed,0L)
            salience_map <- cienv$np$array(  ImGrad_fxn(
                                        cienv$jmp$cast_to_full(ModelList), cienv$jmp$cast_to_full(ModelList_fixed),
                                        cienv$jmp$cast_to_full(im_orig),
                                        cienv$jmp$cast_to_full(XToConcat_values),
                                        cienv$jnp$expand_dims(cienv$jax$random$PRNGKey(10L),0L),
                                        StateList, cienv$jax$random$PRNGKey(10L), MPList )[[1]] )
            if(dataType == "image"){ salience_map <- salience_map[,,1] }
            if(dataType == "video"){ salience_map <- salience_map[,,,1] }

            # do plotting
            orig_scale_im_ <- sapply(1:length(NORM_MEAN), function(band_){
                                       if(dataType == "image"){
                                         im_[,,band_] <- 0.1+im_[,,band_]*NORM_SD[band_] + NORM_MEAN[band_]
                                         return( im_[,,band_] )
                                       }
                                       if(dataType == "video"){
                                         im_[,,,band_] <- 0.1+im_[,,,band_]*NORM_SD[band_] + NORM_MEAN[band_]
                                         return( im_[,,,band_] )
                                       }  }, simplify="array")

            # plot results
            par(mar = (mar_vec <- c(2,1,3,1)))
            if(dataType == "image"){
              dim_ <- dim(orig_scale_im_)
              plotRBG <- !(length(plotBands) < 3 | dim_[length(dim_)] < 3)
              if(!plotRBG){
                causalimages::image2(
                  as.matrix( orig_scale_im_[,,plotBands[1]] ),
                  main = long_lat_in_, cex.main = 2.5, col.main =  col_,
                  xlab = ifelse( plot_index_counter == 1,
                                 yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                                 no = "")
                )
              }
              if(plotRBG){
                 plot(0, main = long_lat_in_, col.main = col_,
                      ylab = "", xlab = "",
                      cex.main = 4, ylim = c(0,1), xlim = c(0,1),
                      cex = 0, xaxt = "n",yaxt = "n",bty = "n")
                 mtext(side = 1, ifelse( plot_index_counter == 1,
                               yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                               no = ""), cex = 1)
                 orig_scale_im_raster <- raster::brick(orig_scale_im_[,,plotBands[1:3]])
                 try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3,
                                 add = T, main = long_lat_in_, stretch = "lin"), T)
                 if("try-error" %in% class(try_)){ try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3, add = T, main = long_lat_in_), T) }
              }

              # plot salience map
              #cienv$optax$global_norm( cienv$eq$filter(ModelList, cienv$eq$is_array)[[1]] )
              try_salience <- try({salience_map[salience_map>0] <- salience_map[salience_map>0] / (0.001+sd(salience_map[salience_map>0]))},T)
              if("try-error" %in% class(try_salience)){
                print( try_salience )
                if(atError == "stop"){ stop() }; if(atError == "debug"){ browser() }
              }
              salience_map <- sign(salience_map)*log(abs(salience_map)+1)
              image2( salience_map, xlab = ifelse(tagInFigures, yes = imageKeysOfUnits[in_], no = ""),cex.lab = 1)
            }
            if(dataType == "video"){
              # plot raw image
              nTimeSteps <- dim(salience_map)[1]
              animation::saveGIF({
              # GIF part 1 --- all in outer time step loop
              for (t_ in 1:nTimeSteps){
              par(mfrow = c(1,2));
              dim_ <- dim(salience_map)
              plotRBG <- !(length(plotBands) < 3 | dim_[length(dim_)] < 3)
              par(mar = margins_gif <- c(5,5,2,1))
              if(!plotRBG){
                par(mar = margins_gif)
                causalimages::image2(
                  as.matrix( orig_scale_im_[t_,,,plotBands[1]] ),
                  main = long_lat_in_, cex.main = 1.5, col.main =  col_,
                  xlab = ifelse( plot_index_counter == 1,
                                 yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                                 no = "") )
              }
              if(plotRBG){
                plot(0, main = long_lat_in_, col.main = col_,
                     ylab = "", xlab = "",
                     cex.main = 1.5, ylim = c(0,1), xlim = c(0,1),
                     cex = 0, xaxt = "n",yaxt = "n",bty = "n")
                mtext(side = 1, ifelse( plot_index_counter == 1,
                                        yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                                        no = ""), cex = 1)
                orig_scale_im_raster <- raster::brick(orig_scale_im_[t_,,,plotBands[1:3]])
                try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3,
                                            add = T, main = long_lat_in_, stretch = "lin"), T)
                if("try-error" %in% class(try_)){
                  try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3,
                                              add = T, main = long_lat_in_), T)
                }
              }

              # GIF part 2
              salience_map[salience_map>0] <- salience_map[salience_map>0] / (0.001+sd(salience_map[salience_map>0]))
              salience_map <- sign(salience_map)*log(abs(salience_map)+1)
              par(mar = margins_gif)
              image2( salience_map[t_,,],
                      main = long_lat_in_, cex.main = 1.5, col.main = "white",
                      #xlab = ifelse(tagInFigures, yes = imageKeysOfUnits[in_], no = ""),
                      cex.lab = 1)
              }}, movie.name = sprintf("%s/CSM_%s_%s.gif", figuresPath, FigNameAppend, plot_index_counter),
                  autobrowse = F, autoplay = F,
                  ani.height = 480*1, ani.width = 480*(1+1))
          }
        }

        eval(parse(text = ifelse(dataType == "image", yes = "dev.off()", no = "NULL") ))
        }},T)
        if('try-error' %in% class(salience_try)){
          print(salience_try);
          if(atError == "stop"){ stop("Problem in salience map computation!")  }
          if(atError == "debug"){ browser() }
        }

        if(optimizeImageRep){
          pdf(sprintf("%s/Loss_%s.pdf", figuresPath,FigNameAppend))
          { 
            par(mar = c(6,5,1,1))
            try(plot(loss_vec, cex = 1.5, cex.lab = 2,
                     xlab = "Iteration", ylab = "Loss"),T);
            try(points(smooth.spline(na.omit(loss_vec)),type="l",lwd=3),T)
            mtext(side = 1, ifelse(tagInFigures, yes = figuresTag, no = ""), 
                  line = 4.5, cex = 1)
          }
          dev.off()
        }

        try({
        message("Plotting propensity histogram...")
        pdf(sprintf("%s/Hist_%s.pdf", figuresPath, FigNameAppend))
        {
          par(mfrow=c(1,1))
          d0 <- density(prW_est[obsW==0])
          d1 <- density(prW_est[obsW==1])
          plot(d1,lwd=2,xlim = c(-0.1,1.1),ylim =c(0,max(c(d1$y,d0$y),na.rm=T)*1.2),
               cex.axis = 1.2,ylab = "",xaxt = "n",
               xlab = ifelse(tagInFigures, yes = figuresTag, no = ""),
               main = "Density Plots for \n Estimated Pr(T=1 | Confounders)",cex.main = 2)
          axis(1, at = seq(0,1,by = 0.25))
          points(d0,lwd=2,type = "l",col="gray",lty=2)
          text(d0$x[which.max(d0$y)[1]],
               max(d0$y,na.rm=T)*1.1,label = "T = 0",col="gray",cex=2)
          text(d1$x[which.max(d1$y)[1]],
               max(d1$y,na.rm=T)*1.1,label = "T = 1",col="black",cex=2)
        }
        dev.off()
        }, T)
      }
      makePlots()
    }

    # compute salience for tabular covariates
    SalienceX_se <- SalienceX <- NULL; if(!XisNull){
    if( optimizeImageRep ){
        SalienceX <- c(); samp_counter <- 0
        for(keyNum_ in sample(1:length(unique(imageKeysOfUnits)), 25, replace = T)){
          samp_counter <- samp_counter + 1
          if(samp_counter %% 5 == 0){  message(sprintf("Tabular salience iteration %s of %s", samp_counter, 25)) }
          sampIndex_ <- which(imageKeysOfUnits %in% unique(imageKeysOfUnits)[keyNum_])[1]

          # extract data
          setwd(orig_wd); ds_next_in <- GetElementFromTfRecordAtIndices(
                                                           uniqueKeyIndices = keyNum_,
                                                           filename = file,
                                                           readVideo = useVideoIndicator,
                                                           nObs = length(unique(imageKeysOfUnits) ) ); setwd(new_wd)
          ds_next_in[[1]] <- cienv$jnp$array(ds_next_in[[1]])
          if(length(ds_next_in[[1]]$shape) == 3 & dataType == "image"){ ds_next_in[[1]] <- cienv$tf$expand_dims(ds_next_in[[1]], 0L) }
          if(length(ds_next_in[[1]]$shape) == 4 & dataType == "video"){ ds_next_in[[1]] <- cienv$tf$expand_dims(ds_next_in[[1]], 0L) }

          im_ <- InitImageProcessFn( cienv$jnp$array(ds_next_in[[1]]), cienv$jax$random$PRNGKey(432L), T)
          x_ <- cienv$jnp$array(t(X[sampIndex_,]), cienv$jnp$float16)
          SalienceX_contrib <- cienv$np$array(  dLogProb_dX(  ModelList, ModelList_fixed,
                        cienv$jmp$cast_to_full(im_),
                        cienv$jmp$cast_to_full(x_),
                        cienv$jax$random$split(cienv$jax$random$PRNGKey( 500L+i ),x_$shape[[1]]),
                        StateList, cienv$jax$random$PRNGKey(10L), MPList ) )
          SalienceX <- rbind(SalienceX, SalienceX_contrib)
        }
        SalienceX <- colMeans( SalienceX ); names( SalienceX ) <- colnames(X)
    }
    if( !optimizeImageRep ){
        SalienceX <- myGlmnet_coefs[-1][1:ncol(X)] # drop intercept, then extract variables of interest
        SalienceX_se <- apply(myGlmnet_coefs_mat, 2, sd)[-1][1:ncol(X)] * X_sd 
        if(!is.null(SalienceX)){ names(SalienceX_se) <- colnames(X) }
    } 
      
    # rescale the salience map into original scale: Don't add back X_mean
    SalienceX <- SalienceX*X_sd
    }

    postDiffInLat <- preDiffInLat <- NULL
    if(!is.null(lat)){
      preDiffInLat <- colMeans(cbind(long[obsW == 1],lat[obsW == 1])) -
        colMeans(cbind(long[obsW == 0],lat[obsW == 0]))
      postDiffInLat <- colSums(cbind(long[obsW == 1],lat[obsW == 1])*
                                 prop.table(1/prW_est[obsW == 1])) - 
                        colSums(cbind(long[obsW == 0],lat[obsW == 0])*
                                  prop.table(1/prW_est[obsW == 0]))
    }

    # set salience map names
    if(!is.null(SalienceX)){ names(SalienceX) <- colnames(X) }
    rm( InitImageProcessFn )

    message("Done with image confounding analysis!" ); try(setwd(orig_wd), T)
    return(    list(
      "tauHat_propensityHajek"  = tauHat_propensityHajek,
      "tauHat_propensityHajek_se"  = sd(tauHat_propensityHajek_vec,na.rm=T),
      "tauHat_diffInMeans"  = mean(obsY[which(obsW==1)],na.rm=T) - mean(obsY[which(obsW==0)],na.rm=T),
      "tauHat_diffInMeans_se"  = c(sqrt(se(obsY[which(obsW==1)])^2 + se(obsY[which(obsW==0)])^2)),
      "SalienceX" = SalienceX,
      "SalienceX_se" = SalienceX_se,
      "prW_est" = prW_est,
      "SGD_loss_vec" = loss_vec,
      "LatitudeAnalysis" = list("preDiffInLat" = preDiffInLat, "postDiffInLat"  = postDiffInLat),
      "ModelEvaluationMetrics" = ModelEvaluationMetrics,
      "AUC" = pROC::roc(obsW[testIndices], prW_est[testIndices])$auc,
      "myGlmnet_coefs" = myGlmnet_coefs,
      "ImageRepresentations_df" = ImageRepresentations_df, 
      "ImageRepresentations_df_transport" = ImageRepresentations_df_transport, 
      "tauHat_propensityHajek_vec" = tauHat_propensityHajek_vec,
      "nTrainableParameters" = nTrainable, # parameters actually trained 
      "nParamsRep" = nParamsRep, # parameters in representation 
      "trainIndices" = trainIndices,
      "testIndices" = testIndices
    ) )
  }
}
