#' Decompose treatment effect heterogeneity by image or image sequence
#'
#' Implements the image heterogeneity decomposition analysis of Jerzak, Johansson, and Daoud (2023). Users
#' input in treatment and outcome data, along with a function specifying how to load in images
#' using keys referenced to each unit (since loading in all image data will usually not be possible due to memory limitations).
#' This function by default performs estimation, constructs salience maps, and can optionally perform
#' estimation for new areas outside the original study sites in a transportability analysis.
#'
#' @param obsW A numeric vector where `0`'s correspond to control units and `1`'s to treated units.
#' @param obsY A numeric vector containing observed outcomes.
#' @param kClust_est Integer specifying the number of clusters used in estimation. Default is `2L`.
#' @param file Path to a tfrecord file generated by `WriteTfRecord`.
#' @param transportabilityMat An optional matrix with a column named `key` specifying keys to be used for generating treatment effect predictions for out-of-sample points in earth observation data settings.
#' @param imageKeysOfUnits A vector of length `length(obsY)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into the package to call images into memory.
#' @param long,lat Optional vectors specifying longitude and latitude coordinates for units. Used only for describing highest and lowest probability neighorhood units if specified.
#' @param X Optimal numeric matrix containing tabular information used if `orthogonalize = T`.
#' @param conda_env A `conda` environment where computational environment lives, usually created via `causalimages::BuildBackend()`. Default = `"CausalImagesEnv"`.
#' @param conda_env_required A Boolean stating whether use of the specified conda environment is required.
#' @param orthogonalize A Boolean specifying whether to perform the image decomposition after orthogonalizing with respect to tabular covariates specified in `X`.
#' @param nMonte_variational An integer specifying how many Monte Carlo iterations to use in the
#' calculation of the expected likelihood in each training step.
#' @param optimizeImageRep Boolean specifying whether to optimize over the image model representation (or only over downstream parameters).
#' @param nMonte_predictive An integer specifying how many Monte Carlo iterations to use in the calculation
#' of posterior means (e.g., mean cluster probabilities).
#' @param nMonte_salience An integer specifying how many Monte Carlo iterations to use in the calculation
#' of the salience maps (e.g., image gradients of expected cluster probabilities).
#' @param figuresTag A string specifying an identifier that is appended to all figure names.
#' @param figuresPath A string specifying file path for saved figures made in the analysis.
#' @param nSGD Number of stochastic gradient descent (SGD) iterations.
#' @param batchSize Batch size used in SGD optimization.
#' @param strides Integer specifying the strides used in the convolutional layers.=
#' @param plotResults Should analysis results be plotted?
#' @param plotBands An integer or vector specifying which band position (from the acquired image representation) should be plotted in the visual results. If a vector, `plotBands` should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RGB plotting).
#' @param dataType String specifying whether to assume `"image"` or `"video"` data types.
#' @param nWidth_ImageRep Integer specifying width of image model representation.
#' @param nDepth_ImageRep Integer specifying depth of image model representation.
#' @param nWidth_Dense Integer specifying width of image model representation.
#' @param nDepth_Dense Integer specifying depth of dense model representation.
#' @param kernelSize Dimensions used in spatial convolutions.
#' @param TfRecords_BufferScaler The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#'
#' @return Returns a list consiting of \itemize{
#'   \item `clusterTaus_mean` default
#'   \item `clusterProbs_mean`. Estimated mean image effect cluster probabilities.
#'   \item `clusterTaus_sigma`. Estimated cluster standard deviations.
#'   \item `clusterProbs_lowerConf`. Estimated lower confidence for effect cluster probabilities.
#'   \item `impliedATE`. Implied ATE.
#'   \item `individualTau_est`. Estimated individual-level image-based treatment effects.
#'   \item `transportabilityMat`. Transportability matrix withestimated cluster information.
#'   \item `plottedCoordinates`. List containing coordinates plotted in salience maps.
#'   \item `whichNA_dropped`. A vector containing observations dropped due to missingness.
#' }
#'
#' @section References:
#' \itemize{
#' \item Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Image-based Treatment Effect Heterogeneity. Forthcoming in \emph{Proceedings of the Second Conference on Causal Learning and Reasoning (CLeaR), Proceedings of Machine Learning Research (PMLR)}, 2023.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @import reticulate rrapply
#' @export
#' @md
AnalyzeImageHeterogeneity <- function(obsW,
                                      obsY,
                                      X = NULL,
                                      orthogonalize = F,
                                      imageKeysOfUnits = 1:length(obsY),
                                      kClust_est = 2,
                                      file = NULL,
                                      transportabilityMat = NULL ,
                                      lat = NULL,
                                      long = NULL,
                                      conda_env = "CausalImagesEnv",
                                      conda_env_required = T,

                                      figuresTag = "",
                                      figuresPath = "./",
                                      plotBands = 1L,
                                      heterogeneityModelType = "variational_minimal",
                                      plotResults = F,
                                      optimizeImageRep = T,
                                      nWidth_ImageRep = 64L, nDepth_ImageRep = 1L,
                                      nWidth_Dense = 64L, nDepth_Dense = 1L,
                                      nDepth_TemporalRep = 1L,
                                      useTrainingPertubations = T,
                                      strides = 2L,
                                      pretrainedModel = NULL, 
                                      testFrac = 0.1,
                                      kernelSize = 5L,
                                      learningRateMax = 0.001,
                                      TFRecordControl = NULL, 
                                      patchEmbedDim = 16L,
                                      nSGD  = 500L,
                                      batchSize = 16L,
                                      seed = NULL,
                                      Sys.setenv_text = NULL,

                                      imageModelClass = "VisionTransformer",
                                      nMonte_predictive = 10L,
                                      nMonte_salience = 10L,
                                      nMonte_variational = 2L,
                                      TfRecords_BufferScaler = 4L,
                                      temperature = 1,
                                      inputAvePoolingSize  = 1L,
                                      dataType = "image"){
  # create directory if needed
  if( !dir.exists(figuresPath) ){ dir.create(figuresPath) }

  if(!"jax" %in% ls(envir = cienv)) {
    initialize_jax(conda_env = conda_env, 
                   conda_env_required = conda_env_required,
                   Sys.setenv_text = Sys.setenv_text) 
  }
  
  {
    # image dtype management
    c2f <- cienv$jmp$cast_to_full
    image_dtype <- "float16" 
    if((image_dtype_char <- image_dtype) == "float32"){  image_dtype_tf <- cienv$tf$float16; image_dtype <- cienv$jnp$float32 }
    if(image_dtype_char == "float16"){  image_dtype_tf <- cienv$tf$float16; image_dtype <- cienv$jnp$float16 }
    if(image_dtype_char == "bfloat16"){  image_dtype_tf <- cienv$tf$bfloat16; image_dtype <- cienv$jnp$bfloat16 }
    ComputeDtype <- cienv$jnp$float32; 
    variable_dtype <- cienv$jnp$float32; 
    image_dtype_tf <- cienv$tf$float16; 

    cnst <- function( ar ){ cienv$jnp$array(ar, ComputeDtype) }
    rzip <- function( l1,l2 ){  fl<-list(); for(aia in 1:length(l1)){ fl[[aia]] <- list(l1[[aia]], l2[[aia]]) }; return( fl  ) }
    if(is.null(seed)){ seed <- ai(runif(1,1,100000)) }
  }
  if(!optimizeImageRep & nDepth_ImageRep > 1){ stop("Stopping: When nDepth_ImageRep = T, nDepth_imageRep must be 1L") }
  
  message("Setting up wd logic")
  orig_wd <- getwd()
  cond1 <- substr(figuresPath, start = 0, stop = 1) == "."
  if(cond1){ figuresPath <- gsub(figuresPath, pattern = '\\.', replacement = orig_wd) }
  if(!dir.exists(figuresPath)){ dir.create(figuresPath) }

  BN_EP <- (0.001); bn_momentum <- (.90)
  if(grepl(heterogeneityModelType,pattern = "variational")){ bn_momentum <- bn_momentum^1/nMonte_variational }

  message("tfrecord + train/test fxns")
  changed_wd <- F; {
    message("Establishing connection with tfrecord")
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    new_wd <- paste(tf_record_name[-length(tf_record_name)], collapse = "/")
    message(sprintf("Temporarily re-setting the wd to %s", new_wd ) )
    changed_wd <- T; setwd( new_wd )
    tf_dataset_master <- cienv$tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideoIndicator <- (dataType == "video")
    getParsed_tf_dataset_inference <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){ 
                              parse_tfr_element(element = x,
                                                readVideo = useVideoIndicator,
                                                image_dtype = image_dtype_tf) } ) 
      return( dataset <- dataset$batch( ai(max(2L,round(batchSize/2L)  ))) )
    }
    
    # setup k-fold cross-fitting
    {
      getParsed_tf_dataset_train_Select <- function( tf_dataset ){
        tf_dataset <- tf_dataset$map( function(x){ parse_tfr_element(x, 
                                                                     readVideo = useVideoIndicator, 
                                                                     image_dtype = image_dtype_tf)},
                        num_parallel_calls = cienv$tf$data$AUTOTUNE) 
        return(  tf_dataset ) 
      }
      getParsed_tf_dataset_train_Shuffle <- function( tf_dataset ){
        tf_dataset <- tf_dataset$shuffle(buffer_size = cienv$tf$constant(ai(TfRecords_BufferScaler*batchSize),
                                                                            dtype=cienv$tf$int64),
                                         reshuffle_each_iteration = FALSE )
        return(tf_dataset)
      }
      getParsed_tf_dataset_train_BatchAndShuffle <- function( tf_dataset ){
        tf_dataset <- tf_dataset$shuffle(buffer_size = cienv$tf$constant(ai(TfRecords_BufferScaler*batchSize),
                                                                            dtype=cienv$tf$int64),
                                         reshuffle_each_iteration = TRUE ) 
        tf_dataset <- tf_dataset$batch(  ai(batchSize)   )
        tf_dataset <- tf_dataset$prefetch( cienv$tf$data$AUTOTUNE ) 
        return( tf_dataset )
      }
    }

    # setup iterator
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset_master )

    # reset iterator
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )

    # checks
    # ds_iterator_inference$output_shapes; ds_iterator_train$output_shapes
    # ds_next_inference <- reticulate::iter_next( ds_iterator_inference )
  }

  message("Getting channel normalization parameters...")
  NORM_MEAN <- GetMoments( reticulate::as_iterator( getParsed_tf_dataset_train_BatchAndShuffle( 
                        getParsed_tf_dataset_train_Select(getParsed_tf_dataset_train_Shuffle( tf_dataset_master )  ))), 
                                dataType = dataType, image_dtype = image_dtype, momentCalIters = 34)
  NORM_SD <- NORM_MEAN$NORM_SD_array; 
  NORM_MEAN <- NORM_MEAN$NORM_MEAN_array; 
  
  # reset inference iterator 
  ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )
  
  # clear memory 
  rm(  tmp  ) 

  if(useTrainingPertubations){
    trainingPertubations_OneObs <- function(im_, key){
        AB <- ifelse(dataType == "video", yes = 1L, no = 0L)
        which_path <- cienv$jnp$squeeze(cienv$jax$random$categorical(key = key, logits = cienv$jnp$array(t(rep(0, times = 4)))),0L)# generates random # from 0L to 3L
        im_ <- cienv$jax$lax$cond(cienv$jnp$equal(which_path,cienv$jnp$array(0L)), true_fun = function(){ cienv$jnp$flip(im_, AB+1L) } , false_fun = function(){im_})
        im_ <- cienv$jax$lax$cond(cienv$jnp$equal(which_path,cienv$jnp$array(2L)), true_fun = function(){ cienv$jnp$flip(im_, AB+2L) }, false_fun = function(){im_})
        im_ <- cienv$jax$lax$cond(cienv$jnp$equal(which_path,cienv$jnp$array(3L)), true_fun = function(){ cienv$jnp$flip(cienv$jnp$flip(im_, AB+1L),AB+2L) }, false_fun = function(){im_})
        return( im_ )
    } 
    trainingPertubations <- cienv$jax$vmap(function(im_, key){return( trainingPertubations_OneObs(im_,key) )  }, in_axes = list(0L,0L))
  }
  InitImageProcessFn <- cienv$jax$jit(function(im, key, inference){
      # expand dims if needed
      if(length(imageKeysOfUnits) == 1){ im <- cienv$jnp$expand_dims(im,0L) }

      # normalize
      im <- (im - NORM_MEAN) / NORM_SD

      # training pertubations
      if(useTrainingPertubations){
        im <- cienv$jax$lax$cond(inference, 
                           true_fun = function(){ im }, 
                           false_fun = function(){ trainingPertubations(im, 
                                                        cienv$jax$random$split(key,im$shape[[1]])) } )
      }

      # downshift resolution if desired
      if(inputAvePoolingSize > 1){ im <- cienv$jax$vmap(function(im){ AvePoolingDownshift(im)}, 0L) }

      # return normalized (& pertubed if inference = F) image/image seq
      return( im  )
  })

  # set up placeholders + start loop 
  kFolds <- 3L
  cf_iters <- 2L
  Loss_out_baseline_vec <- Loss_out_vec <- Y0_est_mat <- Y1_est_mat <- tau_est_mat <- c()  
  TestIndices_list <- TrainIndices_list <- replicate(list(), n = cf_iters*kFolds)
  
  # set up loop values 
  nUniqueKeys <- length( unique( imageKeysOfUnits ) )
  cf_keys_split <- 1*as.numeric(cut(1:nUniqueKeys,kFolds))
  cf_keys_split <- sapply( 1:kFolds, function(l_){ list(which(cf_keys_split==l_))})
  cf_keys_toSkip_bounds <- lapply(cf_keys_split,function(l_){c(min(l_), max(l_))})
  
  # start outer CF iteration 
  trainCounter <- 0; for(cf_iter in 1:cf_iters){ 
    
    # shuffle for outer CF iteration 
    tf_dataset_master_ <- getParsed_tf_dataset_train_Shuffle( tf_dataset_master )
    
    # start inner CF iteration 
    for(kf_ in 1:kFolds){
      trainCounter <- trainCounter + 1
      message(sprintf("Starting training at k = %s of %s (%s of %s)...",kf_, kFolds, cf_iter, cf_iters))
      # setup iterator 
      {
        # select a tf record indexed to (1:kFolds([!1:kFolds %in% kf_] (skip indices bounded by cf_keys_toSkip_bounds)
        if(kf_ == 1){ 
          tf_dataset_train <- getParsed_tf_dataset_train_Select(
            tf_dataset_master_$skip( ai(cf_keys_toSkip_bounds[[kf_]][2]) ) )$`repeat`(-1L) 
        }
        if(kf_ == kFolds){ 
          tf_dataset_train <- getParsed_tf_dataset_train_Select(
            tf_dataset_master_$take( ai(cf_keys_toSkip_bounds[[kf_]][1]-1L) ) )$`repeat`(-1L) 
        }
        if(kf_ > 1 & kf_ < kFolds){ 
           tf_dataset_train <- getParsed_tf_dataset_train_Select(
             tf_dataset_master_$take( ai(cf_keys_toSkip_bounds[[kf_]][1]-1L) )$concatenate(
               tf_dataset_master_$skip( ai(cf_keys_toSkip_bounds[[kf_]][2]) ) ))$`repeat`(-1L) # repeat to avoid out of sequence errors 
        }
        tf_dataset_train <- getParsed_tf_dataset_train_BatchAndShuffle( tf_dataset_train )
        ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
      }
      
      # obtain image representation function
      {
        message("Initializing image representation functions...")
        SharedImageRepresentation <- T;
        setwd(orig_wd); ImageRepresentations <- GetImageRepresentations(
            file = file, conda_env = conda_env,
            dataType = dataType,
            nWidth_ImageRep = nWidth_ImageRep,
            nDepth_ImageRep = nDepth_ImageRep,
            NORM_MEAN = NORM_MEAN, 
            NORM_SD = NORM_SD, 
            InitImageProcess = InitImageProcessFn,
            strides = strides,
            nDepth_TemporalRep = nDepth_TemporalRep,
            patchEmbedDim = patchEmbedDim,
            batchSize = batchSize,
            imageModelClass = imageModelClass,
            pretrainedModel = pretrainedModel, 
            kernelSize = kernelSize,
            TfRecords_BufferScaler = 3L,
            imageKeysOfUnits = imageKeysOfUnits[tmp_i <- sample(1:length(imageKeysOfUnits),2*batchSize)],
            lat = lat[tmp_i], 
            long = long[tmp_i], 
            image_dtype = image_dtype, 
            image_dtype_tf = image_dtype_tf, 
            returnContents = T,
            initializingFxns = T, 
            bn_momentum = bn_momentum,
            seed = seed + ai(trainCounter) # seed 
            ); setwd(new_wd)
            ImageModel_And_State_And_MPPolicy_List <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]]
            ImageRepArm_OneObs <- ImageRepresentations[["ImageRepArm_OneObs"]]
            ImageRepArm_batch_R <- ImageRepresentations[["ImageRepArm_batch_R"]]
            InitImageProcessFn <-  ImageRepresentations[["InitImageProcess"]]
    
            message("Done initializing image representation function...")
            rm(ImageRepresentations); gc(); cienv$py_gc$collect()
      }
    
      # set environment of image sampling functions
      figuresPath <- paste(strsplit(figuresPath,split="/")[[1]],collapse = "/")
      windowCounter <- 0
    
      # orthogonalize if specified
      whichNA_dropped <- c()
      if(orthogonalize){
        message("Orthogonalizing Potential Outcomes...")
        if(is.null(X)){stop("orthogonalize set to TRUE, but no X specified to perform orthogonalization!")}
    
        # drop observations with NAs in their orthogonalized outcomes
        whichNA_dropped <- which( is.na(  rowSums( X ) ) )
        if(length(whichNA_dropped) > 0){
          # note: transportabilityMat doesn't need to drop dropNAs
          obsW <- obsW[-whichNA_dropped]
          obsY <- obsY[-whichNA_dropped]
          X <- X[-whichNA_dropped,]
          imageKeysOfUnits <- imageKeysOfUnits[-whichNA_dropped]
          lat <- lat[ -whichNA_dropped ]
          long <- long[ -whichNA_dropped ]
        }
        Yobs_ortho <- resid(temp_lm <- lm(obsY ~ X))
        if(length(Yobs_ortho) != length(obsY)){
          stop("length(Yobs_ortho) != length(obsY)")
        }
        plot(obsY,Yobs_ortho)
        obsY <- Yobs_ortho
      }
    
      # set up holders 
      Tau_sd_vec <- plotting_coordinates_list <- Tau_mean_sd_vec <- loss_vec <- NULL
      Tau_mean_return_vec <- Tau_mean_return_sd_vec <- cienv$jnp$array(1.)
    
      # specify some training parameters + helper functions
      batchFracOut <- max(1/3*batchSize,3) / batchSize
      nMonte_variational <- ai( nMonte_variational  )
      widthFreq <- 20
      WhenPool <- c(1,2)
      #as the temperature goes to 0 the RelaxedOneHotCategorical becomes discrete with a distribution described by the logits or probs parameters
      #plot(as.matrix2(do.call(rbind,replicate(100,tfd$RelaxedOneHotCategorical(temperature = temperature, probs = c(0.1,0.9))$sample(1L))))[,2],ylim = c(0,1));abline(h=0.5)
    
      # set up some placeholders
      y0_true <- r2_y1_out <- r2_y0_out <- ClusterProbs_est <- NULL
      tau_est <- negELL <- y1_est <- y0_est <- y1_true <- y0_true <- NULL
      if(!"ClusterProbs" %in% ls() &
         !"ClusterProbs" %in% ls(envir=globalenv())){ClusterProbs<-NULL}
    
      # normalize outcomes for stability (estimates are re-normalized after training)
      obsY_orig <- obsY
      Y_mean <- mean(obsY); Y_sd <- sd(obsY)
      obsY <- (obsY - Y_mean)  /  Y_sd
      Rescale <- function(x,doMean = F){ return( x*Y_sd + ifelse(doMean, yes = Y_mean, no = 0) ) }
      Tau_mean_init_prior <- Tau_mean_init <- mean(obsY[obsW==1]) - mean(obsY[obsW==0])
      Tau_sd_init <- sqrt( var(obsY[obsW==1]) + var( obsY[obsW==0]) )
      Y0_sd_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); return(sd(obsY[top_][obsW[top_]==0])) }))
      Y1_sd_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); sd(obsY[top_][obsW[top_]==1]) }))
      tau_vec <- na.omit(replicate(10000,{ top_ <- sample(1:length(obsY),batchSize); mean(obsY[top_][obsW[top_]==1]) - mean(obsY[top_][obsW[top_]==0]) }))
      Y0_mean_init_prior <- Y0_mean_init <- mean(obsY[obsW==0]); Y0_sd_raw <- max(0.01, mean(Y0_sd_vec,na.rm=T))
      Y1_mean_init_prior <- Y1_mean_init <- mean(obsY[obsW==1]); Y1_sd_raw <- max(0.01, mean(Y1_sd_vec,na.rm=T))
      softplus_inverse <- function(x){ cienv$jnp$log(cienv$jnp$exp(x) - cnst(1.)) }
      softplus_inverse2 <- function(x){ cienv$jnp$log(cienv$jnp$exp(x) - cienv$jnp$array(1)) }
      Y0_sd_priorMean <- softplus_inverse(cnst(Y0_sd_raw))
      Y1_sd_priorMean <-softplus_inverse(cnst(Y1_sd_raw))
      Y0_sd_initMean <-softplus_inverse(cnst((SD_scaling <- 1)*Y0_sd_raw))#<-SD_scaling*sd(obsY[obsW==0])))
      Y1_sd_initMean <-softplus_inverse(cnst(SD_scaling*Y1_sd_raw))#<-SD_scaling*sd(obsY[obsW==1])))
    
      #for(BAYES_STEP in c(1,2)){if(BAYES_STEP == 1){ cienv$eq$ifelse(BAYES_STEP==1,yes="Empirical Bayes Calibration Step (see  Krishnan et al. (2020))...", no="Empirical Bayes Estimation Step...")) }
      for(BAYES_STEP in c(1)){
        if(BAYES_STEP == 1){
          nSGD_ORIG <- nSGD
          L2_grad_scale <- cnst( 0.5 )
          SD_PRIOR_MODEL <- cnst(.0001); KL_wt <- cnst(0)
          SD_INIT_MODEL <- cnst(.000001)
          PRIOR_MODEL_FXN <- function(name_){
            eval(parse(text = 'function(dtype, shape, name, trainable, add_variable_fn){
          d_prior <- oryx$distributions$Normal(loc = cienv$jnp$zeros(shape), scale = cnst(SD_PRIOR_MODEL))
          tfd$Independent(d_prior, reinterpreted_batch_ndims = cienv$tf$size(d_prior$batch_shape_tensor())) }'))
          }
          PRIOR_MODEL_FXN("hap")
          PosteriorInitializer <- function(){
            eval(parse(text = 'tfp$layers$default_mean_field_normal_fn(
            is_singular = F,
            loc_initializer = cienv$tf$keras$initializers$GlorotUniform(seed = ai(runif(1,1,100000))),
            untransformed_scale_initializer = cienv$tf$keras$initializers$random_normal(mean = -9.0, stddev = 0.001, seed = ai(runif(1,1,100000))),
            loc_regularizer = NULL,
            untransformed_scale_regularizer = NULL,
            loc_constraint = NULL,
            untransformed_scale_constraint = NULL )')) }
        }
        if(BAYES_STEP == 2){
          nSGD <- nSGD_ORIG
          L2_grad_scale <- cnst( 0.5 )
          KL_wt <- cnst( batchSize / length(obsY) )
          PRIOR_MODEL_FXN <- function(name_){
            prior_loc_name <- sprintf("%s_PRIOR_MEAN_HASH818",name_ )
            prior_SD_name <- sprintf("%s_PRIOR_SD_HASH818",name_ )
            ZERO_LEN_IN <- length( eval(parse(text = sprintf("%s$variables",name_)))) == 0
            if( ZERO_LEN_IN){
              prior_loc_name <- "cienv$jnp$zeros(shape)"; prior_SD_name <- "1"
            }
            if( !ZERO_LEN_IN){
              z_name_ref <- eval(parse(text = sprintf("%s$variables[[1]]$name",name_)))
              z_name_ref <- gsub(z_name_ref, pattern = ":",replacement = "XCOLX")
              z_name_ref <- gsub(z_name_ref, pattern = "/",replacement = "XDASHX")
    
              # set mean
              eval.parent(parse(text = sprintf("%s <- cienv$jnp$array(%s$variables[[1]],variable_dtype)",prior_loc_name,name_)))
    
              # set sd
              eval.parent(parse(text = sprintf("%s <- cienv$tf$maximum(cienv$jnp$array(0.001,dtype = variable_dtype),
                                                            cienv$jnp$array(0.1*cienv$tf$sqrt(cienv$tf$math$reduce_variance(%s$variables[[1]])),variable_dtype))",prior_SD_name,name_)))
            }
            eval(parse(text = sprintf('function(dtype, shape, name, trainable, add_variable_fn){
                  d_prior <- oryx$distributions$Normal(loc = (%s),
                                      scale = (%s))
                  tfd$Independent(d_prior, reinterpreted_batch_ndims = cienv$tf$size(d_prior$batch_shape_tensor())) }',
                                      prior_loc_name, prior_SD_name)))
          }
    
          # set other priors
          Tau_mean_init_prior <- as.vector(MeanDist_tau[k_,"Mean"][[1]])
          Y0_sd_priorMean <- as.vector(SDDist_Y0[k_,"Mean"][[1]])
          Y1_sd_priorMean <- as.vector(SDDist_Y1[k_,"Mean"][[1]])
        }
    
        message("Building causal model...")
        {
          batch_axis_name <- "batch"; bn_momentum <- 0.9; ep_BN <- 0.001
          DenseList_Prior <- DenseStateList <- DenseList <- list()
          InvSoftPlus <- function(x){ cienv$jnp$log(cienv$jnp$exp(x) - 1) }
          for(arm_ in c("Tau","EY0")){
            for(dense_ in 1L:nDepth_Dense){
                  # arm_ <- "Tau"; dense_ <- 1L
                  InputDim <- ai( ifelse(dense_==1, yes = nWidth_ImageRep, no = nWidth_Dense) )
                  HiddenDim <- ai( ifelse(dense_ < nDepth_Dense, 
                                          yes = ai( nWidth_Dense*(HiddenWidthDense <- 3.) ), 
                                          no = nWidth_ImageRep) )
                  OutputDim <- ai( ifelse(dense_==nDepth_Dense,
                                             yes = ifelse(arm_ == "Tau", yes = kClust_est-1L, no = 1L),
                                             no = nWidth_Dense))
                  BiasInit <- ifelse(dense_ == nDepth_Dense & arm_ == "EY0", 
                                     yes = mean(obsY[obsW == 0]), 
                                     no = ifelse(dense_ == nDepth_Dense & arm_ == "Tau" & heterogeneityModelType == "tarnet", 
                                                 yes = mean(obsY[obsW == 1]) - mean(obsY[obsW == 0]), 
                                                 no = 0.))
                  LeftNarrowProjDim <- ifelse(dense_ < nDepth_Dense, yes = HiddenDim, no = InputDim)
                  DenseList_d <- list(  "BNLayer"=list('BN'=cienv$eq$nn$BatchNorm( input_size = InputDim, axis_name = batch_axis_name, 
                                                                               momentum = bn_momentum, eps = ep_BN, channelwise_affine = F),
                                                        'BNRescaler'= cienv$jnp$array(rep(1., times = InputDim))), 
                                          "FF" = list('FFWide1' = cienv$jnp$array(matrix(rnorm(InputDim*HiddenDim)*sqrt(2/InputDim), nrow = InputDim)), # swiglu proj wts (1) 
                                                      'FFWide1Bias' = cienv$jnp$array(matrix(rnorm(HiddenDim,sd=0, mean = 0), nrow = HiddenDim)), 
                                               
                                                      'FFWide2' = cienv$jnp$array(matrix(rnorm(InputDim*HiddenDim)*sqrt(2/InputDim), nrow = InputDim)), # swiglu proj wts (2) 
                                                      'FFWide2Bias' = cienv$jnp$array(matrix(rnorm(HiddenDim,sd=0, mean = 0), nrow = HiddenDim)),
                                                     
                                                      'FFNarrow'=cienv$jnp$array(matrix(rnorm(LeftNarrowProjDim*OutputDim)*sqrt(2/InputDim), nrow = LeftNarrowProjDim)), # output proj wts
                                                      'FFNarrowBias'=cienv$jnp$array(matrix(rnorm(OutputDim,sd=0, mean = 0), ncol = OutputDim))
                                                       ),        
                                          'ResidProj' = cienv$jnp$array(matrix(rnorm(InputDim*OutputDim)*sqrt(2/InputDim), nrow = InputDim)), # resid proj wts
                                          'ResidBias' = cienv$jnp$array(matrix(rnorm(InputDim*OutputDim,sd=0, mean = 0), ncol = OutputDim)),
                                                           
                                          'RightWt1' = InvSoftPlus(cienv$jnp$array(1./sqrt( 3^2 * nDepth_Dense ))) # residual wt 
                                      )
                  eval(parse(text = sprintf("DenseList$%s_d%s <- DenseList_d", arm_, dense_ ))); rm(DenseList_d)
                  eval(parse(text = sprintf("DenseList_Prior$%s_d%s <- list(
                                          cienv$jnp$array(matrix(rnorm(OutputDim*InputDim)*sqrt(2/InputDim), nrow = InputDim)),
                                          cienv$jnp$array(matrix(rnorm(OutputDim*InputDim,sd=0.0001, mean = -8), nrow = InputDim)))", arm_, dense_ )))
                  eval(parse(text = sprintf("DenseStateList$BNState_%s_d%s <- cienv$eq$nn$State(  DenseList$%s_d%s$BNLayer$BN  )", arm_, dense_, arm_, dense_ )))
            }; rm( dense_ )
          }
          message("Initializing cluster projection...")
          Tau_mean_init <- mean(obsY[obsW==1]) - mean(obsY[obsW==0])
          Tau_means_init <- Tau_mean_init + .01*seq(-1,1,length.out=kClust_est)*max(0.01,abs(Tau_mean_init))
    
          message("Initializing cluster centers...")
          base_mat <- as.data.frame( matrix(list(),nrow=kClust_est,ncol=3L) ); colnames( base_mat ) <- c("Mean","SD","Prior")
          SDDist_Y1 <- SDDist_Y0 <- MeanDist_tau <- base_mat
          as.numeric2 <- function(x){ as.numeric(cienv$np$array(cienv$tf$cast(x,cienv$tf$float32))) }
          as.matrix2 <- function(x){ as.matrix(cienv$np$array(cienv$tf$cast(x,cienv$tf$float32))) }
          for(k_ in 1:kClust_est){
            sd_init_trainableParams <- as.numeric2(softplus_inverse2(0.00001)) # set this to a small number so network starts off as nearly deterministic
    
            # tau's
            MeanDist_tau[k_,"Mean"][[1]] <- list( cienv$jnp$array(cnst(Tau_means_init[k_]) ) )
            MeanDist_tau[k_,"SD"][[1]] <- list( cienv$jnp$array(cnst(sd_init_trainableParams) ) )
            MeanDist_tau[k_,"Prior"][[1]] <- list( oryx$distributions$Normal(cnst(Tau_mean_init_prior), cnst(2*sd(tau_vec) )))
    
            # Y0
            SDDist_Y0[k_,"Mean"][[1]] <- list( cienv$jnp$array(cnst(as.numeric2(Y0_sd_initMean)) ) )
            SDDist_Y0[k_,"SD"][[1]] <- list( cienv$jnp$array(cnst(sd_init_trainableParams)) )
            SDDist_Y0[k_,"Prior"][[1]] <- list( oryx$distributions$Normal(cnst(as.numeric2(Y0_sd_priorMean)), cnst(2*sd(Y0_sd_vec))))
    
            # Y0
            SDDist_Y1[k_,"Mean"][[1]] <- list( cienv$jnp$array(cnst(Y1_sd_initMean) ) )
            SDDist_Y1[k_,"SD"][[1]] <- list( cienv$jnp$array(cnst(sd_init_trainableParams)) )
            SDDist_Y1[k_,"Prior"][[1]] <- list( oryx$distributions$Normal(cnst(Y1_sd_priorMean),cnst(2*sd(Y1_sd_vec))))
          }
        }
        MeanDist_tau <- unlist(  MeanDist_tau  )
        SDDist_Y1 <- unlist(  SDDist_Y1  )  ;  SDDist_Y0 <- unlist(  SDDist_Y0  )
        names(MeanDist_tau) <- paste("Tau_", names(MeanDist_tau), sep = "")
        names(SDDist_Y0) <- paste("Y0_", names(SDDist_Y0),sep = "")
        names(SDDist_Y1) <- paste("Y1_", names(SDDist_Y1),sep = "")
        CausalList <- c(MeanDist_tau, SDDist_Y0, SDDist_Y1)
        PriorCausalList <- CausalList[ grepl(names(CausalList),pattern = "Prior") ]
        CausalList <- CausalList[ !grepl(names(CausalList),pattern = "Prior") ]
    
        GetDenseNet <- function(ModelList, ModelList_fixed, m,
                                vseed, StateList, seed, MPList, inference, type){
            m <- cienv$jnp$expand_dims(m, 0L)
            for(d__ in 1L:nDepth_Dense){
              print(sprintf("Depth %s of %s (type = %s)", d__, nDepth_Dense, type))
              eval(parse(text = sprintf("ModelList_d <- ModelList$%s_d%s",type, d__)))
              
              # nonlinearity
              mtm1 <- m;  if( d__ < nDepth_Dense ){
                m <- cienv$jax$nn$swish( cienv$jnp$matmul( m, ModelList_d$FF$FFWide1 ) )  * 
                              cienv$jnp$matmul( m, ModelList_d$FF$FFWide2 )
              }
              
              # output projection 
              m <- cienv$jnp$matmul( m, ModelList_d$FF$FFNarrow ) + ModelList_d$FF$FFNarrowBias
              
              # resid path 
              if(d__ < nDepth_Dense){
                m <- cienv$jnp$matmul( mtm1, ModelList_d$ResidProj ) +
                        m * cienv$jax$nn$softplus( ModelList_d$RightWt1$astype(cienv$jnp$float32) )$astype(mtm1$dtype)
              }
          }
    
          if( type == "Tau" & grepl(heterogeneityModelType,pattern="variational") ){  
            m <- cienv$jnp$concatenate(list( MPList[[1]]$cast_to_compute( cienv$jnp$zeros(list(1L,1L)) ), m), 1L)
            m <- cienv$jnp$squeeze( m, axis = 0L)
          } else {
            m <- cienv$jnp$squeeze( m, axis = 1L)
          }
          return( list(m, StateList)  )
        }
        for(type_ in c("EY0","Tau")){
          eval(parse(text =  sprintf("Get%s_batch <- cienv$jax$vmap( Get%s <- function(
            ModelList, ModelList_fixed,
            m, vseed,
            StateList, seed, MPList, inference){
            GetDenseNet(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference, type = '%s')
          }, in_axes = list(NULL, NULL,
                            0L, 0L,
                            NULL, NULL, NULL, NULL),
             axis_name = batch_axis_name,
             out_axes = list(0L, NULL))", type_, type_ , type_)))
        }
        if(grepl(heterogeneityModelType, pattern = "variational")){
          getClusterSamp_logitInput <- function(logits_, seed_){
            return( oryx$distributions$RelaxedOneHotCategorical(
                      temperature = c2f(temperature),
                      logits =  logits_ 
                      )$sample(seed = cienv$jnp$add(452L, seed_)) )
          }
          if(grepl(heterogeneityModelType, pattern = "variational_minimal")){
            GetEY1_batch <-  function(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference){
              EY0 <- GetEY0_batch(ModelList, ModelList_fixed,
                                  m, vseed, StateList, seed, MPList, inference)[[1]]
              Clust_logits <- GetTau_batch(ModelList,ModelList_fixed,
                                  m, vseed, StateList, seed, MPList, inference)[[1]]
              clustT <- getClusterSamp_logitInput(Clust_logits, seed)
              ETau_draw <-  oryx$distributions$Normal(
                              c2f(getTau_means(ModelList)),
                        cienv$jax$nn$softplus(c2f(getTau_sds(ModelList))))$sample(seed = cienv$jnp$add(10L,seed))
              Etau_ <- cienv$jnp$sum( ETau_draw*clustT, axis = 1L, keepdims=T)
              return( EY0 + Etau_ )
            }
          }
          getTau_means <- function(ModelList){ return(  cienv$jnp$stack(list(ModelList$Tau_Mean1, ModelList$Tau_Mean2 )) )  }
          getTau_sds <- function(ModelList){ return(  cienv$jnp$stack(list( ModelList$Tau_SD1, ModelList$Tau_SD2 )) )  }
        }
        getSDY_params <- function(ModelList, qname, pname){ return(
          cienv$jnp$stack(list(LE(ModelList, sprintf("%s_%s1", qname, pname)),
                         LE(ModelList, sprintf("%s_%s2", qname, pname)) )) ) }
        if(grepl(heterogeneityModelType, pattern = "tarnet")){
          GetEY1_batch <-  function(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference){
            EY0 <- GetEY0_batch(ModelList, ModelList_fixed,
                                m, vseed, StateList, seed, MPList, inference)[[1]]
            Etau_ <- GetTau_batch(ModelList, ModelList_fixed,
                                m, vseed, StateList, seed, MPList, inference)[[1]]
            return( EY0 + Etau_ )
          }
        }
    
        GetLikelihoodDraw_batch <- function(
                                            ModelList, ModelList_fixed,
                                            m, treat, y, vseed,
                                            StateList, seed, MPList, inference){
          if(SharedImageRepresentation){
            message("Getting image representation in GetLikelihoodDraw_batch()...")
            m <- ImageRepArm_batch_R(ifelse(optimizeImageRep, yes = list(ModelList), no = list(ModelList_fixed))[[1]],
                                    m, StateList, seed, MPList, inference)
            StateList <- m[[2]]; m <- m[[1]]
          }
          
          message("Getting baseline outcome...")
          EY0_i <- GetEY0_batch(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference)
          StateList <- EY0_i[[2]]; EY0_i <- EY0_i[[1]]
          
          message("Getting cluster logits...")
          clustT <- GetTau_batch(ModelList, ModelList_fixed, m, vseed, StateList, seed, MPList, inference)
          StateList <- clustT[[2]]; clustT <- clustT[[1]]
          
          if(grepl(heterogeneityModelType,pattern="tarnet")){ 
            Etau_ <- clustT
            EY0Uncert_draw <- cienv$jnp$take(cienv$jax$nn$softplus( c2f(getSDY_params(ModelList, "Y0", "Mean"))),0L)
            EY1Uncert_draw <- cienv$jnp$take(cienv$jax$nn$softplus( c2f(getSDY_params(ModelList, "Y1", "Mean"))),0L)
          }
          if(grepl(heterogeneityModelType,pattern="variational")){ 
            clustT <- MPList[[1]]$cast_to_compute( getClusterSamp_logitInput(c2f(clustT), seed) )
            
            # note: use vseed if vmapping and seed if pre-batched
            ETau_draw <- oryx$distributions$Normal(
              c2f(getTau_means( ModelList )),
              cienv$jax$nn$softplus( c2f(getTau_sds( ModelList ) )))$sample( seed = cienv$jnp$add(seed,111L) )
            ETau_draw <- MPList[[1]]$cast_to_compute( ETau_draw )
      
            # get SD draws
            EY0Uncert_draw <- oryx$distributions$Normal(
                            c2f( getSDY_params(ModelList,"Y0", "Mean") ),
                           cienv$jax$nn$softplus( c2f(getSDY_params(ModelList, "Y0","SD") ) ))$sample(  seed = cienv$jnp$add(seed,324L) )
            EY0Uncert_draw <- MPList[[1]]$cast_to_compute( cienv$jax$nn$softplus( EY0Uncert_draw ) )
      
            EY1Uncert_draw <- oryx$distributions$Normal(
                            c2f( getSDY_params(ModelList, "Y1", "Mean")) ,
              cienv$jax$nn$softplus( c2f(getSDY_params(ModelList, "Y1", "SD"))) )$sample(seed = cienv$jnp$add(seed,3234L))
            EY1Uncert_draw <- MPList[[1]]$cast_to_compute( cienv$jax$nn$softplus( EY1Uncert_draw ) )
      
            # setup likelihood
            Etau_ <- cienv$jnp$sum( cienv$jnp$multiply(ETau_draw, clustT), keepdims=F)
            EY0Uncert_draw <- cienv$jnp$sum(cienv$jnp$multiply( EY0Uncert_draw, clustT),keepdims=F)
            EY1Uncert_draw <- cienv$jnp$sum(cienv$jnp$multiply( EY1Uncert_draw, clustT),keepdims=F)
          } 
          
          #Y_Sigma <- ( cnst(1) - treat) * EY0Uncert_draw  + treat * EY1Uncert_draw 
          Y_Mean <- EY0_i + treat*Etau_
    
          # some commented analyses to triple-check code correctness re: initialization
          # lik_dist_draw <- cienv$jnp$mean( oryx$distributions$Normal(loc = c2f(Y_Mean), scale = c2f(Y_Sigma) )$log_prob(c2f(y)) )
    
          # simplify by uncommenting
          #lik_dist_draw <- cienv$jnp$negative(  cienv$jnp$mean( (Y_Mean - y)^2) ) # minimize sum of squared errors 
          
          #log(cosh(x)) is approximately (x**2) / 2 for small x and abs(x) - log(2) for large x. It is a twice differentiable alternative to the Huber loss.
          lik_dist_draw <- cienv$jnp$negative( cienv$jnp$mean(cienv$optax$log_cosh(Y_Mean, y)) )
    
          # return
          return(list(lik_dist_draw, StateList));
        }
    
        # get KL terms
        getKL <- (function(ModelList){
          # specify some distributions
          if(grepl(heterogeneityModelType, pattern = "variational_minimal")){
            Tau_mean_vec <- getTau_means(ModelList)
            MeanDist_Tau_post = oryx$distributions$Normal(Tau_mean_vec, cienv$jax$nn$softplus(c2f(cienv$jnp$stack(MeanDist_tau[,"SD"]))))
          }
          SDDist_Y1_post = oryx$distributions$Normal(cienv$jnp$stack(SDDist_Y1[,"Mean"]), cienv$jax$nn$softplus(cienv$jnp$stack(SDDist_Y1[,"SD"])))
          SDDist_Y0_post = oryx$distributions$Normal(cienv$jnp$stack(SDDist_Y0[,"Mean"]), cienv$jax$nn$softplus(cienv$jnp$stack(SDDist_Y0[,"SD"])))
    
          # generate KL components
          KLterm <- cienv$jnp$zeros(list(), dtype = ComputeDtype)
          if(! heterogeneityModelType  %in% c("variational_minimal_visualizer")){
            if(nDepth_Dense > 0){ for(dense_ in 1:nDepth_Dense){
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Y0_%s_a$kernel_posterior,DenseProj_Y0_%s_a$kernel_prior)",nDepth_Dense, nDepth_Dense)))
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Clust_%s_a$kernel_posterior,DenseProj_Clust_%s_a$kernel_prior)",nDepth_Dense, nDepth_Dense)))
    
              # comment if not using _b
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Y0_%s_b$kernel_posterior,DenseProj_Y0_%s_b$kernel_prior)",nDepth_Dense, nDepth_Dense)))
              KLterm <- KLterm + eval(parse(text=sprintf("tfd$kl_divergence(DenseProj_Clust_%s_b$kernel_posterior,DenseProj_Clust_%s_b$kernel_prior)",nDepth_Dense, nDepth_Dense)))
            }}
          }
          if(heterogeneityModelType == "variational_minimal"){
            KLterm <- KLterm + cienv$jnp$sum(tfd$kl_divergence(MeanDist_Tau_post, (MeanDist_tau[,"Prior"][[1]])))
          }
          KLterm <- KLterm + cienv$jnp$sum(tfd$kl_divergence(SDDist_Y0_post, (SDDist_Y0[,"Prior"][[1]])))
          KLterm <- KLterm + cienv$jnp$sum(tfd$kl_divergence(SDDist_Y1_post, (SDDist_Y1[,"Prior"][[1]])))
          return( KLterm  )
        })
    
        GetExpectedLikelihood <-  function(ModelList, ModelList_fixed,
                                            m, treat, y, vseed,
                                            StateList, seed, MPList, inference){
          Elik <- cienv$jnp$zeros(list(), dtype = ComputeDtype)
          for(mi_ in 1L:nMonte_variational){ # this should be vmapped 
            LikContrib <-  GetLikelihoodDraw_batch(ModelList, ModelList_fixed,
                                                    m, treat, y, cienv$jnp$add(vseed, mi_),
                                                    StateList, cienv$jnp$add(seed, mi_), MPList, inference)
            StateList <- LikContrib[[2]]; LikContrib <- LikContrib[[1]]
            Elik <- Elik + LikContrib / cienv$jnp$array(f2n(nMonte_variational), ComputeDtype)
          }
          return( list(Elik, StateList) )
        }
    
        GetLoss <- function(ModelList, ModelList_fixed,
                            m, x, treat, y, vseed,
                            StateList, seed, MPList, inference){
            ModelList <- MPList[[1]]$cast_to_compute( ModelList )
            ModelList_fixed <- MPList[[1]]$cast_to_compute( ModelList_fixed )
            m <- MPList[[1]]$cast_to_compute( m )
            x <- MPList[[1]]$cast_to_compute( x )
            treat <- MPList[[1]]$cast_to_compute( treat )
            y <- MPList[[1]]$cast_to_compute( y )
            StateList <- MPList[[1]]$cast_to_compute( StateList )
    
            # likelihood and state updates
            Elik <- GetExpectedLikelihood(ModelList, ModelList_fixed,
                                          m, treat, y, vseed,
                                          StateList, seed, MPList, inference)
            StateList <- Elik[[2]]; Elik <- Elik[[1]]
    
            # minimize negative log likelihood and positive KL term
            if(BAYES_STEP == 1){ minThis <- cienv$jnp$negative( Elik ) } # minimize sum of squares (-- = +)
            if(BAYES_STEP == 2){ minThis <- CombineLikelihoodWithKL( Elik, klContrib ) }
    
            message("Returning loss + state...")
            minThis <- MPList[[1]]$cast_to_output( minThis ) # compute to output dtype
            if(image_dtype_char == "float16"){ 
              minThis <- MPList[[2]]$scale( minThis ) # scale loss
            }
            return( list(minThis, StateList)  )
        }
    
        CombineLikelihoodWithKL <- ( function(lik_, kl_){
          minThis <- cienv$tf$negative(cienv$jnp$sum( lik_ ))
          if(BAYES_STEP == 2){minThis <- minThis + KL_wt * kl_ }
          minThis <- minThis / cnst(as.numeric(batchSize)) # normalize
        })
    
        # set state and model lists
        gc(); cienv$py_gc$collect()
        GradAndLossAndAux <-  cienv$eq$filter_jit( cienv$eq$filter_value_and_grad( GetLoss, has_aux = T) )
        if(!optimizeImageRep){
          ModelList <- c(DenseList, CausalList)
          ModelList_fixed <- ImageModel_And_State_And_MPPolicy_List[[1]]
        }
        if(optimizeImageRep){
          ModelList <- c(ImageModel_And_State_And_MPPolicy_List[[1]], DenseList, CausalList)
          ModelList_fixed <- cienv$jnp$array(0.)
        }
        StateList <- c(ImageModel_And_State_And_MPPolicy_List[[2]], DenseStateList)
        MPList <- list(cienv$jmp$Policy(compute_dtype = ComputeDtype, 
                                  param_dtype = cienv$jnp$float32,
                                  output_dtype= (OutputDtype <- cienv$jnp$float32)),
                       cienv$jmp$DynamicLossScale(loss_scale = cienv$jnp$array(2^12,dtype = OutputDtype),
                                            min_loss_scale = cienv$jnp$array(1.,dtype = OutputDtype),
                                            period = 50L))
        ModelList <- MPList[[1]]$cast_to_param( ModelList )
        ModelList_fixed <- MPList[[1]]$cast_to_param( ModelList_fixed )
        rm( ImageModel_And_State_And_MPPolicy_List, DenseStateList, DenseList, CausalList )

        message("Define optimizer and training step...")
        LocalFxnSource(TrainDefine, evaluation_environment = environment())
    
        keys2indices_list <- tapply(1:length(imageKeysOfUnits), imageKeysOfUnits, c)
        if(BAYES_STEP == 2){
          eval(parse(text = sprintf("rm(%s)",paste(ls()[grepl(ls(),pattern="HASH818")] ,collapse= ',') )))
        }
    
        # training loop
        gc(); cienv$py_gc$collect()
        IndicesByW <- tapply(1:length(obsW),obsW,c)
        UniqueImageKeysByW <- tapply(imageKeysOfUnits,obsW,function(zer){sort(unique(zer))})
        L2grad_vec <- loss_vec <- rep(NA,times=(nSGD))
        keysUsedInTraining <- tauMeans <- c();
        justCheckCrossFitter <- F; if(DoTraining <- T){ 
          LocalFxnSource(TrainDo, evaluation_environment = environment())
        }
      }
      
      # remember training sequence 
      trainIndices <- which( imageKeysOfUnits %in% keysUsedInTraining )
      testIndices <- which( !imageKeysOfUnits %in% keysUsedInTraining )
    
    if(!justCheckCrossFitter){ 
      message("Getting predicted quantities...")
      #print("DEBUG MODE IS ON IN GetSummaries()");GetSummaries <- (function(ModelList, ModelList_fixed,
      GetSummaries <- cienv$eq$filter_jit(function(ModelList, ModelList_fixed,
                                               m, vseed,
                                               StateList, seed, MPList){
          ModelList <- MPList[[1]]$cast_to_compute( ModelList )
          ModelList_fixed <- MPList[[1]]$cast_to_compute( ModelList_fixed )
          m <- MPList[[1]]$cast_to_compute( m )
          StateList <- MPList[[1]]$cast_to_compute( StateList )
        
          # image representation model
          if(SharedImageRepresentation){
            m <- ImageRepArm_batch_R(ifelse(optimizeImageRep, yes = list(ModelList), no = list(ModelList_fixed))[[1]],
                                              m, StateList, seed, MPList, T)
            StateList <- m[[2]] ; m <- m[[1]]
          }
          y0_ <- sapply(1L:nMonte_predictive, function(iter){ list(cienv$jnp$expand_dims(
                           GetEY0_batch(ModelList, ModelList_fixed,
                                        m,  cienv$jnp$add(vseed,iter), StateList, cienv$jnp$add(seed,iter), MPList, T)[[1]], 0L)) })
          y1_ <- sapply(1L:nMonte_predictive, function(iter){ list(cienv$jnp$expand_dims(
                          GetEY1_batch(ModelList, ModelList_fixed,
                                        m, cienv$jnp$add(vseed,iter), StateList, cienv$jnp$add(seed,iter), MPList, T), 0L)) })
          y0_ <- cienv$jnp$concatenate(y0_,0L); y0_ <- cienv$jnp$mean(y0_,0L)
          y1_ <- cienv$jnp$concatenate(y1_,0L); y1_ <- cienv$jnp$mean(y1_,0L)
    
          # get predictions
          ClusterProbs_est_ <- cienv$jnp$concatenate(sapply(1L:nMonte_predictive,function(iter){
                              cienv$jnp$expand_dims(cienv$jax$nn$softmax(
                                      GetTau_batch(ModelList, ModelList_fixed,
                                                   m, cienv$jnp$add(vseed,iter),
                                                   StateList, cienv$jnp$add(seed,iter), MPList, T)[[1]]), 0L) } ),0L)
          ClusterProbs_std_ <- cienv$jnp$std(ClusterProbs_est_,0L)
          ClusterProbs_est_ <- cienv$jnp$mean(ClusterProbs_est_,0L)
          ClusterProbs_lower_conf_ <- ClusterProbs_est_ - ClusterProbs_std_
    
          return( list("y0_"=y0_, 
                       "y1_"=y1_,
                      "ClusterProbs_est_"=ClusterProbs_est_,
                      "ClusterProbs_lower_conf_"=ClusterProbs_lower_conf_,
                      "ClusterProbs_std_"=ClusterProbs_std_) )
        })
    
        inference_counter <- 0; nUniqueKeys <- length(unique(imageKeysOfUnits))
        KeyQuantCuts <- gtools::quantcut(1:nUniqueKeys, q = ceiling( nUniqueKeys / (batchSize*0.5) ))
        #passedIterator <- NULL; Results_by_keys <- replicate(nUniqueKeys,list());for(zer in 1:nUniqueKeys){ # use when incorporating X's
        passedIterator <- NULL; Results_by_keys <- replicate(length(unique(KeyQuantCuts)),list());
        for(cut_ in unique(KeyQuantCuts)){ # use when not incorporating X's
          inference_counter <- inference_counter + 1
          zer <- which(cut_ == KeyQuantCuts)
          atP <- max(zer) / nUniqueKeys
          if( any(zer %% 50 == 0) | 1 %in% zer ){ message(sprintf("Proportion done (inference): %.3f", atP)) }
            {
              setwd(orig_wd);ds_next_in <- GetElementFromTfRecordAtIndices(
                                                             uniqueKeyIndices = which(unique(imageKeysOfUnits) %in% unique(imageKeysOfUnits)[zer]),
                                                             filename = file,
                                                             iterator = passedIterator,
                                                             readVideo = useVideoIndicator,
                                                             image_dtype = image_dtype_tf,
                                                             nObs = length(unique(imageKeysOfUnits)),
                                                             return_iterator = T );setwd(new_wd)
              passedIterator <- ds_next_in[[2]]
              key_ <- try(unlist(  lapply( p2l(ds_next_in[[1]][[3]]$numpy()) , as.character) ), T)
              ds_next_in <- ds_next_in[[1]]
    
              # deal with batch 1 case here
              if(length(ds_next_in[[1]]$shape) == 3 & dataType == "image"){ ds_next_in[[1]] <- cienv$tf$expand_dims(ds_next_in[[1]], 0L) }
              if(length(ds_next_in[[1]]$shape) == 4 & dataType == "video"){ ds_next_in[[1]] <- cienv$tf$expand_dims(ds_next_in[[1]], 0L) }
              ds_next_in <- ds_next_in[[1]]
          }
    
          # get summaries and save
          GottenSummaries <- GetSummaries(ModelList, ModelList_fixed,
                                          InitImageProcessFn(cienv$jnp$array(ds_next_in),  cienv$jax$random$PRNGKey(ai(runif(1,0, 10000))), inference = T),
                                          cienv$jax$random$split(cienv$jax$random$PRNGKey(ai(runif(1,0, 10000))), ds_next_in$shape[[1]]),
                                          StateList, cienv$jax$random$PRNGKey(ai(runif(1,0,100000))), MPList)

          ret_list <- list("y0_" = as.matrix2(GottenSummaries$y0_),
                           "y1_" = as.matrix2(GottenSummaries$y1_),
                           "ClusterProbs_est_" = as.matrix2(GottenSummaries$ClusterProbs_est_),
                           "ClusterProbs_lower_conf_" = as.matrix2(GottenSummaries$ClusterProbs_lower_conf_),
                           "ClusterProbs_std_" = as.matrix2(GottenSummaries$ClusterProbs_std_),
                           "key" = as.matrix( key_) )
          Results_by_keys[[inference_counter]] <- ret_list
        }
        message("Done getting predicted quantities...")
        Results_by_keys_list <- Results_by_keys
        Results_by_keys <- apply(do.call(rbind, Results_by_keys_list), 2, 
                                 function(zer){(do.call(rbind,zer))})
    
        # re-order data
        if(!"matrix" %in% class(Results_by_keys)){
          Results_by_keys <- lapply(Results_by_keys, function(zer){
                                          zer[match(as.character(imageKeysOfUnits), 
                                                    as.character(unlist(  Results_by_keys[["key"]] )) ),]})
        }
        if("matrix" %in% class(Results_by_keys)){
          Results_by_keys <- as.data.frame( Results_by_keys[match(as.character(imageKeysOfUnits), 
                                as.character(unlist(  Results_by_keys[,"key"] ))),] ) 
          Results_by_keys[,1:4] <- apply(Results_by_keys[,1:4], 2, f2n)
        }
        
        # mean(as.character(Results_by_keys$key) == as.character(imageKeysOfUnits)) # sanity value of 1 
  
        # process all outcomes
        Y0_est_mat <- cbind(Y0_est_mat, Y0_est <- Rescale(unlist(Results_by_keys$y0_), doMean = T))
        Y1_est_mat <- cbind(Y1_est_mat, Y1_est <- Rescale(unlist(Results_by_keys$y1_), doMean = T))
        Yobs_est <- Y1_est * obsW + Y0_est * (1-obsW)
        tau_est_mat <- cbind(tau_est_mat,  tau_est <- ( Y1_est - Y0_est))# Y(w)_est already re-scaled 
        
        # diagnostics 
        plot(Rescale(obsY, doMean = T), Yobs_est, col = obsW + 1 ); abline(a=0,b=1)
        print( summary(lm(obsY~Yobs_est)) ) 

        Yobs_est_out <- Yobs_est[ testIndices ]
        Loss_out_baseline_vec <- c(Loss_out_baseline_vec,
                                   Loss_baseline_out <- mean( (Yobs_est_out - mean(Yobs_est[ trainIndices ]))^2 )^0.5)
        Loss_out_vec <- c(Loss_out_vec, 
                          Loss_out <- mean( (Yobs_est_out - obsY_orig[testIndices] )^2)^0.5)
    }
  
        # save test indices 
        TestIndices_list[[trainCounter]] <- testIndices
        TrainIndices_list[[trainCounter]] <- trainIndices
    } # end k fold 
  }
  
  if(justCheckCrossFitter){
    return(list("CF_info" = list("TrainIndices_list"=TrainIndices_list, "TestIndices_list"=TestIndices_list)))
  }
  
    tau_est_cf <- Y0_est_cf <- Y1_est_cf <- replicate(rep(NA,times=nrow(Y0_est_mat)), n = trainCounter)
    for(theIt in 1:trainCounter){
      Y0_est_cf[TestIndices_list[[theIt]],theIt] <- (Y0_est_mat[TestIndices_list[[theIt]],theIt] )
      Y1_est_cf[TestIndices_list[[theIt]],theIt] <- (Y1_est_mat[TestIndices_list[[theIt]],theIt] )
      tau_est_cf[TestIndices_list[[theIt]],theIt] <- (tau_est_mat[TestIndices_list[[theIt]],theIt] )
    }
    print( mean( 1:length(obsY) %in% unlist(TestIndices_list) ))  # sanity value = 1 
    print( sprintf("Sanity %s?",cf_iters) )
    print( table( table(unlist(TestIndices_list)) ) ) 
    Y0_est_se <- (rowMeans(Y0_est_cf^2,na.rm=T) - rowMeans(Y0_est_cf,na.rm=T)^2)^0.5
    Y1_est_se <- (rowMeans(Y1_est_cf^2,na.rm=T) - rowMeans(Y1_est_cf,na.rm=T)^2)^0.5
    tau_est_se <- (rowMeans(tau_est_cf^2,na.rm=T) - rowMeans(tau_est_cf,na.rm=T)^2)^0.5
    
    tau_est <- rowMeans(tau_est_cf, na.rm = T) 
    # hist(tau_est)
    # plot(tau_est, SwappedRowsIndicator); cor(tau_est, SwappedRowsIndicator)
    Y0_est <- rowMeans(Y0_est_cf, na.rm = T) 
    Y1_est <- rowMeans(Y1_est_cf, na.rm = T) 
    # plot(tau_est, Y1_est-Y0_est,col=obsW+1);abline(a=0,b=1)
    Yobs_est <- Y1_est * obsW + Y0_est * (1-obsW )
    
    # process outcome predictions
    W_test <- obsW[testIndices]
    Y_test_truth <- obsY[testIndices]
    Y_test_est <- Yobs_est[testIndices]
    rm( Results_by_keys_list )

    # process cluster data
    ClusterProbs_lower_conf <- Results_by_keys$ClusterProbs_lower_conf_
    ClusterProbs_std <- Results_by_keys$ClusterProbs_std_
    
    ClusterProbs_est_full <-  as.matrix(Results_by_keys$ClusterProbs_est_)
    ClusterProbs_est <- ClusterProbs_est_full[,2]
    Clust_probs_marginal_final <- colMeans( ClusterProbs_est_full )
    gc(); cienv$py_gc$collect()

  # get cluster probs
  if(grepl(heterogeneityModelType, pattern = "variational")){
    message("Summarizing results...")
    SDDist_Y1_post <- oryx$distributions$Normal(getSDY_params(ModelList, "Y1", "Mean"),
                                cienv$jax$nn$softplus(getSDY_params(ModelList, "Y1", "SD")))
    SDDist_Y0_post <- oryx$distributions$Normal(getSDY_params(ModelList, "Y0", "Mean"),
                                 cienv$jax$nn$softplus(getSDY_params(ModelList, "Y0", "SD")))
    Sigma1_sd_vec <- as.numeric2(cienv$jnp$mean(cienv$jax$nn$softplus(SDDist_Y1_post$sample(100L, seed = cienv$jax$random$PRNGKey(4L))),0L))
    Sigma0_sd_vec <- as.numeric2(cienv$jnp$mean(cienv$jax$nn$softplus(SDDist_Y0_post$sample(100L, seed = cienv$jax$random$PRNGKey(4L))),0L))

    # get uncertainties
    if(  grepl(heterogeneityModelType, pattern="variational_minimal")  ){
      tau_vec <- as.numeric2( getTau_means(ModelList) ) * Y_sd
      Tau_mean_return_vec <- Rescale( as.numeric2(Tau_mean_vec <- getTau_means(ModelList)), doMean = F)
      Tau_mean_return_sd_vec <- Rescale(as.numeric2(Tau_mean_sd_vec <- cienv$jax$nn$softplus(getTau_sds(ModelList))),
                                      doMean = F)
      MeanDist_Tau_post = (oryx$distributions$Normal(Tau_mean_vec, Tau_sd_vec <- cienv$jax$nn$softplus(getTau_sds(ModelList))))
      Tau_sd_vec_ <- as.numeric2(cienv$tf$sqrt(cienv$tf$math$reduce_variance(MeanDist_Tau_post$sample(
                              100L, seed = cienv$jax$random$PRNGKey(45L)),0L)))
      Tau_sd_vec <- Y_sd*sqrt(   Sigma1_sd_vec^2 + Sigma0_sd_vec^2 + Tau_sd_vec_^2 )
    }
  }

  # transportability analysis
  cluster_prob_transport_means <- NULL; if(!is.null(transportabilityMat)){
    message("Getting posterior predictive mean probabilities for transportability analysis...")
    {
      if(grepl(heterogeneityModelType, pattern = "variational")){ GetProbAndExpand <- function(m){cienv$jnp$expand_dims( cienv$jax$nn$softmax(GetTau(m,inference = T)),0L) }}
      if(grepl(heterogeneityModelType, pattern = "tarnet")){ GetProbAndExpand <- function(m){cienv$jnp$expand_dims( GetTau(m,inference = T),0L) }}
      full_tab <- sort( 1:nrow(transportabilityMat) %% round(nrow(transportabilityMat)/max(1,round(batchFracOut*batchSize))));
      cluster_prob_transport_info <- tapply(1:nrow(transportabilityMat),full_tab,function(zer){
        gc(); cienv$py_gc$collect()
        atP <- max(  zer / nrow(transportabilityMat))
        if((round(atP,2)*100) %% 10 == 0){ message(atP) }
        {
          setwd(orig_wd); ds_next_in <- GetElementFromTfRecordAtIndices(
                                                         uniqueKeyIndices = which(unique(imageKeysOfUnits) %in% imageKeysOfUnits[zer]),
                                                         filename = file,
                                                         readVideo = useVideoIndicator,
                                                         image_dtype = image_dtype_tf,
                                                         nObs = nrow(transportabilityMat) ); setwd(new_wd)
          if(length(ds_next_in[[1]]$shape) == 3 & dataType == "image"){ ds_next_in[[1]] <- cienv$tf$expand_dims(ds_next_in[[1]], 0L) }
          if(length(ds_next_in[[1]]$shape) == 4 & dataType == "video"){ ds_next_in[[1]] <- cienv$tf$expand_dims(ds_next_in[[1]], 0L) }
          ds_next_in <- ds_next_in[[1]]
        }
        im_keys <-  InitImageProcessFn( cienv$jnp$array(ds_next_in),  cienv$jax$random$PRNGKey(600L), inference = T)
        pred_ <- replicate(nMonte_predictive,cienv$np$array(GetProbAndExpand(im_keys) ))
        list("mean"=apply(pred_[1,,,],1:2,mean),
             "var"=apply(pred_[1,,,],1:2,var))
      })
      cluster_prob_transport_info <- do.call(rbind,cluster_prob_transport_info)
      cluster_prob_transport_means <- do.call(rbind, cluster_prob_transport_info[,1])
      cluster_prob_transport_var <- do.call(rbind, cluster_prob_transport_info[,2])
      colnames(cluster_prob_transport_means) <- paste('mean_k',1:ncol(cluster_prob_transport_means), sep = "")
      colnames(cluster_prob_transport_var) <- paste('var_k',1:ncol(cluster_prob_transport_means), sep = "")
      }
    if(TRUE %in% transportabilityMat){
        transportabilityMat <- as.data.frame(cbind(
                                     "key"=imageKeysOfUnits,
                                     "long"=long,
                                     "lat"=lat))
        cluster_prob_transport_means <- Results_by_keys$ClusterProbs_est_
        cluster_prob_transport_var <-  Results_by_keys$ClusterProbs_std_^2
      }
    transportabilityMat <- try(cbind(transportabilityMat,
                                   cluster_prob_transport_means,
                                   cluster_prob_transport_var),T)
  }

  # perform plots
  if( changed_wd ){ setwd(  orig_wd  ) }
  if( plotResults == T){
    LocalFxnSource(CausalImageHeterogeneity_plot, evaluation_environment = environment())
  }
  try(setwd(orig_wd), T)
  
  return( list(
                 "clusterTaus_mean" = as.numeric2(Tau_mean_return_vec),
                 "clusterTaus_sd" = as.numeric2(Tau_mean_return_sd_vec),
                 "clusterSigmas" = Tau_sd_vec,
                 "clusterProbs_mean" = ClusterProbs_est_full,
                 "clusterProbs_sd" = ClusterProbs_std,
                 "clusterProbs_lowerConf" = ClusterProbs_lower_conf,
                 "impliedATE" = mean(  tau_est ),
                 "individualTau_est" = tau_est,
                 "Y0_est" = Y0_est,
                 "Y1_est" = Y1_est,
                 "Yobs_est" = Yobs_est,
                 
                 "Y0_est_se" = Y0_est_se, 
                 "Y1_est_se" = Y1_est_se, 
                 "tau_est" = tau_est, 
                 "tau_est_se" = tau_est_se, 
                 
                 "loss_vec" = loss_vec,
                 "Loss_baseline_out" = Loss_baseline_out,
                 "Loss_out" = Loss_out,
                 "CF_info" = list("cf_keys_split" = cf_keys_split,
                                  "Loss_out_vec" = Loss_out_vec,
                                  "Loss_out_baseline_vec" = Loss_out_baseline_vec,
                                  "Y0_est_mat" = Y0_est_mat,
                                  "Y1_est_mat" = Y1_est_mat, 
                                  "tau_est_mat" = tau_est_mat,
                                  "TestIndices_list" = TestIndices_list,
                                  "TrainIndices_list" = TrainIndices_list),
                 "transportabilityMat" = transportabilityMat,
                 "plottedCoordinatesList" = plotting_coordinates_list,
                 "whichNA_dropped" = whichNA_dropped) )
}

