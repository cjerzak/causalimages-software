#!/usr/bin/env Rscript
#' Perform causal estimation under image confounding
#'
#' @param obsW A numeric vector where `0`'s correspond to control units and `1`'s to treated units.
#' @param obsY A numeric vector containing observed outcomes.
#' @param imageKeysOfUnits A vector of length `length(obsY)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into the package to call images into memory.
#' @param file Path to a tfrecord file generated by `WriteTfRecord`.
#' @param conda_env A `conda` environment where computational environment lives, usually created via `causalimages::BuildBackend()`. Default = `"CausalImagesEnv"`.
#' @param conda_env_required A Boolean stating whether use of the specified conda environment is required.
#' @param X An optional numeric matrix containing tabular information used if `orthogonalize = T`. `X` is normalized internally and salience maps with respect to `X` are transformed back to the original scale.
#' @param long,lat Optional vectors specifying longitude and latitude coordinates for units. Used only for describing highest and lowest probability neighorhood units if specified.
#' @param transportabilityMat Optional matrix with a column named `imageKeysOfUnits` specifying keys to be used by the package for generating treatment effect predictions for out-of-sample points.
#' @param figuresTag A string specifying an identifier that is appended to all figure names.
#' @param figuresPath A string specifying file path for saved figures made in the analysis.
#' @param plotBands An integer or vector specifying which band position (from the image representation) should be plotted in the visual results. If a vector, `plotBands` should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RBG plotting).
#' @param nSGD Number of stochastic gradient descent (SGD) iterations. Default = `400L`
#' @param nBoot Number of bootstrap iterations for uncertainty estimation.
#' @param batchSize Batch size used in SGD optimization. Default = `50L`.
#' @param useTrainingPertubations Boolean specifying whether to randomly the image axes during training to reduce overfitting.
#' @param optimizeImageRep Boolean specifying whether to optimize over the image model representation (or only over downstream parameters).
#' @param dropoutRate Droppout rate used in training used to prevent overfitting (`dropoutRate = 0` corresponds to no dropout).
#' @param testFrac Default = `0.1`. Fraction of observations held out as a test set to evaluate out-of-sample loss values.
#' @param strides (default = `2L`) Integer specifying the strides used in the convolutional layers.
#' @param plotResults (default = `T`) Should analysis results be plotted?
#' @param dataType (default = `"image"`) String specifying whether to assume `"image"` or `"video"` data types.
#' @param nWidth_ImageRep Integer specifying width of image model representation.
#' @param nDepth_ImageRep Integer specifying depth of image model representation.
#' @param nWidth_Dense Integer specifying width of image model representation.
#' @param nDepth_Dense Integer specifying depth of dense model representation.
#' @param kernelSize Dimensions used in spatial convolutions.
#' @param temporalKernelSize Dimensions used in temporal convolutions (if `dataType = "video"``)
#' @param TfRecords_BufferScaler The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#'
#' @return Returns a list consisting of
#' \itemize{
#'   \item `ATE_est` ATE estimate.
#'   \item `ATE_se` Standard error estimate for the ATE.
#'   \item `plotResults` If set to `TRUE`, causal salience plots are saved to disk, characterizing the image confounding structure. See references for details.
#' }
#'
#' @section References:
#' \itemize{
#' \item  Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Integrating Earth Observation Data into Causal Inference: Challenges and Opportunities. *ArXiv Preprint*, 2023.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md

AnalyzeImageConfounding <- function(
                                   obsW,
                                   obsY,
                                   X = NULL,
                                   file = NULL,
                                   imageKeysOfUnits = NULL,
                                   nBoot = 10L,
                                   inputAvePoolingSize = 1L,
                                   useTrainingPertubations = T,

                                   orthogonalize = F,
                                   transportabilityMat = NULL,
                                   lat = NULL,
                                   long = NULL,
                                   conda_env = "CausalImagesEnv",
                                   conda_env_required = T,
                                   Sys.setenv_text = NULL,

                                   figuresTag = NULL,
                                   figuresPath = "./",
                                   plotBands = 1L,
                                   plotResults = T,

                                   optimizeImageRep = T,
                                   nWidth_ImageRep = 64L,  nDepth_ImageRep = 1L, temporalKernelSize = 2L, kernelSize = 5L,
                                   nWidth_Dense = 64L,  nDepth_Dense = 1L,
                                   imageModelClass = "VisionTransformer",

                                   strides = 2L,
                                   nDepth_TemporalRep = 3L,
                                   patchEmbedDim = 16L,
                                   dropoutRate = 0.1,
                                   batchSize = 16L,
                                   nSGD  = 400L,
                                   testFrac = 0.05,
                                   TfRecords_BufferScaler = 4L,
                                   learningRateMax = 0.001,
                                   TFRecordControl = NULL, 
                                   dataType = "image",
                                   image_dtype = "float16",
                                   atError = "stop", # stop or debug
                                   seed = NULL){
  {
    print2("Establishing connection to computational environment (build via causalimages::BuildBackend())")
    if(!is.null(conda_env)){
      try(reticulate::use_condaenv(conda_env, required = conda_env_required),T)
    }
    # note: for balanced training, generate two tf records
    if(!is.null(Sys.setenv_text)){ eval(parse(text = Sys.setenv_text)) }
    jax <<- reticulate::import("jax")
    jnp <<- reticulate::import("jax.numpy")
    np <<- reticulate::import("numpy")
    jmp <<- reticulate::import("jmp")
    optax <<- reticulate::import("optax")
    eq <<- reticulate::import("equinox")
    (py_gc <<- reticulate::import("gc"))$collect(); gc();
    # tf$config$get_visible_devices() # confirm CPU only
    # try(tf$config$experimental$set_visible_devices(list(), "GPU"), T)

    # cleanup
    print2(sprintf("Default device: %s",jnp$array(0.)$device()))

    # set float type
    library( tensorflow );
    if((image_dtype_char <- image_dtype) == "float16"){  image_dtype_tf <- tf$float16; image_dtype <- jnp$float16 }
    if(image_dtype_char == "bfloat16"){  image_dtype_tf <- tf$bfloat16; image_dtype <- jnp$bfloat16 }
    if(is.null(seed)){ seed <- ai(runif(1,1,10000)) }
    obsW <- f2n(obsW); obsY <- f2n(obsY)
  }

  if(!optimizeImageRep & nDepth_ImageRep > 1){
    if(atError == "stop"){ stop("Stopping: When optimizeImageRep = T, nDepth_ImageRep must be 1L") }
    if(atError == "debug"){ browser() }
  }
  FigNameAppend <- sprintf("KW%s_InputAvePool%s_OptimizeImageRep%s_Tag%s",
                           kernelSize, inputAvePoolingSize,
                           optimizeImageRep, figuresTag)
  tagInFigures <- !is.null(figuresTag)
  figuresTag < ifelse(is.null(figuresTag), yes = "", no = figuresTag)

  # make all directory logic explicit
  loss_vec <- NULL
  orig_wd <- getwd()
  if( (cond1 <- substr(figuresPath, start = 0, stop = 1) == ".")  ){
    figuresPath <- gsub(figuresPath, pattern = '\\.', replacement = orig_wd)
  }
  if(!dir.exists(figuresPath)){ dir.create(figuresPath) }

  if(is.null(imageKeysOfUnits) & !is.null(imageKeysOfUnits)){ imageKeysOfUnits <- keys }
  if(batchSize > length(obsW)){ batchSize <- round(length(obsW) * 0.90) }

  XisNull <- is.null( X  )
  if(!XisNull){ if(!"matrix" %in% class(X)){
    print2("Coercing X to matrix class..."); X <- as.matrix(  X )
  } }

  if( !XisNull ){ if(is.na(sum(X))){ stop("Error: is.na(sum(X)) is TRUE; check for NAs or that all variables are numeric.") }}
  if( !XisNull ){ if(any(apply(X,2,sd) == 0)){ stop("Error: any(apply(X,2,sd) == 0) is TRUE; a column in X seems to have no variance; drop column!") }}
  if( XisNull ){ X <- matrix( rnorm(length(obsW)*2, sd = 0.01 ), ncol = 2) }
  X <- t( (t(X) - (X_mean <- colMeans(X)) ) / (0.001+(X_sd <- apply(X,2,sd))) )
  {
    if(is.null(file)){stop("No file specified for tfrecord!")}
    changed_wd <- F; if(  !is.null(  file  )  ){
      print2("Establishing connection with tfrecord")
      tf_record_name <- file
      if( !grepl(tf_record_name, pattern = "/") ){
        tf_record_name <- paste("./",tf_record_name, sep = "")
      }
      tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
      new_wd <- paste(tf_record_name[-length(tf_record_name)], collapse = "/")
      print2( sprintf("Temporarily re-setting the wd to %s", new_wd ) )
      changed_wd <- T; setwd( new_wd )

      # define
      tf_dataset <- tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )
      
      # helper functions
      useVideoIndicator <- dataType == "video"
      getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideoIndicator, image_dtype = image_dtype_tf)} )
        return( dataset <- dataset$batch( ai(max(2L,round(batchSize/2L)  ))) )
      }

      # setup iterators - skip the first test_size observations 
      if(!is.null(TFRecordControl)){
        getParsed_tf_dataset_train_Select <- function( tf_dataset ){
          return( tf_dataset$map( function(x){ parse_tfr_element(x, readVideo = useVideoIndicator, image_dtype = image_dtype_tf)},
                                     num_parallel_calls = tf$data$AUTOTUNE) ) 
        }
        getParsed_tf_dataset_train_BatchAndShuffle <- function( tf_dataset ){
          tf_dataset <- tf_dataset$shuffle(buffer_size = tf$constant(ai(TfRecords_BufferScaler*batchSize),dtype=tf$int64),
                                     reshuffle_each_iteration = T) # set false so same train/test split each re-initialization
          tf_dataset <- tf_dataset$batch(  ai(batchSize)   )
          tf_dataset <- tf_dataset$prefetch( tf$data$AUTOTUNE ) 
          return( tf_dataset )
        }
        tf_dataset_train_control <- getParsed_tf_dataset_train_Select(
                            tf_dataset$skip(  test_size <-  ai( TFRecordControl$nTest)  )
                            )$take( ai(TFRecordControl$nControl) ) 
        tf_dataset_train_treated <- getParsed_tf_dataset_train_Select(
                              tf_dataset$skip( test_size <-  ai( TFRecordControl$nTest) )
                              )$skip( ai(TFRecordControl$nControl)+1L) 
        tf_dataset_train_treated <- getParsed_tf_dataset_train_BatchAndShuffle(tf_dataset_train_treated)
        tf_dataset_train_control <- getParsed_tf_dataset_train_BatchAndShuffle(tf_dataset_train_control)
        ds_iterator_train_treated <- reticulate::as_iterator( tf_dataset_train_treated )
        ds_iterator_train_control <- reticulate::as_iterator( tf_dataset_train_control )
        ds_iterator_train <- reticulate::as_iterator( tf_dataset_train_control )
        if(T == F){ 
        # get next batch  zzz xxx 
        #ds_iterator_train <- reticulate::as_iterator( tf_dataset_train_treated )
        ds_iterator_train <- reticulate::as_iterator( tf_dataset_train_treated  )
        ds_next_train <- ds_iterator_train$`next`()
        
        # select batch indices based on keys
        batch_keys <- unlist(  lapply( p2l(ds_next_train[[3]]$numpy()), as.character) )
        keys2indices_list <- tapply(1:length(imageKeysOfUnits), imageKeysOfUnits, c)
        batch_indices <- sapply(batch_keys,function(key_){ f2n( sample(as.character( keys2indices_list[[key_]] ), 1) ) })
        ds_next_train <- ds_next_train[[1]]
        # imageKeysOfUnits[batch_indices]
        #jnp$array(obsW[batch_indices], dtype = jnp$float16)
        # which(obsW == 1)
        # which(imageKeysOfUnits %in% imageKeysOfUnits[batch_indices])
      }
      }
      if(is.null(TFRecordControl)){
        getParsed_tf_dataset_train <- function( tf_dataset ){
          dataset <- tf_dataset$map( function(x){ parse_tfr_element(x, readVideo = useVideoIndicator, image_dtype = image_dtype_tf)},
                                     num_parallel_calls = tf$data$AUTOTUNE)
          dataset <- dataset$shuffle(buffer_size = tf$constant(ai(TfRecords_BufferScaler*batchSize),dtype=tf$int64),
                                     reshuffle_each_iteration = F) # set false so same train/test split each re-initialization
          dataset <- dataset$batch(  ai(batchSize)   )
          dataset <- dataset$prefetch( tf$data$AUTOTUNE ) 
          return( dataset  )
        }
        
        # shuffle (generating different train/test splits)
        tf_dataset <- tf$data$Dataset$shuffle(  tf_dataset, buffer_size = tf$constant(ai(10L*TfRecords_BufferScaler*batchSize),dtype=tf$int64), reshuffle_each_iteration = F )
        tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset$skip(test_size <-  as.integer(round(testFrac * length(unique(imageKeysOfUnits)) )) ) )$`repeat`(  -1L )
        ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
      }
      # define inference iterator 
      tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )
      ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )
    }

    if(useTrainingPertubations){
      trainingPertubations_OneObs <- function(im_, key){
         AB <- ifelse(dataType == "video", yes = 1L, no = 0L)
         which_path <- jnp$squeeze(jax$random$categorical(key = key, logits = jnp$array(t(rep(0, times = 4)))),0L)# generates random # from 0L to 3L
         im_ <- jax$lax$cond(jnp$equal(which_path,jnp$array(0L)), true_fun = function(){ jnp$flip(im_, AB+1L) }, false_fun = function(){im_})
         im_ <- jax$lax$cond(jnp$equal(which_path,jnp$array(2L)), true_fun = function(){ jnp$flip(im_, AB+2L) }, false_fun = function(){im_})
         im_ <- jax$lax$cond(jnp$equal(which_path,jnp$array(3L)), true_fun = function(){ jnp$flip(jnp$flip(im_, AB+1L),AB+2L) }, false_fun = function(){im_})
         return( im_ )
      } 
      trainingPertubations <- jax$vmap(function(im_, key){return( trainingPertubations_OneObs(im_,key) )  }, in_axes = list(0L,0L))
    }

    InitImageProcessFn <- jax$jit(function(im, key, inference){
        # expand dims if needed
        if(length(imageKeysOfUnits) == 1){ im <- jnp$expand_dims(im,0L) }

        # normalize
        im <- (im - NORM_MEAN_array) / NORM_SD_array

        # downshift resolution if desired
        if(inputAvePoolingSize > 1 & dataType == "image"){
          im <- jax$vmap(function(imm){
            jnp$transpose(  eq$nn$AvgPool2d(kernel_size = ai(c(inputAvePoolingSize,inputAvePoolingSize)),
                            stride = ai(c(inputAvePoolingSize,inputAvePoolingSize)))(
                          jnp$transpose(imm,c(2L,0L,1L)  )), c(1L,2L, 0L)) }, 0L)(im)
        }
        
        # training pertubations
        if(useTrainingPertubations){
          im <- jax$lax$cond(inference, 
                             true_fun = function(){ im }, 
                             false_fun = function(){  trainingPertubations(im, 
                                                              jax$random$split(key,im$shape[[1]])) } )
        }
        return( im  )
    })

    # some hyperparameters parameters
    figuresPath <- paste(strsplit(figuresPath,split="/")[[1]],collapse = "/")

    # get first iter batch for initializations
    print2("Calibrating first moments for input data normalization...")
    NORM_SD <- NORM_MEAN <- c(); for(momentCalIter in 1:(momentCalIters<-20)){
      # get a data batch 
      ds_next_train <- ds_iterator_train$`next`()
      
      # setup normalizations
      ApplyAxis <- ifelse(dataType == "video", yes = 5, no = 4)

      # update normalizations
      NORM_SD <- rbind(NORM_SD, apply(as.array(ds_next_train[[1]]),ApplyAxis,sd))
      NORM_MEAN <- rbind(NORM_MEAN, apply(as.array(ds_next_train[[1]]),ApplyAxis,mean))
    }
    NORM_SD <- apply(NORM_SD,2,median)
    NORM_MEAN <- apply(NORM_MEAN,2,mean)
    NORM_MEAN_array <- jnp$array(array(NORM_MEAN,dim=c(1,1,1,length(NORM_MEAN))),image_dtype)
    NORM_SD_array <- jnp$array(array(NORM_SD,dim=c(1,1,1,length(NORM_SD))),image_dtype)
    if(dataType == "video"){
      NORM_MEAN_array <- jnp$expand_dims(NORM_MEAN_array, 0L)
      NORM_SD_array <- jnp$expand_dims(NORM_SD_array, 0L)
    }
    EP_LSMOOTH <- jnp$array(0.05)
    py_gc$collect()

    # set up holders
    sigmoid <- function(x){1/(1+exp(-x))}
    prW_est <- rep(NA,times = length(obsW))
    tauHat_propensity_vec <- tauHat_propensityHajek_vec <- rep(NA,times = nBoot+1)
    if(!optimizeImageRep){
      # define train/test indices based on out of sample keys
      outKeys <- sample(unique(imageKeysOfUnits), max(c(2,length(unique(imageKeysOfUnits))*testFrac)))
      inKeys <- unique(imageKeysOfUnits[!imageKeysOfUnits %in% outKeys])
      testIndices <- (1:length(obsY))[imageKeysOfUnits %in% outKeys]
      trainIndices <- (1:length(obsY))[imageKeysOfUnits %in% inKeys]

      myGlmnet_coefs_mat <- matrix(NA, nrow = nBoot+1,
                                   ncol = nWidth_ImageRep + 1 + ifelse(!XisNull, yes = ncol(X), no = 0))
      for(jr in 1L:(nBoot+1L)){
        if(nBoot > 0L){ print2( sprintf("Bootstrap iteration %s of %s", jr-1L, nBoot) ) } 
        if(jr != (nBoot+1L)){ indices_ <- sample(1:length( imageKeysOfUnits ), length( imageKeysOfUnits ), replace = T) }
        if(jr == (nBoot+1L)){ indices_ <- 1:length( imageKeysOfUnits ) }

        # note: MyEmbeds_ are indexed by the original data ordering, resampling happens later
        {
          setwd(orig_wd); ImageRepresentations <- GetImageRepresentations(
            file = file,
            dataType = dataType,
            InitImageProcess = InitImageProcessFn,
            nWidth_ImageRep = nWidth_ImageRep,
            nDepth_ImageRep = nDepth_ImageRep,
            strides = strides,
            dropoutRate = 0,
            nDepth_TemporalRep = nDepth_TemporalRep,
            patchEmbedDim = patchEmbedDim,
            batchSize = batchSize,
            temporalKernelSize = temporalKernelSize,
            imageModelClass = imageModelClass,
            optimizeImageRep = optimizeImageRep, 
            kernelSize = kernelSize,
            TfRecords_BufferScaler = 3L,
            inputAvePoolingSize = inputAvePoolingSize,
            imageKeysOfUnits = unique(imageKeysOfUnits),  getRepresentations = T,
            returnContents = T,
            bn_momentum = 0.9,
            conda_env = conda_env,
            conda_env_required = conda_env_required,
            Sys.setenv_text = Sys.setenv_text,
            seed = ai(400L + jr)  ); setwd(new_wd)
          ImageRepresentations_df <- as.data.frame(  ImageRepresentations$ImageRepresentations )
          row.names(ImageRepresentations_df) <- as.character(unique(imageKeysOfUnits))
          ImageRepresentations_df <- ImageRepresentations_df[as.character(imageKeysOfUnits),]
        }
        # subset indices for training
        indices_forTraining <- indices_[indices_ %in% trainIndices]
        glmnetInput <- ifelse(XisNull, yes = list(ImageRepresentations_df),
                                       no = list(cbind(as.matrix(X), ImageRepresentations_df)))[[1]]
        myGlmnet_ <- glmnet::cv.glmnet(
          x = as.matrix(glmnetInput[indices_forTraining,]),
          y = as.matrix(obsW[indices_forTraining]),
          alpha = 0, # alpha = 0 is the ridge penalty
          family = "binomial")
        obsW_ <- obsW[indices_]; obsY_ <- obsY[indices_]

        # compute QOIs
        myGlmnet_coefs_ <- as.matrix( glmnet::coef.glmnet(myGlmnet_, s = "lambda.min") )
        prW_est_ <- predict(myGlmnet_, s ="lambda.min",newx= as.matrix(glmnetInput), type = "response")
        #prW_est_ <- sigmoid( as.matrix(cbind(1, glmnetInput)) %*% myGlmnet_coefs_ )
        tauHat_propensity_vec[jr] <- tauHat_propensity_ <- mean(  obsW_*obsY_/c(prW_est_) - (1-obsW_)*obsY_/c(1-prW_est_) )
        tauHat_propensityHajek_vec[jr] <- tauHat_propensityHajek_ <- sum(  obsY_*prop.table(obsW_/c(prW_est_))) -
                        sum(obsY*prop.table((1-obsW_)/c(1-prW_est_) ))
        myGlmnet_coefs_mat[jr,] <- c(myGlmnet_coefs_)
        if(jr == (nBoot+1L)){
          nTrainable <- length(  myGlmnet_coefs_  )
          tauHat_propensityHajek <- tauHat_propensityHajek_
          tauHat_propensity <- tauHat_propensity_
          myGlmnet_coefs <- myGlmnet_coefs_
          prW_est <- prW_est_
          GetTreatProb_batch <- function( ModelList, ModelList_fixed,
                                          m, x, vseed,
                                          StateList, seed, MPList, inference){
            ImageReps <- ImageRepArm_batch_R(ModelList_fixed, m, StateList, seed, MPList, inference)
            if(!XisNull){
              x_m <- jnp$concatenate(list( jnp$ones(list(m$shape[[1]],1L)), x, ImageReps[[1]] ), 1L)
            }
            if(XisNull){
              x_m <- jnp$concatenate(list( jnp$ones(list(m$shape[[1]],1L)), ImageReps[[1]] ), 1L)
            }
            my_probs <- jax$nn$sigmoid(  jnp$matmul(x_m, LE( ModelList, "myGlmnet_coefs_tf" ) )  )
            my_probs <- (1. - EP_LSMOOTH) * my_probs + EP_LSMOOTH/2.
            return( list(my_probs, StateList) )
          }
          ModelList <- list("myGlmnet_coefs_tf" = jnp$array(myGlmnet_coefs, dtype = jnp$float32))
          ModelList_fixed <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]][[1]]
          StateList <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]][[2]]
          MPList <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]][[3]]
          ImageRepArm_batch_R <- ImageRepresentations$ImageRepArm_batch_R
        }
      }
    }

    if(optimizeImageRep){
      setwd(orig_wd); ImageRepresentations <- GetImageRepresentations(
        file = file,
        dataType = dataType,
        InitImageProcess = InitImageProcessFn,
        nWidth_ImageRep = nWidth_ImageRep,
        nDepth_ImageRep = nDepth_ImageRep,
        strides = strides,
        dropoutRate = dropoutRate,
        nDepth_TemporalRep = nDepth_TemporalRep,
        patchEmbedDim = patchEmbedDim,
        batchSize = batchSize,
        imageModelClass = imageModelClass,
        optimizeImageRep = optimizeImageRep, 
        temporalKernelSize = temporalKernelSize,
        kernelSize = kernelSize,
        inputAvePoolingSize = inputAvePoolingSize,
        TfRecords_BufferScaler = 3L,
        imageKeysOfUnits = (UsedKeys <- sample(unique(imageKeysOfUnits),min(c(length(unique(imageKeysOfUnits)),2*batchSize)))), getRepresentations = T,
        returnContents = T,
        bn_momentum = 0.9,
        conda_env = conda_env,
        conda_env_required = conda_env_required,
        Sys.setenv_text = Sys.setenv_text,
        seed = ai(4003L + seed)  ); setwd(new_wd)
        ImageModel_And_State_And_MPPolicy_List <- ImageRepresentations[["ImageModel_And_State_And_MPPolicy_List"]]
        ImageRepArm_batch_R <- ImageRepresentations[["ImageRepArm_batch_R"]]
        rm( ImageRepresentations )

        batch_axis_name <- "batch"
        DenseList <- DenseStateList <- replicate(nDepth_Dense, list())
        for(d_ in 1L:nDepth_Dense){
          DenseProj_d <- eq$nn$Linear(in_features = ind_ <- ifelse(d_ == 1, yes = (nWidth_ImageRep + ifelse(XisNull, no = ncol(X), yes = 0L)),
                                                                            no =  nWidth_Dense),
                                      out_features = outd_ <- ifelse(d_ == nDepth_Dense,
                                                                            yes = 1L,  no = nWidth_Dense),
                                      use_bias = T, key = jax$random$PRNGKey(d_+44L + as.integer(seed)))
          LayerBN_d  <- eq$nn$BatchNorm( input_size = outd_, axis_name = batch_axis_name,
                                         momentum = 0.9, eps = 0.001, channelwise_affine = F)
          DenseStateList[[d_]] <- eval(parse(text = sprintf("list('BNState_d%s' = eq$nn$State( LayerBN_d ))", d_)))
          DenseList[[d_]] <- eval(parse(text = sprintf('list("DenseProj_d%s" = DenseProj_d,
                                                              "BN_%s" = LayerBN_d)', d_, d_ )))
        }

        # ModelList <- DenseList; StateList <- DenseStateList
        GetDense_OneObs <- function(ModelList, ModelList_fixed, m, x,
                                    vseed, StateList, seed, MPList, inference){
          if(!XisNull){  m <- jnp$concatenate(list(m,x))  }

          for(d__ in 1:nDepth_Dense){
            m <-  LE(ModelList, sprintf("DenseProj_d%s",d__))(  m  )

            # BN + non-linearity
            if(d__ < nDepth_Dense){
              m <- LE(ModelList, sprintf("BN_d%s", d__))(m, state = StateList[[d__]], inference = inference)
              StateIndex <- LE_index(StateList, sprintf("BNState_d%s", d__))
              StateIndex <- paste(sapply(StateIndex, function(zer){ paste("[[", zer, "]]") }), collapse = "")
              eval(parse(text = sprintf("StateList%s <- m[[2]]", StateIndex)))
              m <- m[[1]]

              # Non-linearity
              m <- jax$nn$swish(  m   )
            }
          }
          return( list(m, StateList)  )
        }
        GetDense_batch <- jax$vmap(  function(
                  ModelList, ModelList_fixed,
                  m, x, vseed,
                  StateList, seed, MPList, inference){
                    GetDense_OneObs(ModelList, ModelList_fixed, m, x, vseed, StateList, seed, MPList, inference)
                },
                in_axes = list(NULL, NULL, 0L, 0L, 0L, NULL, NULL, NULL, NULL),
                   axis_name = batch_axis_name,
                   out_axes = list(0L, NULL) )
        GetDense_batch_jit <- eq$filter_jit(   GetDense_batch  )
        GetTreatProb_batch <- function( ModelList, ModelList_fixed,
                                        m, x, vseed,
                                        StateList, seed, MPList, inference){
          # image model
          m <- ImageRepArm_batch_R(ModelList, m, StateList, seed, MPList, inference)
          StateList <- m[[2]]; m <- m[[1]]

          # dense model
          m <- GetDense_batch(ModelList, ModelList_fixed, m, x, vseed, StateList, seed, MPList, inference)
          StateList <- m[[2]]; m <- m[[1]]

          # enforce range in [0,1]
          m <- jax$nn$sigmoid( m )
          
          # label smoothing to prevent NAs via log(0)
          m <- (1. - EP_LSMOOTH) * m + EP_LSMOOTH/2.

          # return contents
          return( list(m, StateList) )
        }

        GetLoss <-  function( ModelList, ModelList_fixed,
                              m, x, treat, vseed,
                              StateList, seed, MPList, inference ){
          ModelList <- MPList[[1]]$cast_to_compute( ModelList ) # compute to output dtype
          ModelList_fixed <- MPList[[1]]$cast_to_compute( ModelList_fixed ) # compute to output dtype
          StateList <- MPList[[1]]$cast_to_compute( StateList ) # compute to output dtype

          m <- GetTreatProb_batch( ModelList, ModelList_fixed,
                                   m, x, vseed,
                                   StateList, seed, MPList, inference )
          StateList <- m[[2]]; m <- jnp$squeeze( m[[1]] )

          # compute negative log-likelihood loss
          m <- MPList[[1]]$cast_to_output( m )
          NegLL <-  jnp$mean( jnp$negative(  treat*jnp$log(m) +  (1-treat)*jnp$log(1-m) )  ) 

          print2("Returning loss + state...")
          if(image_dtype_char == "float16"){ 
            NegLL <- MPList[[1]]$cast_to_output( NegLL ) # compute to output dtype
            NegLL <- MPList[[2]]$scale( NegLL ) # scale loss
            StateList <- MPList[[1]]$cast_to_param( StateList ) # compute to param dtype
          }

          # return
          return( list(NegLL, StateList)  )
        }

        gc(); py_gc$collect()
        # set state and model lists
        GradAndLossAndAux <-  eq$filter_jit( eq$filter_value_and_grad( GetLoss, has_aux = T) )
        ModelList <- list(ImageModel_And_State_And_MPPolicy_List[[1]], DenseList)
        StateList <- list(ImageModel_And_State_And_MPPolicy_List[[2]], DenseStateList)
        ModelList_fixed <- jnp$array(0.)
        MPList <- list(jmp$Policy(compute_dtype="float16", 
                                  param_dtype="float32", 
                                  output_dtype=(outputDtype <- "float32")),
                       jmp$DynamicLossScale(loss_scale = jnp$array(2^15,dtype = eval(parse(text = paste0("jnp$",outputDtype)))),
                                            min_loss_scale = jnp$array(2^2.,dtype = eval(parse(text = paste0("jnp$",outputDtype)))),
                                            period = 50L))
        ModelList <- MPList[[1]]$cast_to_param( ModelList )
        ModelList_fixed <- MPList[[1]]$cast_to_param( ModelList_fixed )
        rm( ImageModel_And_State_And_MPPolicy_List, DenseStateList, DenseList )

        # define optimizer and training step
        {
          LR_schedule <- optax$warmup_cosine_decay_schedule(warmup_steps = (nWarmup <- min(c(100L, nSGD))),
                                                            decay_steps = max(c(101L, nSGD-nWarmup)),
                                                            init_value = learningRateMax/100, 
                                                            peak_value = learningRateMax, 
                                                            end_value =  learningRateMax/100)
          plot(np$array(LR_schedule(jnp$array(1:nSGD))),xlab = "Iteration", ylab="Learning rate")
          optax_optimizer <-  optax$chain(
            optax$clip(1), 
            optax$adaptive_grad_clip(clipping = 0.05),
            optax$adabelief( learning_rate = LR_schedule )
          )

          # model partition, setup state, perform parameter count
          opt_state <- optax_optimizer$init(   eq$partition(ModelList, eq$is_array)[[1]] )
          print2(sprintf("Total trainable parameter count: %s", nTrainable <- sum(unlist(lapply(jax$tree_leaves(eq$partition(ModelList, eq$is_array)[[1]]), function(zer){zer$size})))))
          # unlist(lapply(jax$tree_leaves(eq$partition(ModelList, eq$is_array)[[1]]), function(zer){zer$size}))

          # jit update fxns
          jit_apply_updates <- eq$filter_jit(optax$apply_updates)
          jit_get_update <- eq$filter_jit(optax_optimizer$update)
        }

        print2("Starting training...")
        tmp <- c()
        keys2indices_list <- tapply(1:length(imageKeysOfUnits), imageKeysOfUnits, c)
        GradNorm_vec <- loss_vec <- rep(NA,times=nSGD)
        keysUsedInTraining <- tauMeans <- c();i_<-1L ; DoneUpdates <- 0L; for(i in i_:nSGD){
          t0 <- Sys.time(); if(i %% 5 == 0 | i == 1){gc(); py_gc$collect()}

          if(is.null(TFRecordControl)){ 
            # get next batch 
            ds_next_train <- ds_iterator_train$`next`()
  
            # if we run out of observations, reset iterator
            RestartedIterator <- F; if( is.null(ds_next_train) ){
                print2("Re-setting iterator! (type 1)"); gc(); py_gc$collect()
                ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
                ds_next_train <-  ds_iterator_train$`next`(); gc();py_gc$collect()
            }
  
            # get a new batch if size mismatch - size mismatches generate new cached compiled fxns
            if(!RestartedIterator){ if(dim(ds_next_train[[1]])[1] != batchSize){
                  print2("Re-setting iterator! (type 2)"); gc(); py_gc$collect()
                  ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
                  ds_next_train <-  ds_iterator_train$`next`(); gc(); py_gc$collect()
            } }
            
            # select batch indices based on keys
            batch_keys <- unlist(  lapply( p2l(ds_next_train[[3]]$numpy()), as.character) )
            batch_indices <- sapply(batch_keys,function(key_){ f2n( sample(as.character( keys2indices_list[[key_]] ), 1) ) })
            ds_next_train <- ds_next_train[[1]]
          }
          if(!is.null(TFRecordControl)){ 
            # get next batch 
            ds_next_train_control <- ds_iterator_train_control$`next`()
            
            # if we run out of observations, reset iterator
            RestartedIterator <- F; if( is.null(ds_next_train_control) ){
              print2("Re-setting iterator! (type 1)"); gc(); py_gc$collect()
              ds_iterator_train_control <- reticulate::as_iterator( tf_dataset_train_control )
              ds_next_train_control <-  ds_iterator_train_control$`next`(); gc();py_gc$collect()
            }
            
            # get a new batch if size mismatch - size mismatches generate new cached compiled fxns
            if(!RestartedIterator){ if(dim(ds_next_train_control[[1]])[1] != batchSize){
              print2("Re-setting iterator! (type 2)"); gc(); py_gc$collect()
              ds_iterator_train_control <- reticulate::as_iterator( tf_dataset_train_control )
              ds_next_train_control <-  ds_iterator_train_control$`next`(); gc(); py_gc$collect()
            } }
            
            # get next batch 
            ds_next_train_treated <- ds_iterator_train_treated$`next`()
            
            # if we run out of observations, reset iterator
            RestartedIterator <- F; if( is.null(ds_next_train_treated) ){
              print2("Re-setting iterator! (type 1)"); gc(); py_gc$collect()
              ds_iterator_train_treated <- reticulate::as_iterator( tf_dataset_train_treated )
              ds_next_train_treated <-  ds_iterator_train_treated$`next`(); gc();py_gc$collect()
            }
            
            # get a new batch if size mismatch - size mismatches generate new cached compiled fxns
            if(!RestartedIterator){ if(dim(ds_next_train_treated[[1]])[1] != batchSize){
              print2("Re-setting iterator! (type 2)"); gc(); py_gc$collect()
              ds_iterator_train_treated <- reticulate::as_iterator( tf_dataset_train_treated )
              ds_next_train_treated <-  ds_iterator_train_treated$`next`(); gc(); py_gc$collect()
            } }
            
            # select batch indices based on keys
            batch_keys <- c(unlist(  lapply( p2l(ds_next_train_control[[3]]$numpy()), as.character) ),
                            unlist(  lapply( p2l(ds_next_train_treated[[3]]$numpy()), as.character) ))
            batch_indices <- sapply(batch_keys,function(key_){ f2n( sample(as.character( keys2indices_list[[key_]] ), 1) ) })
            ds_next_train <- tf$concat(list(ds_next_train_control[[1]],
                                            ds_next_train_treated[[1]]), 0L)
            print(table(obsW[batch_indices]))
            tmp <- c(tmp,batch_indices)
            # plot(tmp)
            # plot(diff(obsW[tmp]))
            # plot(head(obsW[tmp],300))
            # StateList[[1]][[3]]$BNState_ImRep_FinalCNNBN$tree_flatten()[[1]]
            # StateList[[1]][[3]]$BNState_ImRep_FinalCNNBN$tree_flatten()[[2]]
          }
          if(any(!batch_indices %in% keysUsedInTraining)){ keysUsedInTraining <- c(keysUsedInTraining, batch_keys[!batch_keys %in% keysUsedInTraining]) }

          # training step
          if(T == F){
            m <- InitImageProcessFn(jnp$array(ds_next_train),  jax$random$PRNGKey(600L+11L), inference = F)
            causalimages::image2(np$array(m)[1,,,2])
            x <- jnp$array(X[batch_indices,],dtype = jnp$float16)
            treat <- jnp$array(obsW[batch_indices],dtype = jnp$float16); inference <- F
            vseed <- jax$random$split( seed <- jax$random$PRNGKey( 500L+i ),batchSize)
          }
          # rm(GradAndLossAndAux); GradAndLossAndAux <-  eq$filter_jit( eq$filter_value_and_grad( GetLoss, has_aux = T) )
          t1 <- Sys.time()
          GradientUpdatePackage <- try(GradAndLossAndAux(
            MPList[[1]]$cast_to_compute(ModelList), MPList[[1]]$cast_to_compute(ModelList_fixed),
            #ModelList, ModelList_fixed,
            InitImageProcessFn(jnp$array(ds_next_train),  jax$random$PRNGKey(600L+i), inference = F), # m
            jnp$array(X[batch_indices,], dtype = jnp$float16), # x
            jnp$array(obsW[batch_indices], dtype = jnp$float16), # treat
            jax$random$split(jax$random$PRNGKey( 500L+i ),length(batch_indices)),  # vseed
            StateList, # StateList
            jax$random$PRNGKey( 123L+i ), # seed
            MPList, # MPlist
            F), T) # inference
          if("try-error" %in% class(GradientUpdatePackage)){
            print( GradientUpdatePackage ); if(atError == "stop"){ stop() }; if(atError == "debug"){ browser() }
          }
          if(!"try-error" %in% class(GradientUpdatePackage)){
            # get updated state
            StateList_tmp <- GradientUpdatePackage[[1]][[2]] # state

            # get loss + grad
            if(image_dtype_char == "float16"){ 
              loss_vec[i] <- myLoss_fromGrad <- np$array( MPList[[2]]$unscale( GradientUpdatePackage[[1]][[1]] ) )# value
            }
            if(image_dtype_char != "float16"){ 
              loss_vec[i] <- myLoss_fromGrad <- np$array( GradientUpdatePackage[[1]][[1]] )# value
            }
            GradientUpdatePackage <- GradientUpdatePackage[[2]] # grads
            GradientUpdatePackage <- eq$partition(GradientUpdatePackage, eq$is_inexact_array)
            GradientUpdatePackage_aux <- GradientUpdatePackage[[2]]; GradientUpdatePackage <- GradientUpdatePackage[[1]]

            # unscale + adjust loss scale is some non-finite or NA
            if(i == 1){
              Map2Zero <- eq$filter_jit(function(input){
                jax$tree_map(function(x){ jnp$where(jnp$isnan(x), jnp$array(0), x)}, input) })
              GetGetNorms <- eq$filter_jit(function(input){
                jax$tree_map(function(x){ jnp$mean(jnp$abs(x)) }, input) })
              AllFinite <- jax$jit( jmp$all_finite )
            }
            if(image_dtype_char == "float16"){ 
              GradientUpdatePackage <- Map2Zero( MPList[[2]]$unscale( GradientUpdatePackage ) )
            }
            AllFinite_DontAdjust <- AllFinite( GradientUpdatePackage )  & jnp$squeeze(jnp$array(!is.infinite(myLoss_fromGrad)))
            MPList[[2]] <- MPList[[2]]$adjust( AllFinite_DontAdjust  )
            # which(is.na( c(unlist(lapply(jax$tree_leaves(myGrad_jax), function(zer){np$array(zer)}))) ) )
            # which(is.infinite( c(unlist(lapply(jax$tree_leaves(myGrad_jax), function(zer){np$array(zer)}))) ) )
            
            # get update norm 
            GradNorm_vec[i] <- mean( WeightGrads <- unlist( lapply(jax$tree_leaves(GradientUpdatePackage),function(zer){ np$array(jnp$mean(jnp$abs(zer) )) }) )  ) 
            print( summary( WeightGrads ) ) 
            # plot(WeightGrads)

            # update parameters if finite gradients
            DoUpdate <- !is.na(myLoss_fromGrad) & np$array(AllFinite_DontAdjust) & 
                        !is.infinite(myLoss_fromGrad) & ( GradNorm_vec[i] > 1e-5)
            if(! DoUpdate ){ print2("Warning: Not updating parameters due to zero or non-finite gradients in mixed-precision training...") }
            if( DoUpdate ){
              DoneUpdates <- DoneUpdates + 1

              # cast updates to param 
              GradientUpdatePackage <- MPList[[1]]$cast_to_param( GradientUpdatePackage )
              
              # get updates
              GradientUpdatePackage <- jit_get_update( updates = GradientUpdatePackage,
                                                       state = opt_state,
                                                       params = eq$partition(ModelList, eq$is_array)[[1]])

              # separate updates from state
              opt_state <- GradientUpdatePackage[[2]]
              GradientUpdatePackage <- eq$combine(GradientUpdatePackage[[1]], 
                                                  GradientUpdatePackage_aux)

              # perform updates
              ModelList <- eq$combine( jit_apply_updates(
                                          params = GlobalPartition(eq$partition(ModelList, eq$is_array)[[1]], PartFxn)[[1]],
                                          updates = GradientUpdatePackage),
                                  eq$partition(ModelList, eq$is_array)[[2]])
              StateList <- StateList_tmp
              rm(StateList_tmp, GradientUpdatePackage)
            }
            # print diagnostics
            i_ <- i ; if(i %% 10 == 0 | i < 10 ){
              print2(sprintf("SGD iteration %s of %s - Loss: %.2f (%.1f%%) - - Total time (s): %.2f - Grad time (s): %.2f",
                             i,  nSGD, loss_vec[i], 100*mean(loss_vec[i] <= loss_vec[1:i],na.rm=T),
                             (Sys.time() - t0)[[1]], (Sys.time() - t1)[[1]] ) )
              loss_vec <- f2n(loss_vec); loss_vec[is.infinite(loss_vec)] <- NA
              par(mfrow=c(1,2))
              try(plot(rank(na.omit(loss_vec)), cex.main = 0.95,ylab = "Loss Function Rank",xlab="SGD Iteration Number"), T)
              if(i > 10){ try_ <- try(points(smooth.spline( rank(na.omit(loss_vec) )), col="red",type = "l",lwd=5),T) }
              try(plot(na.omit(GradNorm_vec), cex.main = 0.95,ylab = "GradNorm",xlab="SGD Iteration Number"), T)
            }
          }
        } # end for(i in i_:nSGD){

        print2("Getting predicted quantities...")
        GetImageArm_OneX <- eq$filter_jit( function(ModelList, ModelList_fixed,
                 m, vseed,
                 StateList, seed, MPList){
          # image representation model
          m <- ImageRepArm_batch_R(ModelList, m, StateList, seed, MPList, T)
          StateList <- m[[2]] ; m <- m[[1]]
          
          # sigmoid 
          m <- jax$nn$sigmoid( m )
          
          # label smoothing to prevent NAs via log(0)
          m <- (1. - EP_LSMOOTH) * m + EP_LSMOOTH/2.
          
          return( m )
        })

        inference_counter <- 0; nUniqueKeys <- length( unique(imageKeysOfUnits) )
        #KeyQuantCuts <- gtools::quantcut(1:nUniqueKeys, q = ceiling( nUniqueKeys / (batchSize*0.33) ))
        KeyQuantCuts <- 1L:nUniqueKeys
        passedIterator <- NULL; Results_by_keys <- replicate(length(unique(KeyQuantCuts)),list());
        ImageRepArm_batch_jit <- eq$filter_jit( ImageRepArm_batch_R )
        usedKeys <- c()
        for(cut_ in unique(KeyQuantCuts)){ # use when not incorporating X's
          # cut_ <- unique(KeyQuantCuts)[1]
          inference_counter <- inference_counter + 1
          zer <- which(cut_  ==  KeyQuantCuts)
          #gc(); py_gc$collect()
          atP <- max(zer)/nUniqueKeys
          if( any(zer %% 10 == 0) | 1 %in% zer ){ print2(sprintf("Proportion done: %.3f", atP)) }
          {
            setwd(orig_wd); ds_next_in <- GetElementFromTfRecordAtIndices(
                                                uniqueKeyIndices = which(unique(imageKeysOfUnits) %in% unique(imageKeysOfUnits)[zer]),
                                                filename = file,
                                                iterator = passedIterator,
                                                readVideo = useVideoIndicator,
                                                image_dtype = image_dtype_tf,
                                                nObs = length(unique(imageKeysOfUnits)),
                                                return_iterator = T ); setwd(new_wd)
            passedIterator <- ds_next_in[[2]]
            key_ <- unlist(  lapply( p2l(ds_next_in[[1]][[3]]$numpy() ), as.character) )
            ds_next_in <-  jnp$array( ds_next_in[[1]][[1]] )

            # deal with batch 1 case here
            if(length(ds_next_in$shape) == 3 & dataType == "image"){ ds_next_in <- jnp$expand_dims(ds_next_in, 0L) }
            if(length(ds_next_in$shape) == 4 & dataType == "video"){ ds_next_in <- jnp$expand_dims(ds_next_in, 0L) }
          }

          # get summaries and save
          usedKeys <- c(usedKeys, key_)
          obs_with_key <- which(imageKeysOfUnits %in% key_)
          x <- jnp$expand_dims(jnp$array(  ifelse(length(obs_with_key) == 1, yes = list(t(X[obs_with_key,])),
                                                              no = list(X[obs_with_key,]))[[1]],
                           dtype = jnp$float16), 0L)$transpose( c(1L, 0L, 2L) )
          m_ImageRep <- ImageRepArm_batch_jit(ifelse(optimizeImageRep, yes = ModelList, no = ModelList_fixed),
                                          InitImageProcessFn(jnp$array(ds_next_in),  jax$random$PRNGKey(600L+i), inference = T),
                                          StateList, seed, MPList, T)[[1]]
          GottenSummaries <- sapply(1L:ifelse(XisNull, yes = 1L, no = x$shape[[1]]), function(r_){
            m <- GetDense_batch_jit(ModelList, ModelList_fixed,
                                m_ImageRep,
                                x[r_-1L,],
                                jax$random$split(jax$random$PRNGKey(as.integer(runif(1,0, 10000))), ds_next_in$shape[[1]]),
                                StateList,
                                jax$random$PRNGKey(as.integer(runif(1,0,100000))),
                                MPList, T)[[1]]
            m <- jax$nn$sigmoid( m )
            m <- (1. - EP_LSMOOTH) * m + EP_LSMOOTH/2.
            if(XisNull){m <- list(replicate(m, n = x$shape[[1]]))}
            return( m )
          })
          GottenSummaries <- as.matrix(np$array(jnp$concatenate(unlist(GottenSummaries),0L)))
          ret_list <- list("ProbW" = GottenSummaries,
                           "obsIndex" = as.matrix(obs_with_key),
                           "key" = as.matrix( rep(key_, length(obs_with_key)) ))
          Results_by_keys[[inference_counter]] <- ret_list
        }
        Results_by_keys <- as.data.frame(
                        apply(do.call(rbind, Results_by_keys),2,function(zer){(do.call(rbind,zer))}))
        prW_est <-  Results_by_keys$ProbW <-  f2n(  Results_by_keys$ProbW ) 
        
        # checks
        # usedKeys  == unique(imageKeysOfUnits)
        # unlist(Results_by_keys[["key"]]) == unique(imageKeysOfUnits)
        # mean(unlist(Results_by_keys[["key"]]) %in% imageKeysOfUnits)
        # mean(imageKeysOfUnits %in% unlist(  Results_by_keys[["key"]] ))
        trainIndices <- which( imageKeysOfUnits %in% keysUsedInTraining )
        testIndices <- which( !imageKeysOfUnits %in% keysUsedInTraining )
        tauHat_propensityHajek <- sum(  obsY*prop.table(obsW/c(prW_est))) - sum(obsY*prop.table((1-obsW)/c(1-prW_est) ))
        tauHat_propensityHajek_vec <- unlist(replicate(nBoot, { i_ <- sample(1:length(obsW),length(obsW), T)
                        sum(  obsY[i_]*prop.table(obsW[i_]/c(prW_est[i_]))) -
                          sum(obsY[i_]*prop.table((1-obsW)[i_]/(1-prW_est)[i_] )) } ))
    }

    # process in and out of sample losses
    prWEst_baseline <- prW_est 
    prWEst_baseline[] <- mean( obsW[trainIndices] )
    
    # cross entropy loss calcs
    binaryCrossLoss <- function(W,prW){ return( - mean( log(prW)*W + log(1-prW)*(1-W) ) ) }
    lossCE_OUT_baseline <- binaryCrossLoss(obsW[testIndices], prWEst_baseline[testIndices])
    lossCE_IN_baseline <- binaryCrossLoss(obsW[trainIndices], prWEst_baseline[trainIndices])
    lossCE_OUT <-  binaryCrossLoss(  obsW[testIndices], prW_est[testIndices]  )
    lossCE_IN <-  binaryCrossLoss(  obsW[trainIndices], prW_est[trainIndices]  )

    # class error calcs
    lossClassError_OUT_baseline <- 1/length(testIndices) * (sum( prWEst_baseline[testIndices][ obsW[testIndices] == 1] < 0.5) +
                           sum( prWEst_baseline[testIndices][ obsW[testIndices] == 0] > 0.5))
    lossClassError_IN_baseline <- 1/length(trainIndices) * (sum( prWEst_baseline[trainIndices][ obsW[trainIndices] == 1] < 0.5) +
                                                              sum( prWEst_baseline[trainIndices][ obsW[trainIndices] == 0] > 0.5))
    lossClassError_OUT <- 1/length(testIndices) * (sum( prW_est[testIndices][ obsW[testIndices] == 1] < 0.5) +
                           sum( prW_est[testIndices][ obsW[testIndices] == 0] > 0.5))
    lossClassError_IN <- 1/length(trainIndices) * (sum( prW_est[trainIndices][ obsW[trainIndices] == 1] < 0.5) +
                                                     sum( prW_est[trainIndices][ obsW[trainIndices] == 0] > 0.5))
    
    # store output
    ModelEvaluationMetrics <- list(
      "CELoss_out" = lossCE_OUT,
      "CELoss_out_baseline" = lossCE_OUT_baseline,
      "CELoss_in" = lossCE_IN,
      "CELoss_in_baseline" = lossCE_IN_baseline,
      "ClassError_out" = lossClassError_OUT,
      "ClassError_out_baseline" = lossClassError_OUT_baseline,
      "ClassError_in" = lossClassError_IN,
      "ClassError_in_baseline" = lossClassError_IN_baseline
    )

    # reset to original wd which was altered during records initialization
    # do this before plotting to avoid disrupting plot save locations
    if( changed_wd ){ setwd(  orig_wd  ) }

    # do some analysis with examples
    processedDims <- NULL; if( plotResults ){
      print2("Plotting image confounding results...")
      indices_t <- (1:length(obsW))[which(obsW==1)]
      indices_c <- (1:length(obsW))[which(obsW==0)]

      showPerGroup <- min(c(3,unlist(table(obsW))), na.rm = T)
      top_control <- ordered_control <- indices_c[order_c <- order(prW_est[indices_c],decreasing = F)]
      top_treated <- ordered_treated <- indices_t[order_t <- order(prW_est[indices_t],decreasing = T)]

      # drop duplicates
      if(!is.null(long)){
        longLat_t <- paste(round(long[indices_t[order_t]],5L),
                                round(lat[indices_t[order_t]],5L),sep="_")
        longLat_c <- paste(round(long[indices_c[order_c]],5L),
                                round(lat[indices_c[order_c]],5L),sep="_")
        top_treated <- ordered_treated[!duplicated(longLat_t)]
        top_control <- ordered_control[!duplicated(longLat_c)]
      }
      plot_indices <- c( top_control <- top_control[1:showPerGroup],
                         top_treated <- top_treated[1:showPerGroup] )

      for(pos_ in 2L:3L){
        dLogProb_d <- jax$grad(function(ModelList, ModelList_fixed,
                                        m, x, vseed,
                                        StateList, seed, MPList){
                    ModelList <- MPList[[1]]$cast_to_param( ModelList )
                    ModelList_fixed <- MPList[[1]]$cast_to_param( ModelList_fixed )
                    StateList <- MPList[[1]]$cast_to_param( StateList )
                    m <- MPList[[1]]$cast_to_param( m ); x <- MPList[[1]]$cast_to_param( x )
                    m <- jax$device_put(m, jax$devices('cpu')[[1]])
                    out_ <-  GetTreatProb_batch(ModelList, ModelList_fixed,
                                           m, x, vseed,
                                           StateList, seed, MPList, T)[[1]]  # scaling for non-zero gradients
                    out_ <- jnp$log(out_ / (1-out_))
                    return(  jnp$squeeze(out_)  ) }, pos_)
        if(pos_ == 2L){ dLogProb_dImage <- dLogProb_d }
        if(pos_ == 3L){ dLogProb_dX <- eq$filter_jit( dLogProb_d ) }
      }
      ImGrad_fxn <- eq$filter_jit( function(ModelList, ModelList_fixed,
                                            m, x, vseed,
                                            StateList, seed, MPList){
        # cast to float32
        ModelList <- MPList[[1]]$cast_to_param( ModelList )
        ModelList_fixed <- MPList[[1]]$cast_to_param( ModelList_fixed )
        StateList <- MPList[[1]]$cast_to_param( StateList )
        m <- MPList[[1]]$cast_to_param( m ); x <- MPList[[1]]$cast_to_param( x )
        m <- jax$device_put(m, jax$devices('cpu')[[1]])
        ImageGrad_o <- jnp$squeeze(dLogProb_dImage(ModelList, ModelList_fixed,
                                       m, x, vseed, StateList, seed, MPList), 0L)
        reduceDim <- ifelse( dataType == "video", yes = 3L, no = 2L)
        ImageGrad_L2 <- jnp$linalg$norm(ImageGrad_o+0.000001, axis = reduceDim, keepdims = T)
        ImageGrad_mean <- jnp$mean(ImageGrad_o, axis = reduceDim, keepdims = T)
        return( list(ImageGrad_L2,  # salience magnitude
                     ImageGrad_mean) ) # salience direction
      }, device = jax$devices('cpu')[[1]])
      MPList <- list(jmp$Policy(compute_dtype="float32", param_dtype="float32", output_dtype="float32"),
                      jmp$DynamicLossScale(jnp$array(2^15), 
                                           period = 20L))
      makePlots <- function(){
        salience_try <- try({
        print2("Plotting salience maps...")
        nrows_im <- 2
        eval(parse(text = ifelse(dataType == "image", yes = 'pdf(sprintf("%s/CSM_%s.pdf", figuresPath, FigNameAppend),
            width = length(plot_indices)*5+2,height = nrows_im*5)', no = "NULL") ))
        {
          layout(matrix(1:(nrows_im*(1+length(plot_indices))),
                        ncol = 1+length(plot_indices)),
                 width = c(0.5,rep(5,length(plot_indices))),
                 height = rep(5,times=nrows_im)); in_counter <- 0

        # create axis labels in plot positions 1 and 2
        for(text_ in c("Raw Image","Salience Map")){
            if(dataType == "image"){
              par(mar=c(0,0,0,0));
              plot(0, main = "", ylab = "",cex = 0, xlab = "", ylim = c(0,1), xlim = c(0,1), xaxt = "n",yaxt = "n",bty = "n")
              text(0.5,0.5,labels = text_, srt=90,cex=3)
            }
        }

        # generate rest of plot
          plot_index_counter <- 0; for(in_ in plot_indices){
            print(c(text_, in_))
            gc(); py_gc$collect()
            plot_index_counter <- plot_index_counter + 1

            # get data
            setwd(orig_wd); ds_next_in <- GetElementFromTfRecordAtIndices(
                                                    uniqueKeyIndices = which(unique(imageKeysOfUnits) %in% imageKeysOfUnits[in_]),
                                                    filename = file,
                                                    readVideo = useVideoIndicator,
                                                    nObs = length(imageKeysOfUnits) ); setwd(new_wd)
            ds_next_in[[1]] <- jnp$array( ds_next_in[[1]] )
            if(length(ds_next_in[[1]]$shape) == 3 & dataType == "image"){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
            if(length(ds_next_in[[1]]$shape) == 4 & dataType == "video"){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }

            col_ <- ifelse(in_ %in% top_treated, yes = "black", no = "gray")
            in_counter <- in_counter + 1
            long_lat_in_ <- ""; if(  !is.null(lat)  ){ long_lat_in_ <- sprintf("Lat-Lon: %.3f, %.3f", f2n(lat[in_]), f2n(long[in_])) }

            im_orig <- im_ <- InitImageProcessFn( im = jnp$array(ds_next_in[[1]]), key = jax$random$PRNGKey(3L), inference = T )
            XToConcat_values <- jnp$array(t(X[in_,]),jnp$float16)
            im_ <- np$array(jnp$squeeze(im_,c(0L)))

            # calculate salience map using log probabilities
            # m <- jmp$cast_to_full(im_orig); x <- jmp$cast_to_full(XToConcat_values); seed <- jax$random$PRNGKey(10L); vseed <- jnp$expand_dims(seed,0L)
            salience_map <- np$array(  ImGrad_fxn(
                                        jmp$cast_to_full(ModelList), jmp$cast_to_full(ModelList_fixed),
                                        jmp$cast_to_full(im_orig),
                                        jmp$cast_to_full(XToConcat_values),
                                        jnp$expand_dims(jax$random$PRNGKey(10L),0L),
                                        StateList, jax$random$PRNGKey(10L), MPList )[[1]] )
            if(dataType == "image"){ salience_map <- salience_map[,,1] }
            if(dataType == "video"){ salience_map <- salience_map[,,,1] }

            # do plotting
            orig_scale_im_ <- sapply(1:length(NORM_MEAN), function(band_){
                                       if(dataType == "image"){
                                         im_[,,band_] <- 0.1+im_[,,band_]*NORM_SD[band_] + NORM_MEAN[band_]
                                         return( im_[,,band_] )
                                       }
                                       if(dataType == "video"){
                                         im_[,,,band_] <- 0.1+im_[,,,band_]*NORM_SD[band_] + NORM_MEAN[band_]
                                         return( im_[,,,band_] )
                                       }  }, simplify="array")

            # plot results
            par(mar = (mar_vec <- c(2,1,3,1)))
            if(dataType == "image"){
              plotRBG <- !(length(plotBands) < 3 | dim(orig_scale_im_)[length(dim(orig_scale_im_))] < 3)
              if(!plotRBG){
                causalimages::image2(
                  as.matrix( orig_scale_im_[,,plotBands[1]] ),
                  main = long_lat_in_, cex.main = 2.5, col.main =  col_,
                  xlab = ifelse( plot_index_counter == 1,
                                 yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                                 no = "")
                )
              }
              if(plotRBG){
                 plot(0, main = long_lat_in_, col.main = col_,
                      ylab = "", xlab = "",
                      cex.main = 4, ylim = c(0,1), xlim = c(0,1),
                      cex = 0, xaxt = "n",yaxt = "n",bty = "n")
                 mtext(side = 1, ifelse( plot_index_counter == 1,
                               yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                               no = ""), cex = 1)
                 orig_scale_im_raster <- raster::brick(orig_scale_im_[,,plotBands[1:3]])
                 try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3,
                                 add = T, main = long_lat_in_, stretch = "lin"), T)
                 if("try-error" %in% class(try_)){ try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3, add = T, main = long_lat_in_), T) }
              }

              # plot salience map
              #optax$global_norm( eq$filter(ModelList, eq$is_array)[[1]] )
              try_salience <- try({salience_map[salience_map>0] <- salience_map[salience_map>0] / (0.001+sd(salience_map[salience_map>0]))},T)
              if("try-error" %in% class(try_salience)){
                print( try_salience )
                if(atError == "stop"){ stop() }; if(atError == "debug"){ browser() }
              }
              salience_map <- sign(salience_map)*log(abs(salience_map)+1)
              image2( salience_map, xlab = ifelse(tagInFigures, yes = imageKeysOfUnits[in_], no = ""),cex.lab = 1)
            }
            if(dataType == "video"){
              # plot raw image
              nTimeSteps <- dim(salience_map)[1]
              animation::saveGIF({
              # GIF part 1 --- all in outer time step loop
              for (t_ in 1:nTimeSteps){
              par(mfrow = c(1,2));
              plotRBG <- !(length(plotBands) < 3 | dim(orig_scale_im_)[length(dim(orig_scale_im_))] < 3)
              par(mar = margins_gif <- c(5,5,2,1))
              if(!plotRBG){
                par(mar = margins_gif)
                causalimages::image2(
                  as.matrix( orig_scale_im_[t_,,,plotBands[1]] ),
                  main = long_lat_in_, cex.main = 1.5, col.main =  col_,
                  xlab = ifelse( plot_index_counter == 1,
                                 yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                                 no = "")
                )
              }
              if(plotRBG){
                plot(0, main = long_lat_in_, col.main = col_,
                     ylab = "", xlab = "",
                     cex.main = 1.5, ylim = c(0,1), xlim = c(0,1),
                     cex = 0, xaxt = "n",yaxt = "n",bty = "n")
                mtext(side = 1, ifelse( plot_index_counter == 1,
                                        yes = ifelse(tagInFigures, yes = figuresTag, no = ""),
                                        no = ""), cex = 1)
                orig_scale_im_raster <- raster::brick(orig_scale_im_[t_,,,plotBands[1:3]])
                try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3,
                                            add = T, main = long_lat_in_, stretch = "lin"), T)
                if("try-error" %in% class(try_)){
                  try_ <- try(raster::plotRGB(orig_scale_im_raster, r = 1, g = 2, b = 3,
                                              add = T, main = long_lat_in_), T)
                }
              }

              # GIF part 2
              salience_map[salience_map>0] <- salience_map[salience_map>0] / (0.001+sd(salience_map[salience_map>0]))
              salience_map <- sign(salience_map)*log(abs(salience_map)+1)
              par(mar = margins_gif)
              image2( salience_map[t_,,],
                      main = long_lat_in_, cex.main = 1.5, col.main = "white",
                      #xlab = ifelse(tagInFigures, yes = imageKeysOfUnits[in_], no = ""),
                      cex.lab = 1)
              }}, movie.name = sprintf("%s/CSM_%s_%s.gif", figuresPath, FigNameAppend, plot_index_counter),
                  autobrowse = F, autoplay = F,
                  ani.height = 480*1, ani.width = 480*(1+1))
          }
        }

        eval(parse(text = ifelse(dataType == "image", yes = "dev.off()", no = "NULL") ))
        }},T)
        if('try-error' %in% class(salience_try)){
          print(salience_try);
          if(atError == "stop"){ stop("Problem in salience map computation!")  }
          if(atError == "debug"){ browser() }
        }

        if(optimizeImageRep){
          pdf(sprintf("%s/Loss_%s.pdf", figuresPath,FigNameAppend))
            par(mar = c(5,5,1,1))
            try(plot(loss_vec, cex = 1.5, cex.lab = 2,
                     xlab = "Iteration",
                     ylab = "Loss"),T);
            try(points(smooth.spline(na.omit(loss_vec)),type="l",lwd=3),T)
          dev.off()
        }

        try({
        print2("Plotting propensity histogram...")
        pdf(sprintf("%s/Hist_%s.pdf", figuresPath, FigNameAppend))
        {
          par(mfrow=c(1,1))
          d0 <- density(prW_est[obsW==0])
          d1 <- density(prW_est[obsW==1])
          plot(d1,lwd=2,xlim = c(-0.1,1.1),ylim =c(0,max(c(d1$y,d0$y),na.rm=T)*1.2),
               cex.axis = 1.2,ylab = "",xaxt = "n",
               xlab = ifelse(tagInFigures, yes = figuresTag, no = ""),
               main = "Density Plots for \n Estimated Pr(T=1 | Confounders)",cex.main = 2)
          axis(1, at = seq(0,1,by = 0.25))
          points(d0,lwd=2,type = "l",col="gray",lty=2)
          text(d0$x[which.max(d0$y)[1]],
               max(d0$y,na.rm=T)*1.1,label = "T = 0",col="gray",cex=2)
          text(d1$x[which.max(d1$y)[1]],
               max(d1$y,na.rm=T)*1.1,label = "T = 1",col="black",cex=2)
        }
        dev.off()
        }, T)
      }
      makePlots()
    }

    # compute salience for tabular covariates
    SalienceX_se <- SalienceX <- NULL; if(!XisNull){
    if( optimizeImageRep ){
        SalienceX <- c(); samp_counter <- 0
        for(keyNum_ in sample(1:length(unique(imageKeysOfUnits)), 25, replace = T)){
          samp_counter <- samp_counter + 1
          if(samp_counter %% 5 == 0){  print2(sprintf("Tabular Salience Iteration %s of %s", samp_counter, 25)) }
          sampIndex_ <- which(imageKeysOfUnits %in% unique(imageKeysOfUnits)[keyNum_])[1]

          # extract data
          setwd(orig_wd); ds_next_in <- GetElementFromTfRecordAtIndices(
                                                           uniqueKeyIndices = keyNum_,
                                                           filename = file,
                                                           readVideo = useVideoIndicator,
                                                           nObs = length(unique(imageKeysOfUnits) ) ); setwd(new_wd)
          ds_next_in[[1]] <- jnp$array(ds_next_in[[1]])
          if(length(ds_next_in[[1]]$shape) == 3 & dataType == "image"){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }
          if(length(ds_next_in[[1]]$shape) == 4 & dataType == "video"){ ds_next_in[[1]] <- tf$expand_dims(ds_next_in[[1]], 0L) }

          im_ <- InitImageProcessFn( jnp$array(ds_next_in[[1]]), jax$random$PRNGKey(432L), T)
          x_ <- jnp$array(t(X[sampIndex_,]), jnp$float16)
          # m <- jmp$cast_to_full(im_); x <- jmp$cast_to_full(x_); vseed <- seed <- jax$random$PRNGKey(10L)
          SalienceX_contrib <- np$array(  dLogProb_dX(  ModelList, ModelList_fixed,
                        jmp$cast_to_full(im_),
                        jmp$cast_to_full(x_),
                        jax$random$split(jax$random$PRNGKey( 500L+i ),x_$shape[[1]]),
                        StateList, jax$random$PRNGKey(10L), MPList ) )
          SalienceX <- rbind(SalienceX, SalienceX_contrib)
        }
        SalienceX <- colMeans( SalienceX ); names( SalienceX ) <- colnames(X)
    }
    if( !optimizeImageRep ){
        SalienceX <- myGlmnet_coefs[-1][1:ncol(X)] # drop intercept, then extract variables of interest
        SalienceX_se <- apply(myGlmnet_coefs_mat, 2, sd)[-1][1:ncol(X)]
        if(!is.null(SalienceX)){ names(SalienceX_se) <- colnames(X) }
    } 
      SalienceX <- SalienceX*X_sd  +  X_mean
    }
    # rescale the salience map into original scale

    postDiffInLat <- preDiffInLat <- NULL
    if(!is.null(lat)){
      preDiffInLat <- colMeans(cbind(long[obsW == 1],lat[obsW == 1])) -
        colMeans(cbind(long[obsW == 0],lat[obsW == 0]))
      postDiffInLat <- colSums(cbind(long[obsW == 1],lat[obsW == 1])*prop.table(1/prW_est[obsW == 1])) -
        colSums(cbind(long[obsW == 0],lat[obsW == 0])*prop.table(1/(1-prW_est[obsW == 0])))
    }

    # set salience map names
    if(!is.null(SalienceX)){ names(SalienceX) <- colnames(X) }
    rm( InitImageProcessFn )

    print2( "Done with image confounding analysis!" ); try(setwd(orig_wd), T)
    return(    list(
      "tauHat_propensityHajek"  = tauHat_propensityHajek,
      "tauHat_propensityHajek_se"  = sd(tauHat_propensityHajek_vec,na.rm=T),
      "tauHat_diffInMeans"  = mean(obsY[which(obsW==1)],na.rm=T) - mean(obsY[which(obsW==0)],na.rm=T),
      "tauHat_diffInMeans_se"  = c(sqrt(se(obsY[which(obsW==1)])^2 + se(obsY[which(obsW==0)])^2)),
      "SalienceX" = SalienceX,
      "SalienceX_se" = SalienceX_se,
      "prW_est" = prW_est,
      "SGD_loss_vec" = loss_vec,
      "LatitudeAnalysis" = list("preDiffInLat" = preDiffInLat, "postDiffInLat"  = postDiffInLat),
      "ModelEvaluationMetrics" = ModelEvaluationMetrics,
      "nTrainableParameters" = nTrainable,
      "trainIndices" = trainIndices,
      "testIndices" = testIndices
    ) )
  }
}
