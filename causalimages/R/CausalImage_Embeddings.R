#!/usr/bin/env Rscript
#' Generates image and video embeddings useful in earth observation tasks for casual inference, following the approach in Rolf, Esther, et al.  (2021).
#'
#' Generates image and video embeddings useful in earth observation tasks for casual inference, following the approach in Rolf, Esther, et al.  (2021).
#'
#' @usage
#'
#' GetImageEmbeddings(imageKeysOfUnits, acquireImageFxn, nEmbedDim, ...)
#'
#' @param acquireImageFxn A function specifying how to load images representations associated with `imageKeysOfUnits` into memory. For example, if observation `3` has a value  of `"a34f"` in `imageKeysOfUnits`, `acquireImageFxn` should extract the image associated with the unique key `"a34f"`.
#' First argument should be image key values and second argument have be `training` (in case different behavior in training/inference mode).
#' @param imageKeysOfUnits A vector of length `length(imageKeysOfUnits)` specifying the unique image ID associated with each unit. Samples of `imageKeysOfUnits` are fed into `acquireImageFxn` to call images into memory.
#' @param conda_env (default = `NULL`) A string specifying a conda environment wherein `tensorflow`, `tensorflow_probability`, and `gc` are installed.
#' @param conda_env_required (default = `F`) A Boolean stating whether use of the specified conda environment is required.
#' @param kernelSize (default = `5L`) Dimensions used in the convolution kernels.
#' @param temporalKernelSize (default = `2L`) Dimensions used in the temporal part of the convolution kernels if using image sequences.
#' @param nEmbedDim (default = `128L`) Number of embedding features output.
#' @param strides (default = `2L`) Integer specifying the strides used in the convolutional layers.
#' @param InitImageProcess (default = `NULL`)
#' @param batchSize (default = `50L`) Integer specifying batch size in obtaining embeddings.
#' @param file (default = `NULL`) Path to a tfrecord file generated by `WriteTfRecord`.
#' @param TfRecords_BufferScaler (default = `10L`) The buffer size used in `tfrecords` mode is `batchSize*TfRecords_BufferScaler`. Lower `TfRecords_BufferScaler` towards 1 if out-of-memory problems.
#' @param seed (default = `NULL`) Integer specifying the seed.
#' @param outputType (default = `"R"`) Either `"R"` or `"tensorflow"` indicating whether to output R or tensorflow arrays.
#'
#' @return A list containing two items:
#' \itemize{
#' \item `embeddings` (matrix) A matrix containingimage/video embeddings, with rows corresponding to observations.
#' \item `embeddings_fxn` (function) The functioning performing the embedding, returned for re-use.
#' }
#'
#' @section References:
#' \itemize{
#' \item Rolf, Esther, et al. "A generalizable and accessible approach to machine learning with global satellite imagery." *Nature Communications* 12.1 (2021): 4392.
#' }
#'
#' @examples
#' # For a tutorial, see
#' # github.com/cjerzak/causalimages-software/
#'
#' @export
#' @md

GetImageEmbeddings <- function(
    acquireImageFxn  = NULL,
    imageKeysOfUnits = NULL,
    file = NULL,
    conda_env = NULL,
    conda_env_required = F,

    InitImageProcess = NULL,
    nEmbedDim = 128L,
    batchSize = 50L,
    strides = 1L,
    temporalKernelSize = 2L,
    kernelSize = 3L,
    TfRecords_BufferScaler = 10L,
    dataType = "image",
    image_dtype = "float16",
    inputAvePoolingSize = 1L, # set > 1L if seeking to downshift the image resolution
    seed = NULL,
    quiet = F){


  # initialize tensorflow if not already initialized
  if(   !"logical" %in% class(try(as.numeric(tf$square(1.))==1,T))   ){
    print("Initializing the tensorflow environment...")
    print("Looking for Python modules tensorflow, gc...")
    library(tensorflow); # library(keras)
    if(!is.null(conda_env)){
      try(tensorflow::use_condaenv(conda_env, required = conda_env_required),T)
    }
    Sys.sleep(1.); try(tf$square(1.),T); Sys.sleep(1.);
    try(tf$config$experimental$set_memory_growth(tf$config$list_physical_devices('GPU')[[1]],T),T)
    try( tf$config$set_soft_device_placement( T ) , T)

    try(tf$random$set_seed(  c( ifelse(is.null(tf_seed),
                                       yes = 123431L, no = as.integer(tf_seed)  ) )), T)
    try(tf$keras$utils$set_random_seed( c( ifelse(is.null(tf_seed),
                                                  yes = 123419L, no = as.integer(tf_seed)  ) )), T)

    # import python garbage collectors
    py_gc <- reticulate::import("gc")
  }
  gc(); try(py_gc$collect(), T)

  # image dtype management
  image_dtype_ <- try(eval(parse(text = sprintf("tf$%s",image_dtype))), T)
  if("try-error" %in% class(image_dtype_)){ image_dtype_ <- try(eval(parse(text = sprintf("tf$%s",image_dtype$name))), T) }
  image_dtype <- image_dtype_
  float_dtype <- image_dtype

  if( batchSize > length(imageKeysOfUnits) ){
    batchSize <- length( imageKeysOfUnits  )
  }

  acquireImageMethod <- "functional";
  # define base tf record + train/test fxns
  orig_wd <- getwd()
  if(  !is.null(  file  )  ){
    acquireImageMethod <- "tf_record"

    # established tfrecord connection
    tf_record_name <- file
    if( !grepl(tf_record_name, pattern = "/") ){
      tf_record_name <- paste("./",tf_record_name, sep = "")
    }
    tf_record_name <- strsplit(tf_record_name,split="/")[[1]]
    new_wd <- paste(tf_record_name[-length(tf_record_name)],collapse = "/")
    setwd( new_wd )
    tf_dataset = tf$data$TFRecordDataset(  tf_record_name[length(tf_record_name)] )

    # helper functions
    useVideo <- (dataType == "video")
    getParsed_tf_dataset_inference <- function(tf_dataset){
        dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo)} ) # return
      return( dataset <- dataset$batch( as.integer(max(2L, round(batchSize/2L)  ))) )
    }

    getParsed_tf_dataset_train <- function(tf_dataset){
      dataset <- tf_dataset$map( function(x){parse_tfr_element(x, readVideo = useVideo)} ) # return
      dataset <- dataset$shuffle(tf$constant(as.integer(TfRecords_BufferScaler*batchSize), dtype=tf$int64),
                                 reshuffle_each_iteration = T)
      dataset <- dataset$batch(as.integer(batchSize))
    }

    # setup iterators
    tf_dataset_train <- getParsed_tf_dataset_train( tf_dataset )
    tf_dataset_inference <- getParsed_tf_dataset_inference( tf_dataset )

    # reset iterators
    ds_iterator_train <- reticulate::as_iterator( tf_dataset_train )
    ds_iterator_inference <- reticulate::as_iterator( tf_dataset_inference )

    # checks
    # ds_iterator_inference$output_shapes; ds_iterator_train$output_shapes
    # ds_next_train <- reticulate::iter_next( ds_iterator_train )
    # ds_next_inference <- reticulate::iter_next( ds_iterator_inference )
  }

  # acquire image
  if(acquireImageMethod == "functional"){
    myType <- acquireImageFxn(imageKeysOfUnits[1:2], training = F)
    environment(acquireImageFxn) <- environment()
    test_ <- acquireImageFxn(imageKeysOfUnits[1:5],training = F)
    if(!"tensorflow.tensor" %in% class(test_)){
      acquireImageFxn_as_input <- acquireImageFxn
      acquireImageFxn <- function(keys, training){
        m_ <- tf$constant(acquireImageFxn_as_input(keys, training),image_dtype)
        if(length(m_$shape) == 3){
          # expand across batch dimension if receiving no batch dimension
          m_ <- tf$expand_dims(m_,0L)
        }
        return( m_ )
      }
    }
  }
  if(acquireImageMethod == "tf_record"){
    setwd(orig_wd)
    test_ <- tf$expand_dims(GetElementFromTfRecordAtIndices( indices = 1L,
                                                             filename = file,
                                                             readVideo = useVideo,
                                                             image_dtype = image_dtype,
                                                             nObs = length(imageKeysOfUnits))[[1]],0L)
    setwd(new_wd)
  }

  imageDims <- length( dim(test_) ) - 2L
  useAvePooling <- F; if(useAvePooling){
    if(nEmbedDim %% 2 == 0){ OddInput <- F; nFilters <-  nEmbedDim/2 }
    if(nEmbedDim %% 2 == 1){ OddInput <- T; nFilters <-  ceiling(nEmbedDim/2) }
  }
  if(!useAvePooling){ nFilters <-  nEmbedDim  }
  if(imageDims == 2){
    if(inputAvePoolingSize > 1){
      AvePoolingDownshift <- tf$keras$layers$AveragePooling2D(pool_size = as.integer(c(inputAvePoolingSize,inputAvePoolingSize)))
    }
    myConv <- tf$keras$layers$Conv2D(filters=nFilters,
                          kernel_size = c(kernelSize,kernelSize),
                          activation = "linear",
                          strides = c(strides,strides),
                          padding = "valid",
                          trainable = F)
    GlobalMaxPoolLayer <- tf$keras$layers$GlobalMaxPool2D(data_format="channels_last",name="GlobalMax")
    if(useAvePooling){ GlobalAvePoolLayer <- tf$keras$layers$GlobalAveragePooling2D(data_format="channels_last",name="GlobalAve") }
  }
  if(imageDims == 3){
    if(inputAvePoolingSize > 1){
      AvePoolingDownshift <- tf$keras$layers$AveragePooling3D(pool_size = as.integer(c(1L, inputAvePoolingSize,inputAvePoolingSize)))
    }
    # 3D conv method
    {

      if(T == F){
        myConv <- tf$keras$layers$Conv3D(filters = nFilters,
                                         kernel_size = c(temporalKernelSize, kernelSize, kernelSize),
                                         activation = "linear",
                                         groups = 1L,
                                         dtype = float_dtype,
                                         strides = c(1L, strides, strides), padding = "valid", trainable = F)
      }
      if(T == T){
        #myConv_spatial_fxn <- tf$keras$layers$Conv3D(filters = nFilters, kernel_size = c(1L, kernelSize, kernelSize),
                                         #activation = "linear",
                                         #strides = c(1L, strides, strides), padding = "valid", trainable = F)
        myConv_spatial <- tf$keras$layers$SeparableConv2D(filters = nFilters,
                                                 kernel_size = c(kernelSize, kernelSize),
                                                 activation = "linear",
                                                 dtype = float_dtype,
                                                 strides = c(strides, strides), padding = "valid", trainable = F)
        myConv_spatial_fxn <- (function(m){
          m_ <- tf$transpose(m, c(1L,0L,2L,3L,4L)) # swap for vmap
          m_ <- tf$vectorized_map(myConv_spatial, m_) # vectorized_map only seems to work across axis 0L
          m_ <- tf$transpose(m_, c(1L,0L,2L,3L,4L)) # swap back for rest of analysis
          return( m_ )
        })
        myConv_temporal <- tf$keras$layers$Conv3D(filters = nFilters,
                                                 kernel_size = c(temporalKernelSize, 1L, 1L),
                                                 activation = "linear",
                                                 groups = 1L,
                                                 dtype = float_dtype,
                                                 strides = c(1L, strides, strides),
                                                 #strides = c(1L, 1L, 1L),
                                                 padding = "valid", trainable = F)
        # m <- batch_inference[[1]]
        # myConv_spatial ( tf$gather(m,1L,axis = 1L) )

        # myConv_spatial ( m );  myConv(m)
        myConv <- ( function(m){ myConv_temporal( myConv_spatial_fxn( m ) ) })
      }
      GlobalMaxPoolLayer <- tf$keras$layers$GlobalMaxPool3D(data_format="channels_last", name="GlobalMax")
      if(useAvePooling){ GlobalAvePoolLayer <- tf$keras$layers$GlobalAveragePooling3D(data_format="channels_last", name="GlobalAve") }
    }
  }
  GlobalPoolLayer <- (function(z){
    if(useAvePooling){
      z <- tf$concat(list(GlobalMaxPoolLayer(z),GlobalAvePoolLayer(z)),1L)
      if(OddInput){ z <- tf$gather(z, as.integer(0L:(nEmbedDim-1L)), axis = 1L) }
    }
    if(!useAvePooling){ z <- GlobalMaxPoolLayer(z) }
    return( z )
  })
  if(is.null(InitImageProcess)){
    InitImageProcess <- (function(im, training = F){

      # normalize if desired
      # im <- tf$divide(tf$subtract(im, NORM_MEAN_array), NORM_SD_array)

      # downshift resolution if desired
      if(inputAvePoolingSize > 1){ im <- AvePoolingDownshift(im) }
      return( im  )
    })
  }
  getEmbedding <- tf_function(function(im_){
    #with( tf$xla$experimental$jit_scope(compile_ops=F), {
      im_ <- GlobalPoolLayer( myConv( InitImageProcess( im_ , training = F)  ) )
    #})
    return( im_  )
  } )

  embeddings <- matrix(NA,nrow = length(imageKeysOfUnits), ncol = nEmbedDim)
  last_i <- 0; ok_counter <- 0; ok<-F; while(!ok){
      ok_counter <- ok_counter + 1

      batch_indices <- (last_i+1):(last_i+batchSize)
      batch_indices <- batch_indices[batch_indices <= length(imageKeysOfUnits)]
      last_i <- batch_indices[length(batch_indices)]
      if(last_i == length(imageKeysOfUnits)){ ok <- T }

      batchSizeOneCorrection <- F; if(length(batch_indices) == 1){
        batchSizeOneCorrection <- T
      }

      # in functional mode
      if(acquireImageMethod == "functional"){
        if(batchSizeOneCorrection){ batch_indices <- c(batch_indices, batch_indices) }
        batch_inference <- list(
          tf$cast(acquireImageFxn(imageKeysOfUnits[batch_indices], training = F),image_dtype)
        )
      }

      if(acquireImageMethod == "tf_record"){
        setwd(orig_wd)
        batch_inference <- GetElementFromTfRecordAtIndices( indices = batch_indices,
                                                            filename = file,
                                                            nObs = length(imageKeysOfUnits),
                                                            return_iterator = T,
                                                            readVideo = useVideo,
                                                            image_dtype = image_dtype,
                                                            iterator = ifelse(ok_counter > 1,
                                                                              yes = list(saved_iterator),
                                                                              no = list(NULL))[[1]])
        if(batchSizeOneCorrection){
          batch_inference[[1]][[1]] <- tf$concat(list(tf$expand_dims(batch_inference[[1]][[1]],0L),
                                                 tf$expand_dims(batch_inference[[1]][[1]],0L)), 0L)
        }
        setwd(new_wd)
        saved_iterator <- batch_inference[[2]]
        batch_inference <- batch_inference[[1]]
        # batch_inference[[2]]; batch_indices
      }

      # getEmbedding(batch_inference[[1]])
      embed_ <- try(as.matrix(  getEmbedding(batch_inference[[1]])  ), T)
      if("try-error" %in% class(embed_)){ browser() }
      if(batchSizeOneCorrection){ batch_indices <- batch_indices[-1]; embed_ <- embed_[1,] }
      embeddings[batch_indices,] <- embed_

      print(sprintf("[%s] %.2f%% done getting image/video embeddings", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), 100*last_i / length(imageKeysOfUnits)))

    gc(); try(py_gc$collect(), T)
  }
  print(sprintf("[%s] done with getting image/video embeddings", format(Sys.time(), "%Y-%m-%d %H:%M:%S")))

  # reset wd (may have been changed via tfrecords use)
  setwd(  orig_wd  )

  return( list( "embeddings"= embeddings,
                 "embeddings_fxn" = getEmbedding ) )
}
