% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CI_ImageModelBackbones.R
\name{GetImageRepresentations}
\alias{GetImageRepresentations}
\title{Generates image and video representations useful in earth observation tasks for causal inference.}
\usage{
GetImageRepresentations(
  X = NULL,
  imageKeysOfUnits = NULL,
  file = NULL,
  conda_env = "CausalImagesEnv",
  conda_env_required = T,
  returnContents = T,
  getRepresentations = T,
  imageModelClass = "VisionTransformer",
  NORM_MEAN = NULL,
  NORM_SD = NULL,
  Sys.setenv_text = NULL,
  InitImageProcess = NULL,
  pretrainedModel = NULL,
  lat = NULL,
  long = NULL,
  image_dtype = NULL,
  image_dtype_tf = NULL,
  XCrossModal = T,
  XForceModal = F,
  nWidth_ImageRep = 64L,
  nDepth_ImageRep = 1L,
  nDepth_TemporalRep = 1L,
  batchSize = 16L,
  nonLinearScaler = NULL,
  optimizeImageRep = T,
  strides = 1L,
  kernelSize = 3L,
  patchEmbedDim = 16L,
  TfRecords_BufferScaler = 10L,
  dropoutRate = 0,
  droppathRate = 0,
  dataType = "image",
  temporalAggregation = "concatenate",
  bn_momentum = 0.99,
  inputAvePoolingSize = 1L,
  CleanupEnv = FALSE,
  initializingFxns = FALSE,
  seed = NULL
)
}
\arguments{
\item{X}{Optional numeric matrix of tabular covariates for cross-modal learning.}

\item{imageKeysOfUnits}{A vector of length \code{length(imageKeysOfUnits)} specifying the unique image ID associated with each unit. Samples of \code{imageKeysOfUnits} are fed into the package to call images into memory.}

\item{file}{Path to a tfrecord file generated by \code{causalimages::WriteTfRecord}.}

\item{conda_env}{A \code{conda} environment where computational environment lives, usually created via \code{causalimages::BuildBackend()}. Default = \code{"CausalImagesEnv"}}

\item{conda_env_required}{A Boolean stating whether use of the specified conda environment is required.}

\item{returnContents}{Boolean specifying whether to return internal model contents. Default = \code{TRUE}.}

\item{getRepresentations}{Boolean specifying whether to compute and return representations. Default = \code{TRUE}.}

\item{imageModelClass}{String specifying the image model architecture. Options include \code{"VisionTransformer"} (default) or \code{"CNN"}.}

\item{NORM_MEAN}{Numeric vector of length 1-3 specifying dataset mean(s) for normalization. Used with pretrained models.}

\item{NORM_SD}{Numeric vector of length 1-3 specifying dataset standard deviation(s) for normalization. Used with pretrained models.}

\item{Sys.setenv_text}{Optional string for setting environment variables before Python initialization.}

\item{InitImageProcess}{(default = \code{NULL}) Initial image processing function. Usually left \code{NULL}.}

\item{pretrainedModel}{Optional string specifying a pretrained vision model to use for feature extraction.
Built-in options include:
\itemize{
\item \code{"vit-base"} - Google's Vision Transformer (ViT-Base, 768-dim embeddings)
\item \code{"clip-rsicd"} - CLIP fine-tuned on remote sensing data (512-dim embeddings)
\item \code{"clip-rsicd-v0"} - Legacy CLIP-RSICD implementation
}
For generic HuggingFace models, use the \code{"transformers-"} prefix followed by the model name:
\itemize{
\item \code{"transformers-facebook/dinov2-base"} - DINOv2 self-supervised ViT
\item \code{"transformers-microsoft/resnet-50"} - ResNet-50
\item \code{"transformers-facebook/convnext-base-224"} - ConvNeXt
\item \code{"transformers-<any-huggingface-model>"} - Any vision model from HuggingFace
}
The generic handler uses \code{AutoModel.from_pretrained()} and automatically detects hidden dimensions,
input sizes, and handles various output formats. Default is \code{NULL} (uses custom model architecture).}

\item{lat, long}{Optional vectors specifying latitude and longitude coordinates for spatial context.}

\item{image_dtype}{JAX dtype for image data. Usually set internally.}

\item{image_dtype_tf}{TensorFlow dtype for image data. Usually set internally.}

\item{XCrossModal}{Boolean specifying whether to use cross-modal learning with tabular data. Default = \code{TRUE}.}

\item{XForceModal}{Boolean specifying whether to force modal learning. Default = \code{FALSE}.}

\item{nWidth_ImageRep}{Number of embedding features output.}

\item{nDepth_ImageRep}{Integer specifying depth of image representation model. Default = \code{1L}.}

\item{nDepth_TemporalRep}{Integer specifying depth of temporal representation model for video data. Default = \code{1L}.}

\item{batchSize}{Integer specifying batch size in obtaining representations.}

\item{nonLinearScaler}{Optional string specifying non-linear scaling function for outputs.}

\item{optimizeImageRep}{Boolean specifying whether to optimize image representation parameters. Default = \code{TRUE}.}

\item{strides}{Integer specifying the strides used in the convolutional layers.}

\item{kernelSize}{Dimensions used in the convolution kernels.}

\item{patchEmbedDim}{Integer specifying patch embedding dimension for Vision Transformer. Default = \code{16L}.}

\item{TfRecords_BufferScaler}{The buffer size used in \code{tfrecords} mode is \code{batchSize*TfRecords_BufferScaler}. Lower \code{TfRecords_BufferScaler} towards 1 if out-of-memory problems.}

\item{dropoutRate}{Dropout rate used in training. Default = \code{0}.}

\item{droppathRate}{Droppath rate used in training. Default = \code{0}.}

\item{dataType}{String specifying whether to assume \code{"image"} or \code{"video"} data types. Default is \code{"image"}.}

\item{temporalAggregation}{String specifying how to aggregate embeddings across time periods for video/image sequence data. Options are \code{"transformer"} which uses a temporal transformer with attention pooling, \code{"concatenate"} which simply concatenates the frame-level embeddings, \code{"difference"} which computes temporal differences to capture change dynamics (outputs mean embeddings concatenated with mean of first-order differences), or \code{"variance"} which concatenates temporal mean with temporal variance to capture both typical state and volatility/instability.}

\item{bn_momentum}{Batch normalization momentum. Default = \code{0.99}.}

\item{inputAvePoolingSize}{Integer specifying average pooling size for downshifting image resolution. Default = \code{1L}.}

\item{CleanupEnv}{Boolean specifying whether to clean up environment after processing. Default = \code{FALSE}.}

\item{initializingFxns}{Boolean specifying whether to only initialize functions without computing representations. Default = \code{FALSE}.}

\item{seed}{Optional integer for reproducibility.}
}
\value{
A list containing two items:
\itemize{
\item \code{Representations} (matrix) A matrix containing image/video representations, with rows corresponding to observations.
\item \verb{ImageRepArm_OneObs,ImageRepArm_batch_R, ImageRepArm_batch} (functions) Image modeling functions.
\item \code{ImageModel_And_State_And_MPPolicy_List} List containing image model parameters fed into functions.
}
}
\description{
Generates image and video representations useful in earth observation tasks for causal inference.
}
\section{References}{

\itemize{
\item Rolf, Esther, et al. "A generalizable and accessible approach to machine learning with global satellite imagery." \emph{Nature Communications} 12.1 (2021): 4392.
}
}

\examples{
# For a tutorial, see
# github.com/cjerzak/causalimages-software/

}
