% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CausalImage_Confounding.R
\name{AnalyzeImageConfounding}
\alias{AnalyzeImageConfounding}
\title{Perform causal estimation under image confounding}
\usage{
AnalyzeImageConfounding(obsW, obsY, imageKeysOfUnits, acquireImageFxn, ...)
}
\arguments{
\item{obsW}{A numeric vector where \code{0}'s correspond to control units and \code{1}'s to treated units.}

\item{obsY}{A numeric vector containing observed outcomes.}

\item{X}{(optional) A numeric matrix containing tabular information used if \code{orthogonalize = T}. \code{X} is normalized internally and salience maps with respect to \code{X} are transformed back to the original scale.}

\item{file}{(default = \code{NULL}) Absolute path to a tfrecord file generated by \code{WriteTfRecord}.}

\item{imageKeysOfUnits}{(default = \code{1:length(obsY)}) A vector of length \code{length(obsY)} specifying the unique image ID associated with each unit. Samples of \code{imageKeysOfUnits} are fed into \code{acquireImageFxn} to call images into memory.}

\item{doConvLowerDimProj}{(default = \code{T}) Should we project the \code{nFilters} convolutional feature dimensions down to \code{nDimLowerDimConv} to reduce the number of required parameters.}

\item{nDimLowerDimConv}{(default = \code{3L}) If \code{doConvLowerDimProj = T}, then, in each convolutional layer, we project the \code{nFilters} feature dimensions down to \code{nDimLowerDimConv} to reduce the number of parameters needed.}

\item{nFilters}{(default = \code{50L}) Integer specifying the number of convolutional filters used.}

\item{modelClass}{(default = \code{"cnn"}) Either \code{"cnn"} or \code{"embeddings"}.}

\item{nBoot}{(default = \code{100L}) Number of bootstrap iterations for uncertainty estimation.}

\item{typeBoot}{(default = \code{SamplingOnly}) Bootstrap type. \code{typeBoot = 'SamplingOnly'} captures sampling uncertainty only. \code{typeBoot = 'EstimationAndSampling'} captures both estimation and sampling uncertainty.}

\item{nEmbedDim}{(default = \code{96L}) Integer specifying the image/image sequence embedding dimension. Used if \code{modelClass = "embeddings"}.}

\item{acquireImageFxn}{A function specifying how to load images representations associated with \code{imageKeysOfUnits} into memory. For example, if observation \code{3} has a value  of \code{"a34f"} in \code{imageKeysOfUnits}, \code{acquireImageFxn} should extract the image associated with the unique key \code{"a34f"}.
First argument should be image key values and second argument have be \code{training} (in case different behavior in training/inference mode).}

\item{transportabilityMat}{(optional) A matrix with a column named \code{imageKeysOfUnits} specifying keys to be used by \code{acquireImageFxn} for generating treatment effect predictions for out-of-sample points.}

\item{long, lat}{(optional) Vectors specifying longitude and latitude coordinates for units. Used only for describing highest and lowest probability neighorhood units if specified.}

\item{conda_env}{(default = \code{NULL}) A string specifying a conda environment wherein \code{tensorflow}, \code{tensorflow_probability}, and \code{gc} are installed.}

\item{conda_env_required}{(default = \code{F}) A Boolean stating whether use of the specified conda environment is required.}

\item{figuresTag}{(default = \code{""}) A string specifying an identifier that is appended to all figure names.}

\item{figuresPath}{(default = \code{"./"}) A string specifying file path for saved figures made in the analysis.}

\item{tagInFigures}{(default = \code{F}) A Boolean specifying whether to visually include the tag in the figures.}

\item{plotBands}{(default = \code{1L}) An integer or vector specifying which band position (from the acquired image representation) should be plotted in the visual results. If a vector, \code{plotBands} should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RBG plotting).}

\item{simMode}{(default = \code{F}) Should the analysis be performed in comparison with ground truth from simulation?}

\item{plotResults}{(default = \code{T}) Should analysis results be plotted?}

\item{nDepthHidden_conv}{(default = \code{3L}) Hidden depth of convolutional layer.}

\item{nDepthHidden_dense}{(default = \code{0L}) Hidden depth of dense layers. Default of \code{0L} means a single projection layer is performed after the convolutional layer (i.e., no hidden layers are used).}

\item{maxPoolSize}{(default = \code{2L}) Integer specifying the max pooling size used in the convolutional layers.}

\item{strides}{(default = \code{2L}) Integer specifying the strides used in the convolutional layers.}

\item{dropoutRate}{(default = \code{0.1}) Droppout rate used in training used to prevent overfitting (\code{dropoutRate = 0} corresponds to no dropout).}

\item{batchSize}{(default = \code{50L}) Batch size used in SGD optimization.}

\item{kernelSize}{(default = \code{5L}) Dimensions used in convolution kernels.}

\item{nSGD}{(default = \code{400L}) Number of stochastic gradient descent (SGD) iterations.}

\item{testFrac}{(default = \code{0.01}) Fraction of observations held out as a test set to evaluate out-of-sample loss values.}

\item{nDenseWidth}{(default = \code{32L}) Width of dense projection layers post-convolutions.}

\item{channelNormalize}{(default = \code{T}) Should channelwise image feature normalization be attempted? Default is \code{T}, as this improves training.}

\item{TfRecords_BufferScaler}{(default = \code{4L}) The buffer size used in \code{tfrecords} mode is \code{batchSize*TfRecords_BufferScaler}. Lower \code{TfRecords_BufferScaler} towards 1 if out-of-memory problems.}

\item{tf_seed}{(default = \code{NULL}) Specification for the tensorflow seed.}

\item{quiet}{(default = \code{F}) Should we suppress information about progress?}
}
\value{
A list consiting of \itemize{
\item \code{ATE_est} ATE estimate.
\item \code{ATE_se} Standard error estimate for the ATE.
\item \verb{(images saved to disk if plotResults = T)} If \code{plotResults = T}, causal salience plots are saved to disk characterizing the image confounding structure. See references for details.
}
}
\description{
Perform causal estimation under image confounding
}
\section{References}{

\itemize{
\item  Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Integrating Earth Observation Data into Causal Inference: Challenges and Opportunities. \emph{ArXiv Preprint}, 2023.
}
}

\examples{
# For a tutorial, see
# github.com/cjerzak/causalimages-software/

}
