% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CI_PredictiveRun.R
\name{PredictiveRun}
\alias{PredictiveRun}
\title{Perform predictive modeling using images or videos}
\usage{
PredictiveRun(
  obsY,
  imageKeysOfUnits = NULL,
  file = NULL,
  fileTransport = NULL,
  imageKeysOfUnitsTransport = NULL,
  nBoot = 10L,
  inputAvePoolingSize = 1L,
  useTrainingPertubations = T,
  useScalePertubations = F,
  X = NULL,
  conda_env = "CausalImagesEnv",
  conda_env_required = T,
  Sys.setenv_text = NULL,
  figuresTag = NULL,
  figuresPath = "./",
  plotBands = 1L,
  plotResults = T,
  XCrossModal = T,
  XForceModal = F,
  optimizeImageRep = T,
  nWidth_ImageRep = 64L,
  nDepth_ImageRep = 1L,
  kernelSize = 5L,
  nWidth_Dense = 64L,
  nDepth_Dense = 1L,
  imageModelClass = "VisionTransformer",
  pretrainedModel = NULL,
  strides = 2L,
  nonLinearScaler = NULL,
  nDepth_TemporalRep = 3L,
  patchEmbedDim = 16L,
  dropoutRate = 0.1,
  droppathRate = 0.1,
  batchSize = 16L,
  nSGD = 400L,
  testFrac = 0.05,
  TfRecords_BufferScaler = 4L,
  learningRateMax = 0.001,
  TFRecordControl = NULL,
  dataType = "image",
  temporalAggregation = "transformer",
  image_dtype = "float16",
  atError = "stop",
  seed = NULL,
  modelPath = "./trained_model.eqx",
  metricsPath = "./evaluation_metrics.rds"
)
}
\arguments{
\item{obsY}{A numeric vector containing observed outcomes to predict.}

\item{imageKeysOfUnits}{A vector of length \code{length(obsY)} specifying the unique image ID associated with each unit. Samples of \code{imageKeysOfUnits} are fed into the package to call images into memory.}

\item{file}{Path to a tfrecord file generated by \code{WriteTfRecord}.}

\item{fileTransport}{Path to a tfrecord file for transportability analysis (out-of-sample prediction).}

\item{imageKeysOfUnitsTransport}{A vector of image keys for transportability analysis units.}

\item{nBoot}{Number of bootstrap iterations for uncertainty estimation.}

\item{inputAvePoolingSize}{Integer specifying average pooling size for downshifting image resolution. Default = \code{1L} (no downshift).}

\item{useTrainingPertubations}{Boolean specifying whether to randomly perturb the image axes during training to reduce overfitting.}

\item{useScalePertubations}{Boolean specifying whether to use scale perturbations during training. Default = \code{FALSE}.}

\item{X}{An optional numeric matrix containing tabular information. \code{X} is normalized internally.}

\item{conda_env}{A \code{conda} environment where computational environment lives, usually created via \code{causalimages::BuildBackend()}. Default = \code{"CausalImagesEnv"}.}

\item{conda_env_required}{A Boolean stating whether use of the specified conda environment is required.}

\item{Sys.setenv_text}{Optional string for setting environment variables before Python initialization.}

\item{figuresTag}{A string specifying an identifier that is appended to all figure names.}

\item{figuresPath}{A string specifying file path for saved figures made in the analysis.}

\item{plotBands}{An integer or vector specifying which band position (from the image representation) should be plotted in the visual results. If a vector, \code{plotBands} should have 3 (and only 3) dimensions (corresponding to the 3 dimensions to be used in RGB plotting).}

\item{plotResults}{(default = \code{T}) Should analysis results be plotted?}

\item{XCrossModal}{Boolean specifying whether to use cross-modal learning with tabular data. Default = \code{TRUE}.}

\item{XForceModal}{Boolean specifying whether to force modal learning. Default = \code{FALSE}.}

\item{optimizeImageRep}{Boolean specifying whether to optimize over the image model representation (or only over downstream parameters).}

\item{nWidth_ImageRep}{Integer specifying width of image model representation.}

\item{nDepth_ImageRep}{Integer specifying depth of image model representation.}

\item{kernelSize}{Dimensions used in spatial convolutions.}

\item{nWidth_Dense}{Integer specifying width of image model representation.}

\item{nDepth_Dense}{Integer specifying depth of dense model representation.}

\item{imageModelClass}{String specifying the image model architecture. Options include \code{"VisionTransformer"} (default) or \code{"CNN"}.}

\item{pretrainedModel}{Optional string specifying a pretrained model to use. Options include \code{"vit-base"}, \code{"clip-rsicd"}, or HuggingFace model names with \code{"transformers-"} prefix.}

\item{strides}{(default = \code{2L}) Integer specifying the strides used in the convolutional layers.}

\item{nonLinearScaler}{Optional string specifying non-linear scaling function for outputs.}

\item{nDepth_TemporalRep}{Integer specifying depth of temporal representation model. Default = \code{3L}.}

\item{patchEmbedDim}{Integer specifying patch embedding dimension for Vision Transformer. Default = \code{16L}.}

\item{dropoutRate}{Dropout rate used in training to prevent overfitting (\code{dropoutRate = 0} corresponds to no dropout).}

\item{droppathRate}{Droppath rate used in training to prevent overfitting (\code{droppathRate = 0} corresponds to no droppath).}

\item{batchSize}{Batch size used in SGD optimization. Default = \code{50L}.}

\item{nSGD}{Number of stochastic gradient descent (SGD) iterations. Default = \code{400L}}

\item{testFrac}{Default = \code{0.1}. Fraction of observations held out as a test set to evaluate out-of-sample loss values.}

\item{TfRecords_BufferScaler}{The buffer size used in \code{tfrecords} mode is \code{batchSize*TfRecords_BufferScaler}. Lower \code{TfRecords_BufferScaler} towards 1 if out-of-memory problems.}

\item{learningRateMax}{Maximum learning rate for the optimizer. Default = \code{0.001}.}

\item{TFRecordControl}{Optional list for advanced TFRecord configuration.}

\item{dataType}{(default = \code{"image"}) String specifying whether to assume \code{"image"} or \code{"video"} data types.}

\item{temporalAggregation}{String specifying how to aggregate embeddings across time periods for video/image sequence data. Options are \code{"transformer"} (default) which uses a temporal transformer with attention pooling, or \code{"concatenate"} which simply concatenates the frame-level embeddings.}

\item{image_dtype}{String specifying image data type. Options are \code{"float16"} (default), \code{"bfloat16"}, or \code{"float32"}.}

\item{atError}{String specifying behavior on error. Options are \code{"stop"} (default) or \code{"debug"}.}

\item{seed}{Optional integer for reproducibility.}

\item{modelPath}{Path to save the trained model. Default = \code{"./trained_model.eqx"}.}

\item{metricsPath}{Path to save the evaluation metrics as a RDS file. Default = \code{"./evaluation_metrics.rds"}.}
}
\value{
Returns a list consisting of
\itemize{
\item \code{predictedY} Predicted values for all units.
\item \code{ModelEvaluationMetrics} Rigorous evaluation metrics (e.g., MSE, R2 for continuous; AUC, accuracy for binary).
}
}
\description{
Perform predictive modeling using images or videos
}
\section{References}{

\itemize{
\item  Connor T. Jerzak, Fredrik Johansson, Adel Daoud. Integrating Earth Observation Data into Causal Inference: Challenges and Opportunities. \emph{ArXiv Preprint}, 2023.
}
}

\examples{
# For a tutorial, see
# github.com/cjerzak/causalimages-software/

}
